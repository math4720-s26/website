---
title: 'Sampling Distribution and Central Limit Theorem `r fontawesome::fa("infinity")`'
title-slide-attributes:
  data-background-image: ../images/bg.png
  # data-background-size: stretch
  # data-slide-number: none
format: 
  live-revealjs: 
    output-file: 08-sampling-dist-clt-slides.html
    # theme: slides.scss
webr:
  cell-options:
    autorun: false
  packages:
    - tidyverse
knitr:
  opts_chunk:
    out-width: 100%
    echo: false
---


# {visibility="hidden"}

\def\bx{\mathbf{x}}
\def\bg{\mathbf{g}}
\def\bw{\mathbf{w}}
\def\bbeta{\boldsymbol \beta}
\def\bX{\mathbf{X}}
\def\by{\mathbf{y}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\T{\text{T}}
\def\cov{\mathrm{Cov}}
\def\cor{\mathrm{Corr}}
\def\var{\mathrm{Var}}
\def\E{\mathrm{E}}
\def\bmu{\boldsymbol \mu}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\Trace{\text{Trace}}


```{r}
#| label: pkg
#| include: false
#| eval: true
library(emo)
library(kableExtra)
library(countdown)
library(knitr)
library(openintro)
options(digits = 3)
```

<!-- begin: webr fodder -->

{{< include ./_extensions/r-wasm/live/_knitr.qmd >}}

<!-- end: webr fodder -->

# [Sampling Distribution]{.orange}{background-image="./images/08-sampling-dist-clt/Bernoulli.jpg" background-size="cover" background-position="50% 50%" background-color="#447099"}

::: notes
- Sampling distribution is an important concept that connect probability and statistics together. 
- Til now, we can start talk a little bit about statistical inference. 
- After we finish the discussion of probability this week, we are going to formally study statistical inference next week.
- To do statistical inference, we use the sampling distribution a lot, and that's why we have to learn what it is before doing statistical inference.
:::

## Parameter

- **Parameters** in a probability distribution are the values describing the entire distribution.
  + <span style="color:blue"> Binomial: two parameters $n$ and $\pi$ </span>
  + <span style="color:blue"> Poisson: one parameter $\lambda$ </span>
  + <span style="color:blue"> Normal: two parameters $\mu$ and $\sigma$ </span>
  
. . .

- In statistics, we usually assume our target population follows some distribution, but _its parameters are **unknown** to us_.

. . .

::::: columns

::: {.column width="50%"}

<span style="color:blue"> Human weight follows $N(\mu, \sigma^2)$ </span>

```{r}
#| out-width: 60%
# https://unsplash.com/photos/5jctAMjz21A
knitr::include_graphics("./images/08-sampling-dist-clt/weight.jpeg")
```
:::

::: {.column width="50%"}

<span style="color:blue"> # of snowstorms in one year follows $Poisson(\lambda)$ </span>

```{r}
#| out-width: 55%
knitr::include_graphics("./images/08-sampling-dist-clt/storm.jpeg")
```
:::

:::::

## Treat Each Data Point as a Random Variable

- $n$ random variables $X_1, X_2, \dots, X_n$.

- $X_1, X_2, \dots, X_n$ come from the **same** distribution.

::: alert
View $X_i$ as *a data point to be drawn* from a population with some distribution, say $N(\mu, \sigma^2)$.
:::


```{r}
par(mar = 0*c(1,1,1,1))
plot(c(0, 2),
     c(0, 1.1),
     type = 'n',
     axes = FALSE, xlab = "", ylab = "")
temp <- seq(0, 2 * pi, 2 * pi / 100)
x <- 0.5 + 0.5 * cos(temp)
y <- 0.5 + 0.5 * sin(temp)
lines(x, y)

s <- matrix(runif(700), ncol = 2)
S <- matrix(NA, 350, 2)
j <- 0
for (i in 1:nrow(s)) {
  if(sum((s[i, ] - 0.5)^2) < 0.23){
    j <- j + 1
    S[j, ] <- s[i, ]
  }
}
points(S, col = COL[1, 3], pch = 20)
text(0.5, 1, 'Population', pos = 3)

set.seed(50)
N <- sample(j, 25)
lines((x - 0.5) / 2 + 1.5, (y - 0.5) / 2 +  0.5, pch = 20)

SS <- (S[N, ] - 0.5) / 2 + 0.5
these <- c(2, 5, 11, 10, 12)
points(SS[these, 1] + 1,
       SS[these, 2],
       col = COL[4, 2],
       pch = 20,
       cex = 1.5)
text(1.5, 0.75, 'Sample', pos = 3)

for (i in these) {
  arrows(S[N[i], 1], S[N[i], 2],
         SS[i, 1] + 1 - 0.03, SS[i, 2],
         length = 0.08, col = COL[5], lwd = 1.5)
}
```

::: notes
- Previously when we talk about probability distributions, we usually assume there is a r.v. that follows some distribution, right? For example, $X$ from binomial, poisson, normal
- Now, let's assume we have a bunch of r.v.
- $n$ random variables: $X_1, X_2, \dots, X_n$.
- Assume $X_1, X_2, \dots, X_n$ follow the **same** distribution, e.g., <span style="color:blue"> $X_i \sim N(\mu, \sigma^2), i = 1, \dots, n$ </span>.
:::


## Treat Each Data Point as a Random Variable

- Assume that $X_1, X_2, \dots, X_n$ are **independent**, i.e., the distribution/value of $X_i$ is not affected by any other $X_j$.

- With the same distribution, $X_1, X_2, \dots, X_n$ are _**i**ndependent and **i**dentically **d**istributed_ (i.i.d.): <span style="color:blue"> $X_1, X_2, \dots, X_n \stackrel{iid}{\sim} N(\mu, \sigma^2)$ </span>

- $(X_1, X_2, \dots, X_n)$ is a **random sample** of size $n$ from the population. 
  + <span style="color:blue"> $X_1, X_2, \dots, X_{50}$ are randomly selected SAT scores from the SAT score population that follows $N(1100, 200^2)$ </span>

. . .

::: alert
- *Before* we actually collect the data, the data $X_1, X_2, \dots, X_n$ are random variables from the population distribution $N(\mu, \sigma^2)$.
- Once we collect the data, we know the *realized* value of these random variables, $x_1, x_2, \dots, x_n$.
:::
  
  
::: notes
- Assume $X_1, X_2, \dots, X_n$ follow the **same** distribution.
- Assume that $X_1, X_2, \dots, X_n$ are **independent**, i.e., the distribution of $X_i$ is not affected by any other $X_j$.
- Now this is where we connect the idea of sample data and population with the random variable and the distribution.
- View $X_i$ as a data point to be drawn from a population with some distribution, say $N(\mu, \sigma^2)$.
- We haven't really sample those points yet. Before we actually collect the data, they are random variables, and we don't know their value.
- Once we collect the data, we know the realized value of these random variables.
- $(X_1, X_2, \dots, X_n)$ is our random sample data that are sampled or drawn from the population that are assumed normally distributed with mean mu and SD sigma.

When we say $X$ follows some distribution, it means that
:::


## Sampling Distribution

- Any value computed from a sample $(X_1, X_2, \dots, X_n)$ is called a **(sample) statistic**.
  + <span style="color:blue"> Sample mean $\frac{1}{n}\sum_{i=1}^n X_i$ is a statistic. </span>

::: question
Can you provide another statistic?
:::

. . .

::: alert
<span style="color:blue"> Sample variance $\frac{\sum_{i=1}^n \left(X_i - \overline{X}\right)^2}{n-1}$ is a statistic. </span>
:::

. . .

- Since $X_1, X_2, \dots, X_n$ are random variables, *any transformation or function of $(X_1, X_2, \dots, X_n)$, or statistic, is also a random variable.* 


- The probability distribution of a statistic is called the **sampling distribution** of that statistic.

. . .

::: question
Does the sample mean $\overline{X} = \frac{1}{n}\sum_{i=1}^n X_i$ have a sampling distribution?
:::



## Sampling Distribution

- It is the probability distribution of that statistic if we *__were to repeatedly__ draw samples of the same size from the population*.

::: tiny
```{r}
#| fig-cap: "Biostatistics for the Biological and Health Sciences p.241"
knitr::include_graphics("images/08-sampling-dist-clt/sampling_dist_mean.png")
```
:::

## Sampling Distribution

- [Sampling Distribution Applet](https://www.statcrunch.com/applets/type3&samplingdist)

::: question
What are the differences between the sampling distribution of $\overline{X}$ and the population distribution each individual r.v. $X_i$ is drawn from?
:::

. . .

- Sample means $(\overline{X})$ are <span style="color:red"> **less variable** </span> than individual observations $X_i$.

- Sample means $(\overline{X})$ are <span style="color:red"> **more normal** </span> than individual observations $X_i$. 



::: notes
- [Sampling Distribution Applet](http://www.mscs.mu.edu/~mehdi/applets/sample_dist/index.html)
:::

## Example: Sampling Distribution of the Sample Mean

- Roll a fair die 3 times `r emo::ji('game_die')``r emo::ji('game_die')` `r emo::ji('game_die')` independently to obtain 3 values from the population $\{1, 2, 3, 4, 5, 6\}$.

- Repeat the process 10,000 times and plot the histogram of the sampling mean.

::::: columns

::: {.column width="50%"}

```{r}
plot(c("1", "2", "3", "4", "5", "6"), rep(1/6, 6), type = "h", ylim = c(0, 0.2),
     lwd = 5, col = "#003366", ylab = "P(X = x)", xlab = "x", las = 1, axes = F, 
     main = "Population Distribution")
abline(v = 3.5, col = "#FFCC00", lwd = 5)
axis(1, 1:6, c("1", "2", "3", "4", "5", "6"), col.axis = "black")
axis(2, c(0, 1/3, 1/6), c("0", "1/3", "1/6"), col.axis = "black", las = 2)
```
:::

::: {.column width="50%"}

```{r}
total_sample <- replicate(10000, sample(1:6, 3, replace = TRUE))
sample_of_sample_mean <- apply(total_sample, 2, mean)
hist(sample_of_sample_mean, main = "Sampling distribution of sample mean", 
     xlab = "sample mean", col = "#003366", border = "white", breaks = 60, las = 1)
abline(v = 3.5, col = "#FFCC00", lwd = 5)
legend("topright", c("population mean"), col = "#FFCC00", lwd = 5, bty = "n")
```
:::

:::::


##

<iframe src="https://cheyu.shinyapps.io/rolling-die/"
        width="100%" height="90%" style="border:none;">
</iframe>


## Sampling Distribution of Sample Mean

<!-- - Assume that the population distribution has mean $\mu$ and standard deviation $\sigma$. -->
- Suppose $(X_1, \dots, X_n)$ is the random sample from a population distribution with mean $\mu$ and standard deviation $\sigma$.

- The mean of the sampling distribution of the sample mean, $\overline{X} = \frac{\sum_{i=1}^nX_i}{n}$, is <span style="color:blue"> $\mu_{\overline{X}} = \mu$ </span>.

- The standard deviation of the sampling distribution of the sample mean $\overline{X}$ is <span style="color:blue"> $\sigma_{\overline{X}} = \frac{\sigma}{\sqrt{n}}$ </span>.
<!-- - $\sigma_{\overline{X}}$ is also known as the **standard error** of $\overline{X}$. -->

- If the population distribution is <span style="color:blue"> $N(\mu, \sigma^2)$ </span>, the sampling distribution of $\overline{X}$ is **exactly** <span style="color:blue"> $N\left(\mu, \frac{\sigma^2}{n} \right)$ </span>.

::: notes
$\sigma$.
- Suppose $(X_1, \dots, X_n)$ is the random sample from ach $X_i$ is sampled from the same population (same $\mu$ and same $\sigma$).
:::



## Sampling Distribution of Sample Mean Illustration

```{r}
par(mar = c(1, 0, 1.9, 0), mgp = c(0, 0.4, 0), las = 1)
x <- seq(-5,7,length=100)
hx <- dnorm(x, 1, 2)
n_vec <- c(2, 4, 8)
plot(x, hx, type="n", xlab="", ylab="", axes = FALSE, ylim = c(0, 0.8),
     main = "Sampling distribution of Sample Mean")
lines(x, hx, col = "black", lwd = 3)
lines(x, dnorm(x, 1, 2/sqrt(n_vec[1])), col = "red", lwd = 3)
lines(x, dnorm(x, 1, 2/sqrt(n_vec[2])), col = "blue", lwd = 3)
lines(x, dnorm(x, 1, 2/sqrt(n_vec[3])), col = "green", lwd = 3)
legend("topright", c("Population distribution: N(mean = 1, var = 4)", 
                     expression(paste("Sampling distributioin of ", bar(X), " (n = 2): N(1, 4/2)")), 
                     expression(paste("Sampling distributioin of ", bar(X), " (n = 4): N(1, 4/4)")),
                     expression(paste("Sampling distributioin of ", bar(X), " (n = 8): N(1, 4/8)"))),
       bty = "n", col = c("black", "red", "blue", "green"), lwd = c(3, 3, 3, 3))
# qq <- round(qnorm(c(0.5, 1.5, 2.5, 3.5, 4.5)/5), 2)
axis(1, pos=0, cex.axis = 1)
segments(1, 0, 1, dnorm(1, 1, 2/sqrt(n_vec[3])), lty = 2, lwd = 0.5)
# text(0,-0.05, expression(bold(mu[0])), col = "blue", cex = 1.5)

```


## Standardization of Sample Mean 

- For a single random variable $X \sim N(\mu, \sigma^2)$, $Z = \frac{X - \mu}{\sigma} \sim N(0, 1)$.

- For the sample mean of $n$ variables, $\overline{X} \sim N(\mu_{\overline{X}}, \sigma^2_{\overline{X}}) = N(\mu, \frac{\sigma^2}{n})$, and hence 

<span style="color:blue">$$Z = \frac{\overline{X} - \mu_{\overline{X}}}{\sigma_{\overline{X}}} = \frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \sim N(0, 1)$$</span>
  <!-- + The solution is the **Central Limit Theorem**! -->
  
  
## Example - Psychomotor retardation

::::: columns

::: {.column width="90%"}

- Psychomotor retardation scores for a group of patients have a normal distribution with a mean of 930 and a standard deviation of 130.

- What is the probability that the *mean* retardation score of a random sample of 20 patients was between 900 and 960?

:::

::: {.column width="10%"}

```{r}
knitr::include_graphics("./images/08-sampling-dist-clt/depression.jpeg")
```

:::

:::::

. . .

- $X_1, \dots, X_{20}  \stackrel{iid}{\sim} N(930, 130^2)$, then $\overline{X} = \frac{\sum_{i=1}^{20}X_i}{20} \sim N\left(930, \frac{130^2}{20} \right)$.

. . .

$$\small \begin{align} 
P(900 < \overline{X} < 960) &= P\left( \frac{900-930}{130/\sqrt{20}} < \frac{\overline{X}-930}{130/\sqrt{20}} < \frac{960-930}{130/\sqrt{20}}\right)=P(-1.03 < Z < 1.03)\\
&=P(Z < 1.03) - P(Z < -1.03)
  \end{align}$$

. . .

```{r}
#| echo: true
pnorm(1.03) - pnorm(-1.03)
```

. . .

```{r}
#| echo: true
pnorm(960, mean = 930, sd = 130/sqrt(20)) - pnorm(900, mean = 930, sd = 130/sqrt(20))
```


# [Central Limit Theorem]{.orange}{background-image="./images/08-sampling-dist-clt/magic.jpg" background-size="cover" background-position="50% 50%" background-color="#447099"}

## Central Limit Theorem

- If $X_i \stackrel{iid}{\sim} N(\mu, \sigma^2)$ , then $\overline{X} \sim N\left(\mu, \frac{\sigma^2}{n} \right)$.

- <span style="color:blue"> What if the population distribution is **NOT** normal? </span> 

. . .

<br>

<br>

<br>

::: center
::: large
The **central limit theorem (CLT)** gives us the answer!
:::
:::


## Why Use Normal? <span style="color: red">**Central Limit Theorem**</span>
**Central Limit Theorem (CLT)**: 

> Suppose $\overline{X}$ is from a random sample of size $n$ and from a population distribution having mean $\mu$ and standard deviation $\sigma < \infty$. <br>
As $n$ increases, the sampling distribution of $\overline{X}$ looks **more and more like $N(\mu, \sigma^2/n)$, regardless of the distribution from which we are sampling!**

::: tiny
```{r}
#| fig-cap: "https://en.wikipedia.org/wiki/Central_limit_theorem#/media/File:IllustrationCentralTheorem.png"
#| out-width: 67%
knitr::include_graphics("./images/08-sampling-dist-clt/clt.png")
```
:::


## CLT Illustration: A Right-Skewed Distribution

```{r}
#| fig-asp: 0.3
par(mfrow = c(1, 4))
par(mar = c(3, 1, 1, 0), mgp = c(1.5, 0.5, 0), las = 1)
shape <- 1
rate <- 1/2
# gamma_sample <- rgamma(1000, shape = shape, rate = rate)
# hist(gamma_sample, breaks = 30, col = "#FFCC00", border = "white", xlab = "x",
#      main = "Population distr. w/ mean 2 var 4", prob = TRUE)
x <- seq(0, 12, length.out = 1000)
plot(x, dgamma(x, shape = shape, rate = rate), type="n", xlab="x", ylab="", axes = TRUE, ylim = c(0, 0.6),
     main = "Population distr. w/ mean 2 var 4", cex.main = 0.8)
lines(x, dgamma(x, shape = shape, rate = rate), col = "#003366", lwd = 3)
polygon(c(x,rev(x)), c(rep(0, 1000), rev(dgamma(x, shape = shape, rate = rate))), col="#FFCC00")

# lines(density(gamma_sample), col = "red", lwd = 4) 
n_s <- 2
n_m <- 10
n_l <- 1000
sample_s <- replicate(10000, rgamma(n_s, shape = shape, rate = rate))
sample_m <- replicate(10000, rgamma(n_m, shape = shape, rate = rate))
sample_l <- replicate(10000, rgamma(n_l, shape= shape, rate = rate))
sample_mean_s <- apply(sample_s, 2, mean)
sample_mean_m <- apply(sample_m, 2, mean)
sample_mean_l <- apply(sample_l, 2, mean)
x_s <- seq(-6, 8, length.out = 1000)
x_m <- seq(-1, 5, length.out = 1000)
x_l <- seq(1.7, 2.3, length.out = 1000)
hist(sample_mean_s, breaks = 30, col = 4, border = "white", xlim = range(x_s),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_s, ")"),
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
# lines(x_s, dnorm(x_s, shape/rate, sqrt(shape/rate^2)/sqrt(n_s)), lwd = 3, col = "red")
# lines(density(sample_mean_s), col = "red", lwd = 4) 
hist(sample_mean_m, breaks = 30, col = 4, border = "white", xlim = range(x_m),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_m, ")"), 
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
# lines(x_m, dnorm(x_m, shape/rate, sqrt(shape/rate^2)/sqrt(n_m)), lwd = 3, col = "red")
# lines(density(sample_mean_m), col = "red", lwd = 4) 
hist(sample_mean_l, breaks = 30, col = 4, border = "white", xlim = range(x_l),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_l, ")"), 
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
lines(x_l, dnorm(x_l, shape/rate, sqrt(shape/rate^2)/sqrt(n_l)), lwd = 1, col = "red")
# lines(density(sample_mean_l), col = "red", lwd = 4) 
# paste("Mean of the sample mean of size 1000 =", mean(sample_mean_l))
# paste("Variance of the sample mean of size 1000 =", var(sample_mean_l))
# paste("True mean of the sample mean =", shape / rate)
# paste("True variance of the sample mean =", (shape / rate ^ 2) / n_l)
```



## CLT Illustration: A U-shaped Distribution

```{r}
#| fig-asp: 0.3
par(mfrow = c(1, 4))
par(mar = c(3, 1, 1, 0), mgp = c(1.5, 0.5, 0), las = 1)
shape1 <- 1/2
shape2 <- 1/2
x <- seq(0, 1, length.out = 1000)
plot(x, dbeta(x, shape1 = shape1, shape2 = shape2), type="n", xlab="x", ylab="", axes = TRUE, ylim = c(0, 3), cex.main = 0.8,
     main = "Popu. distr. w/ mu 0.5 var 0.125")
lines(x, dbeta(x, shape1 = shape1, shape2 = shape2), col = "#003366", lwd = 3)
# polygon(c(x, rev(x)), c(rev(rep(0, 1000)), (dbeta(x, shape1 = shape1, shape2 = shape2))), col="#FFCC00")

# lines(density(gamma_sample), col = "red", lwd = 4) 
n_s <- 2
n_m <- 10
n_l <- 1000
sample_s <- replicate(10000, rbeta(n_s, shape1 = shape1, shape2 = shape2))
sample_m <- replicate(10000, rbeta(n_m, shape1 = shape1, shape2 = shape2))
sample_l <- replicate(10000, rbeta(n_l, shape1 = shape1, shape2 = shape2))
sample_mean_s <- apply(sample_s, 2, mean)
sample_mean_m <- apply(sample_m, 2, mean)
sample_mean_l <- apply(sample_l, 2, mean)
x_s <- seq(-0.5, 1.5, length.out = 1000)
x_m <- seq(0, 1, length.out = 1000)
x_l <- seq(0.45, 0.55, length.out = 1000)
hist(sample_mean_s, breaks = 20, col = 4, border = "white", xlim = range(x_s),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_s, ")"),
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
# lines(x_s, dnorm(x_s, shape1/(shape1+shape2), sqrt(0.125)/sqrt(n_s)), lwd = 3, col = "red")
# lines(density(sample_mean_s), col = "red", lwd = 4) 
hist(sample_mean_m, breaks = 20, col = 4, border = "white", xlim = range(x_m),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_m, ")"), 
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
# lines(x_m, dnorm(x_m, shape1/(shape1+shape2), sqrt(0.125)/sqrt(n_m)), lwd = 3, col = "red")
# lines(density(sample_mean_m), col = "red", lwd = 4) 
hist(sample_mean_l, breaks = 20, col = 4, border = "white", xlim = range(x_l),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_l, ")"), 
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
lines(x_l, dnorm(x_l, shape1/(shape1+shape2), sqrt(0.125)/sqrt(n_l)), lwd = 1, col = "red")
# lines(density(sample_mean_l), col = "red", lwd = 4) 
# paste("Mean of the sample mean of size 1000 =", mean(sample_mean_l))
# paste("Variance of the sample mean of size 1000 =", var(sample_mean_l))
# paste("Theoretical mean of the sample mean =", shape1/(shape1+shape2))
# paste("Theoretical variance of the sample mean =", 0.125 / n_l)
```



<!-- ## Central Limit Theorem Applet -->

<!-- {{< include prob-clt-ojs.qmd >}} -->



## Why CLT is Important?

<!-- - Many statistics have distributions that are approximately normal for large sample sizes, even when we are sampling from a distribution that is not normal. -->
- Many well-developed statistical methods are based on normal distribution assumption.

- With CLT, we can use those methods even if we are sampling from a non-normal distribution, or we have no idea of the population distribution, provided that the sample size is large.


## CLT Example


::::: columns

::: {.column width="85%"}

- Suppose that selling prices of houses in Milwaukee are known to have a mean of $382,000 and a standard deviation of $150,000.

- In 100 randomly selected sales, what is the probability the *average* selling price is more than $400,000?

:::

::: {.column width="15%"}

```{r}
knitr::include_graphics("./images/08-sampling-dist-clt/house.jpeg")
```

:::

:::::

. . .

<!-- - We need the sampling distribution of the average selling price in order to obtain the probability.  -->
- Since the sample size is fairly large $(n = 100)$, by CLT, the sampling distribution of the average selling price is approximately normal with mean 382,000 and SD $150,000 / \sqrt{100}$.

- $P(\overline{X} > 400000) = P\left(\frac{\overline{X} - 382000}{150000/\sqrt{100}}  > \frac{400000 - 382000}{150000/\sqrt{100}}\right) \approx P(Z > 1.2)$ where $Z \sim N(0, 1)$.

```{r}
#| echo: true
pnorm(1.2, lower.tail = FALSE)
pnorm(400000, mean = 382000, sd = 150000/sqrt(100), lower.tail = FALSE)
```






