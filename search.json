[
  {
    "objectID": "present-work-2.html",
    "href": "present-work-2.html",
    "title": "Midterm Project II Proposal and Presentation",
    "section": "",
    "text": "The presentation order is determined by the time you sent me your member info (c(\"John\", \"Jeremy\", \"Praful\") is the first to let me know, followed by c(\"Sajjad\", \"Tanjina\", \"Dewan\"), etc) and random sampling as follows.\n\nteam_lst &lt;- list(c(\"John\", \"Jeremy\", \"Praful\"), \n                 c(\"Sajjad\", \"Tanjina\", \"Dewan\"),\n                 c(\"Ethan\", \"Navid\", \"Sylvester\"), \n                 c(\"Violet\", \"Vanessa\", \"Michele\"), \n                 c(\"Rakesh\", \"Daniel\", \"Jeremy\"), \n                 c(\"Sai\", \"Rohith\", \"Shristi\"))\nset.seed(2025)\nteam_lst[sample(1:6, 6)]\n\n[[1]]\n[1] \"Rakesh\" \"Daniel\" \"Jeremy\"\n\n[[2]]\n[1] \"Violet\"  \"Vanessa\" \"Michele\"\n\n[[3]]\n[1] \"Sai\"     \"Rohith\"  \"Shristi\"\n\n[[4]]\n[1] \"Sajjad\"  \"Tanjina\" \"Dewan\"  \n\n[[5]]\n[1] \"John\"   \"Jeremy\" \"Praful\"\n\n[[6]]\n[1] \"Ethan\"     \"Navid\"     \"Sylvester\""
  },
  {
    "objectID": "present-work-2.html#presentation-order",
    "href": "present-work-2.html#presentation-order",
    "title": "Midterm Project II Proposal and Presentation",
    "section": "",
    "text": "The presentation order is determined by the time you sent me your member info (c(\"John\", \"Jeremy\", \"Praful\") is the first to let me know, followed by c(\"Sajjad\", \"Tanjina\", \"Dewan\"), etc) and random sampling as follows.\n\nteam_lst &lt;- list(c(\"John\", \"Jeremy\", \"Praful\"), \n                 c(\"Sajjad\", \"Tanjina\", \"Dewan\"),\n                 c(\"Ethan\", \"Navid\", \"Sylvester\"), \n                 c(\"Violet\", \"Vanessa\", \"Michele\"), \n                 c(\"Rakesh\", \"Daniel\", \"Jeremy\"), \n                 c(\"Sai\", \"Rohith\", \"Shristi\"))\nset.seed(2025)\nteam_lst[sample(1:6, 6)]\n\n[[1]]\n[1] \"Rakesh\" \"Daniel\" \"Jeremy\"\n\n[[2]]\n[1] \"Violet\"  \"Vanessa\" \"Michele\"\n\n[[3]]\n[1] \"Sai\"     \"Rohith\"  \"Shristi\"\n\n[[4]]\n[1] \"Sajjad\"  \"Tanjina\" \"Dewan\"  \n\n[[5]]\n[1] \"John\"   \"Jeremy\" \"Praful\"\n\n[[6]]\n[1] \"Ethan\"     \"Navid\"     \"Sylvester\""
  },
  {
    "objectID": "present-work-2.html#project-materials",
    "href": "present-work-2.html#project-materials",
    "title": "Midterm Project II Proposal and Presentation",
    "section": "Project Materials",
    "text": "Project Materials\n\nGroup 1 (Rakesh, Daniel, Jeremy): proposal\nGroup 2 (Violet, Vanessa, Michele): proposal\nGroup 3 (Sai, Rohith, Shristi) : proposal\nGroup 4 (Sajjad, Tanjina, Dewan): proposal\nGroup 5 (John, Jeremy, Praful): proposal\nGroup 6 (Ethan, Navid, Sylvester): proposal"
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Syllabus for Section 101 2 - 3:15 PM.\nSyllabus for Section 102 3:30 - 4:45 PM.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#time-and-location",
    "href": "course-syllabus.html#time-and-location",
    "title": "Syllabus",
    "section": "Time and location",
    "text": "Time and location\n\n\n\nDay\nTime\nLocation\n\n\n\nSec 101\nMo & We\n2:00 - 3:15 PM\nCudahy Hall 131\n\n\nSec 102\nMo & We\n3:30 - 4:45 PM\nCudahy Hall 131",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#office-hours",
    "href": "course-syllabus.html#office-hours",
    "title": "Syllabus",
    "section": "Office Hours",
    "text": "Office Hours\n\nMy in-person office hours are MoWe 4:50 - 5:50 PM and Tu 1 - 2 PM in Cudahy Hall room 353.\nYou are welcome to schedule an online meeting via Microsoft Teams if you need/prefer.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nMATH 1400, 1410, or 1450.\nBasic computer and internet use expected. The course will also assume facility with using the internet and a personal computer.\nA portion of the course involves programming in  using RStudio, but prior programming experience is not required.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#e-mail-policy",
    "href": "course-syllabus.html#e-mail-policy",
    "title": "Syllabus",
    "section": "E-mail Policy",
    "text": "E-mail Policy\n\nI will attempt to reply your email quickly, at least within 24 hours.\nExpect a reply on Monday if you send a question during weekends. If you do not receive a response from me within two days, re-send your question/comment in case there was a ‚Äúmix-up‚Äù with email communication (Hope this won‚Äôt happen!).\nPlease start your e-mail subject line with [math4720] or [mssc5720] followed by a clear description of your question. See an example below.\n\n\n\n\n\nEmail Subject Line Example\n\n\n\n\nEmail etiquette is important. Please read this article to learn more about email etiquette.\n\nI am more than happy to answer your questions about this course or statistics in general. However, due to time constraint, I may choose NOT to respond to students‚Äô e-mail if\n\nThe student could answer his/her own inquiry by reading the syllabus or information on the course website or D2L.\nThe student is asking for an extra credit opportunity. The answer is ‚Äúno‚Äù.\nThe student is requesting an extension on homework. The answer is ‚Äúno‚Äù.\nThe student is asking for a grade to be raised for no legitimate reason. The answer is ‚Äúno‚Äù.\nThe student is sending an email with no etiquette.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#required-textbook",
    "href": "course-syllabus.html#required-textbook",
    "title": "Syllabus",
    "section": "Required Textbook",
    "text": "Required Textbook\n\n\n[IS] Introduction to Statistics, by Dr.¬†Cheng-Han Yu. (My online book)",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#optional-references",
    "href": "course-syllabus.html#optional-references",
    "title": "Syllabus",
    "section": "Optional References",
    "text": "Optional References\n\n\n[OI] (optional) Introduction to Modern Statistics, 2nd edition, by Mine √áetinkaya-Rundel and Johanna Hardin. Publisher: OpenIntro. (computation and data-oriented)",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#grading-policy",
    "href": "course-syllabus.html#grading-policy",
    "title": "Syllabus",
    "section": "Grading Policy",
    "text": "Grading Policy\n\n\nThe final grade is earned out of 1000 total points distributed as follows:\n\nHomework 1 to 8: 175 pts (25 pts each)\nQuiz 1 to 4: 200 pts (50 pts each)\nExam 1 and 2: 300 pts (150 pts each)\nFinal exam: 150 pts\nProject: 150 pts\nClass participation: 25 pts\n\n\nYou will NOT be allowed any extra credit projects/homework/exam to compensate for a poor average. Everyone must be given the same opportunity to do well in this class. Individual exam will NOT be curved. \nThe final grade is based on your percentage of points earned out of 1000 points and the grade-percentage conversion Table. \\([x, y)\\) means greater than or equal to \\(x\\) and less than \\(y\\). For example, 94.1 is in \\([94, 100]\\) and the grade is A and 92.8 is in \\([90, 94)\\) and the grade is A-.\n\n\n\n\nGrade-Percentage Conversion\n\nGrade\nPercentage\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[73, 77)\n\n\nC-\n[70, 73)\n\n\nD+\n[65, 70)\n\n\nD\n[60, 65)\n\n\nF\n[0, 60)\n\n\n\n\n\n\nThis is not a course that gives most of students grade A. If you want to obtain a good grade, study hard. No pain, no gain.\n\nHomework\n\nHomework will be assigned through the course website.\nTo submit your homework, please go to D2L &gt; Assessments &gt; Dropbox and upload your homework in PDF format.\nThere will be 8 homework sets.\nEvery homework is due by Friday 11:59 PM  (Don‚Äôt miss it. This is a hard deadline. No late submission).\nMSSC 5720 students may have more or different homework questions.\nThe lowest score of the homework sets will be dropped when your final grade is calculated.\nGenerative AI (GenAI) is allowed, but you must carefully cite it or reveal your use of AI. See Section¬†8 for more details.\n\nQuizzes\n\nThere will be 4 in-class 15-min quizzes.\nQuiz questions are similar to the questions in the homework assignments.\nQuizzes are individual and in closed-book and no-tech format, except a calculator.\n‚ùå No cheat sheet or GenAI is allowed.\nNo make-up quizzes unless you got an excused absence.\nIf you miss a quiz due to an excused absence as defined in Attendance in Academic Regulations, the 50 pts will be added to your prorated final exam pts, i.e., (150 + 50 = 250) pts. If you miss more than one quiz, only one quiz pts can be added to the final exam. You get 0 pt for other quizzes.\nExams\n\nThere will be 2 midterm exams and 1 final exam.\nMidterm exams have in-class part and take-home part. The in-class part tests your understanding of mathematical and statistical intuitions. The take-home part tests your ability to do statistical data analysis using statistical software such as R.\nFor the in-class parts, one piece of letter size cheat sheet is allowed. It has to be turned-in with your exam.\nFor the take-home parts, you are allowed to use GenAI tools. However, you must carefully cite it or reveal your use of AI. See Section¬†8 for more details. No late submission is allowed.\nPlease go to Assessments &gt; Dropbox to submit your take-home exam in PDF format.\nExam 1 covers Week 1 to 6. Exam 2 covers Week 7 to 11. In-class final exam is comprehensive and covers all course materials.\nNo make-up exams for any reason unless you got an excused absence\nIf you miss an midterm exam due to an excused absence defined in Attendance in Academic Regulations, the 150 pts will be added to your prorated final exam pts, i.e., (150 + 150 = 300) pts. If you miss two midterm exams, only one exam pts can be added to the final exam. You get 0 pt for the other.\nAI Project\n\nThere will be one (or two?) project released around Week 13.\nYou have to use GenAI to self learn a topic that is not covered in my lectures, and generate a data analysis report.\nThe project topic will also be tested in the final exam.\nYou must carefully cite your GenAI tool or reveal your use of AI. See Section¬†8 for more details.\nNo late submission is allowed.\nClass Participation\n\nI will randomly take attendance throughout the semester.\nIf you are absent on a day when attendance is taken once (twice), 15 (25) points will be deducted from your grade unless you have excused absences.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#sec-genai",
    "href": "course-syllabus.html#sec-genai",
    "title": "Syllabus",
    "section": "Generative Artificial Intelligence (GenAI) Policy",
    "text": "Generative Artificial Intelligence (GenAI) Policy\n\nYou are responsible for the content of all work submitted for this course.\nFor your homework, take-home exams, and project, you are allowed to use generative AI tools such as ChatGPT to generate a draft of text of your work.\n\nTo avoid any academic integrity issue, you must cite your AI usage, or screenshot your entire AI usage history. Check the followings on how to cite it.\n\nHow to cite ChatGPT\nHow to Cite AI-Generated Content\n\n\n\nIf you use GenAI, please include the followings in your submitted work:\n\nWhy/How I used AI (prompts or questions)\nGenerated output (screenshot or copy-paste excerpt)\nHow I used the output\n\n\n\nHere is an example.\n\n\nWhy/How I used AI (prompts and questions)\n\nI asked ChatGPT to generate a histogram using R.\n\n\nGenerated output (screenshot or copy-paste excerpt)\n\n\n\n\n\n\n\n\n\n\n\nHow I used the output\n\nI reviewed the suggestions, but I did not use the exact code. Instead, I change the code format and breaks value to 50.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#academic-integrity",
    "href": "course-syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\n\nWatch the video below to learn the importance of using GenAI properly.\n\n\n  \n\n\nThis course expects all students to follow University and College statements on academic integrity.\nHonor Pledge and Honor Code: I recognize the importance of personal integrity in all aspects of life and work. I commit myself to truthfulness, honor, and responsibility, by which I earn the respect of others. I support the development of good character, and commit myself to uphold the highest standards of academic integrity as an important aspect of personal integrity. My commitment obliges me to conduct myself according to the Marquette University Honor Code.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#accommodation",
    "href": "course-syllabus.html#accommodation",
    "title": "Syllabus",
    "section": "Accommodation",
    "text": "Accommodation\nIf you need to request accommodations, or modify existing accommodations that address disability-related needs, please contact Disability Service.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\n\nSep 1: Labor day\n\nSep 2: Last day to add/swap/drop\n\nOct 6: In-class Exam 1\n\nOct 16-17: Midterm break\n\nOct 21: Midterm grade submission\n\nNov 10: In-class Exam 2\n\nNov 14: Withdrawal deadline\n\nNov 26 - 30: Thanksgiving break\n\nDec 6: Last day of class\n\nDec 10: In-class Final Exam (Sec 101 class time 2 PM)\n\nDec 12: In-class Final Exam (Sec 102 class time 3:30 PM)\n\nDec 16: Final grade submission\n\nClick here for the full Marquette academic calendar.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "hw.html",
    "href": "hw.html",
    "title": "Homework",
    "section": "",
    "text": "More homework sets to be added as the semester progresses.\n\n\n\n\n\n\n\n\n\n\nNo.\n\n\n\nTitle\n\n\n\nDue date\n\n\n\n\n\n\n\n\nHappy Halloween!üéÉüëª\n\n\nExtra credit opportunity\n\n\n\n\n\n\n\n\n\nHomework 1\n\n\nData Collection and Data Type\n\n\n\n\n\n\n\n\n\nHomework 2\n\n\nBasic R, Data Description and Graphics\n\n\n\n\n\n\n\n\n\nHomework 3\n\n\nProbability Fundamentals\n\n\n\n\n\n\n\n\n\nHomework 4\n\n\nProbability Distributions\n\n\n\n\n\n\n\n\n\nHomework 5\n\n\nInferences about population means\n\n\n\n\n\n\n\n\n\nHomework 6\n\n\nComparing Two Population Means and Inference about Variances\n\n\n\n\n\n\n\n\n\nHomework 7\n\n\nInference about Variances\n\n\n\n\n\n\n\n\n\nHomework 8\n\n\nCorrelation and Linear Regression\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Homework"
    ]
  },
  {
    "objectID": "exam/take_home_exam1.html",
    "href": "exam/take_home_exam1.html",
    "title": "Take-home Exam 1",
    "section": "",
    "text": "Important\n\n\n\nDue Wednesday, Oct 8, 11:59 PM\n\n\n\nPlease submit your work in one PDF file to D2L &gt; Assessments &gt; Dropbox. Multiple files or a file that is not in pdf format is not allowed.\nWith your submission, you commit to academic integrity through the following honor pledge:\n\n\nI recognize the importance of personal integrity in all aspects of life and work. I commit myself to truthfulness, honor and responsibility, by which I earn the respect of others. I support the development of good character and commit myself to uphold the highest standards of academic integrity as an important aspect of personal integrity. My commitment obliges me to conduct myself according to the Marquette University Honor Code.\n\n\nThis exam is to be entirely your own efforts. You may use any resources, including Generative AI tools, while working on this exam. However, you are NOT allowed to discuss the exam with anyone except Dr.¬†Yu, nor may you post any of the exam questions on any platform to solicit answers. Any violation of this policy will be treated as academic misconduct. If you are caught cheating, your case will be reported immediately to the Academic Integrity Council.\nIn your exam, please number problems/questions in order.\nEven if you use R to compute your final answer, you must show your calculation steps and/or explain your reasoning in order to receive full credit.\nHandwritten tables and figures receive no credits.\n\n\nQuestions started with (MSSC) are required for MSSC 5720 students, and optional for MATH 4720 students.\n\nExam Problems (50 points)\n\n\nNormal Approximation to Binomial. Suppose that 60% of Americans have had chickenpox by the time they reach adulthood.\n\n\nSuppose we take a random sample of 6 American adults. Let \\(X\\) be the number of people out of the 6 who had chickenpox during childhood. If \\(X \\sim binomial(n, \\pi)\\), determine the value of \\(n\\) and \\(\\pi\\).\nWith (a), compute the probability that at least 3 adults have had chickenpox.\nWith (a), what is the value of the mean \\(\\mu\\) and variance \\(\\sigma^2\\) of \\(X \\sim binomial(n, \\pi)\\)?\nNow treat \\(X\\) as a normal random variable, i.e., \\(X \\sim N(\\mu, \\sigma^2)\\), where \\(\\mu\\) and \\(\\sigma^2\\) are the values obtained in (c). Compute the probability that at least 3 adults have had chickenpox.\nSuppose now we take a random sample of 60 American adults. Using \\(binomial(n, \\pi)\\) and \\(N(\\mu, \\sigma^2)\\) with new parameter values \\(n\\), \\(\\mu\\), and \\(\\sigma^2\\) to compute the probability that at least 30 adults have had chickenpox.\nCompare binomial and normal probabilities when sample size are 6 and 60. In which case, binomial and normal probabilities are closer?\n(MSSC) Is there any issue when we use a normal distribution to approximate a binomial distribution? Use continuity correction on (e). Do you get a better approximation result?\n\n\n\nAs part of a study to determine factors that may explain differences in animal species relative to their size, the following body masses (in grams) of 50 different bird species were reported in the paper ``Temperature and the Northern Distributions of Wintering Birds,‚Äô‚Äô by Richard Repasky (1991). The data are provided in exam1data.csv. First download and upload the data to your RStudio. Then import the data using the following command like\n\n\ndata &lt;- read.csv(\"./exam1data.csv\")\n\nwhere \"./exam1data.csv\" is the file path and ./ means your current working directory.\n\nMake a histogram of bodymass.\nConstruct a boxplot with label X-axis or Y-axis (depending on whether the plot is horizontal or vertical) as Body Mass (in grams) with a title Boxplot of Body Mass (g). Are there any outliers in the data set? If yes, identify them.\nFind 30-th, 60-th and 90-th percentiles (or 0.3, 0.6 and 0.9 quantiles) as wells as the sample mean and median. Based on (a), (b), and (c), comment the skewness of the data, and which should be a better measure of center.\n\nMany statistical methods assume data are normally distributed, and the Quantile-Quantile plot or QQ plot helps us check the normality assumption. If the data are approximately normally distributed, the points on the QQ plot will lie close to a straight line.\n\nRemove the outliers you identified in (b) and construct a QQ-plot for the data by using the functions qqnorm() and qqline(). Does the distribution without outliers look like normal? Please comment.\nTransform the data with outliers removed by taking the natural log with base \\(e = 2.71828...\\). Construct a QQ-plot for the transformed data and comment on its normality.\n\n\n\n\nAI Usage Declaration. Using GenAI is permitted for this course. If you choose to use GenAI to assist with your exam, you must include a brief statement documenting your use. Please provide the following information:\n\n\nWhy/How I Used AI Why do you need to use GenAI? Which tool did you use? Describe your prompts or questions. What and how did you ask the AI to help you?\nGenerated Output Include a screenshot or excerpt (copy and paste) of the AI‚Äôs response.\nHow I Used the Output Did you revise it? Did you use it directly, or compare it with your answers? What decisions did you make based on the output?"
  },
  {
    "objectID": "exercise/exercise_mlr.html",
    "href": "exercise/exercise_mlr.html",
    "title": "Key Topics of Multiple Linear Regression",
    "section": "",
    "text": "The meaning of multiple linear regression and fitted regression ‚Äúline‚Äù\nInterpretation of coefficients\nConfidence interval and hypothesis testing for individual coefficients\nANOVA table and overall significance testing\nRegression diagnostics and model assumption checking"
  },
  {
    "objectID": "exercise/exercise3.html",
    "href": "exercise/exercise3.html",
    "title": "Exercise 3",
    "section": "",
    "text": "About 30% of human twins are identical, and the rest are fraternal. Identical twins are necessarily the same sex: half are males and the other half are females. One-quarter of fraternal twins are both male, one-quarter both female, and one-half are mixes: one male, one female. You have just become a parent of twins and are told they are both girls. Given this information, what is the probability that they are identical?\n\n\n## [(0.5)(0.3)]/[(0.5)(0.3)+(1/4)(0.7)]\n\n\n\nA patient named Diana was diagnosed with Fibromyalgia, a long-term syndrome of body pain, and was prescribed anti-depressants. Being the skeptic that she is, Diana didn‚Äôt initially believe that anti-depressants would help her symptoms. However after a couple months of being on the medication she decides that the anti-depressants are working, because she feels like her symptoms are in fact getting better.\n\nWrite the hypotheses in words for Diana‚Äôs skeptical position when she started taking the anti-depressants.\nWhat is a Type 1 Error in this context?\nWhat is a Type 2 Error in this context?\n\n\n\n\n## (a)\n## H0: Anti-depressants do not affect the symptoms of Fibromyalgia. \n## HA: Anti-depressants do affect the symptoms of Fibromyalgia (either helping or harming).\n## (b)\n## Concluding that anti-depressants either help or worsen Fibromyalgia symptoms \n## when they actually do neither.\n## (c)\n## Concluding that anti-depressants do not affect Fibromyalgia symptoms when they actually do.\n\n\n\nA Rasmussen Reports survey of 1,000 US adults found that 42% believe raising the minimum wage will help the economy. Construct a 99% confidence interval for the true proportion of US adults who believe this.\n\n\n# 0.42 -+ 2.58 (0.016)\n\n\n\n\nTrue or false.\n\nThe chi-square distribution, just like the normal distribution, has two parameters, mean and standard deviation.\nConsider two sets of data that are paired with each other. Each observation in one data set has a natural correspondence with exactly one observation from the other data set.\nAs the degrees of freedom increases, the \\(t\\)-distribution approaches normality.\nA correlation coefficient of -0.90 indicates a stronger linear relationship than a correlation of 0.5.\n\n\n\n\n## F\n## T\n## T\n## T\n\n\n\nConsider a regression predicting weight (kg) from height (cm) for a sample of adult males. What are the units of the correlation coefficient, the intercept, and the slope?\n\n\n# Correlation: no units. Intercept: kg. Slope: kg/cm.\n\n\n\nDetermine if I or II is higher or if they are equal. Explain your reasoning. For a regression line, the uncertainty associated with the slope estimate, \\(b_1\\), is higher when\n\n\n[I] there is a lot of scatter around the regression line\n[II] there is very little scatter around the regression line\n\n\n# I is higher, the more the scatter the lower the correlation coefficient, and hence the higher the uncertainty around the regression line.\n\n\n\nIs the gestational age (time between conception and birth) of a low birth-weight baby useful in predicting head circumference at birth? Twenty-five low birth-weight babies were studied at a Harvard teaching hospital; the investigators calculated the regression of head circumference (measured in centimeters) against gestational age (measured in weeks). The estimated regression line is \\[\\widehat{\\text{head circumference}} = 3.91 + 0.78 \\times \\text{gestational age}\\]\n\nWhat is the predicted head circumference for a baby whose gestational age is 28 weeks?\nThe standard error for the coefficient of gestational age is 0. 35, which is associated with \\(df = 23\\). Does the model provide strong evidence that gestational age is significantly associated with head circumference?\n\n\n\n\n# 3.91 + 0.78 * 28 = 25.75\n# t_test = 2.23 p-value = 0.02\n\n\n\nLipitor (atorvastatin) is a drug used to control cholesterol. In clinical trials of Lipitor, 98 subjects were treated with Lipitor and 245 subjects were given a placebo. Among those treated with Lipitor, 6 developed infections. Among those given a placebo, 24 developed infections. Use a 0.05 significance level to test the claim that the rate of inflections was the same for those treated with Lipitor and those given a placebo.\n\nTest the claim using the critical-value and p-value methods.\nTest the claim by constructing a confidence interval.\n\n\n\n\n(prop_test &lt;- prop.test(x = c(6, 24), n = c(98, 245), \n          alternative = \"two.sided\", correct = FALSE))\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  c(6, 24) out of c(98, 245)\nX-squared = 1.1835, df = 1, p-value = 0.2766\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.09705436  0.02358497\nsample estimates:\n    prop 1     prop 2 \n0.06122449 0.09795918 \n\n## test statistic = -1.09\n## critical value = -1.96\n## p-value = 0.2766\n## CI = (-0.097, 0.0234)\n\n\n\nA researcher has developed a model for predicting eye color. After examining a random sample of parents, she predicts the eye color of the first child. The table below lists the eye colors of offspring. On the basis of her theory, she predicted that 87% of the offspring would have brown eyes, 8% would have blue eyes, and 5% would have green eyes. Use 0.05 significance level to test the claim that the actual frequencies correspond to her predicted distribution.\n\n\n\nEye Color\nBrown\nBlue\nGreen\n\n\nFrequency\n127\n21\n5\n\n\n\nobs &lt;- c(127, 21, 5)\npi_0 &lt;- c(0.87, 0.08, 0.05)\nchisq.test(x = obs, p = pi_0)\n\n\n    Chi-squared test for given probabilities\n\ndata:  obs\nX-squared = 7.4678, df = 2, p-value = 0.0239\n\n## critical value = 5.99\n\n\n\nIn a study of high school students at least 16 years of age, researchers obtained survey results summarized in the accompanying table. Use a 0.05 significance level to test the claim of independence between texting while driving and driving when drinking alcohol. Are these two risky behaviors independent of one another?\n\n\n\n\nDrove after drinking alcohol?\n\n\n\n\n\nYes\nNo\n\n\nTexted while driving\n720\n3027\n\n\nDid not text while driving\n145\n4472\n\n\n\n\n(contingency_table &lt;- matrix(c(720, 145, 3027, 4472), nrow = 2, ncol = 2))\n\n     [,1] [,2]\n[1,]  720 3027\n[2,]  145 4472\n\n(ind_test &lt;- chisq.test(x = contingency_table))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  contingency_table\nX-squared = 574.68, df = 1, p-value &lt; 2.2e-16\n\nind_test$expected\n\n         [,1]     [,2]\n[1,] 387.5126 3359.487\n[2,] 477.4874 4139.513\n\nqchisq(0.05, df = (2 - 1) * (2 - 1), lower.tail = FALSE)  \n\n[1] 3.841459"
  },
  {
    "objectID": "project/project.html",
    "href": "project/project.html",
    "title": "Learning Statistics using Generative AI",
    "section": "",
    "text": "Important\n\n\n\nDue Friday, Nov 28, 11:59 PM but extendable\nThis project allows you to demonstrate your understanding of multiple linear regression and your ability to interpret model results in context. You will explore a simulated health related data set with one response variable (stress) and two predictors (sleep and caffeine). The final product is a clearly written statistical analysis report.\nBy completing this project you will:",
    "crumbs": [
      "Learning Statistics using Generative AI"
    ]
  },
  {
    "objectID": "project/project.html#data-analysis-report",
    "href": "project/project.html#data-analysis-report",
    "title": "Learning Statistics using Generative AI",
    "section": "Data Analysis Report",
    "text": "Data Analysis Report\n\nThe data analysis report of multiple linear regression, including relevant tables and figures, should meet the following formatting requirements\n\nLength: 3 to 5 pages\nSpacing: single space\nFont: Times New Roman, 12 pt\nMargins: one inch on all sides\n\nPlease do not show any code in the report.\n\n\nRequired sections\nPlease use the provided project template to complete your report.\n1. Introduction\nBriefly introduce the topic. Define the variables. State the objective of the analysis.\n2. Exploratory Data Analysis\nInclude scatterplots of stress versus sleep, stress versus caffeine, and sleep versus caffeine. Write short descriptions of what you observe.\n3. Model Building\nFit a regression model that predicts stress from sleep and caffeine. Include the full model output.\n4. Interpretation of Results\nInterpret each coefficient in context. Explain signs, magnitudes, and meaning.\n5. Evaluation of Model Fit\nReport and interpret R squared and other relevant information. Comment on model quality and limitations.\n6. Conclusion\nWrite a clear summary of your findings in ordinary language.\n\n\nData set\nThe data set can be downloaded here. This data set contains three numerical variables measured for a group of individuals. Each variable is described below.\n\nVariable sleep\n\nDescription: Average number of hours of sleep per night during a typical week\nUnit: Hours\nTypical Range: About 5.0 to 8.5 hours\n\n\n\nVariable caffeine\n\nDescription: Average daily caffeine intake from all sources including coffee, tea, soda, or energy drinks\nUnit: Milligrams of caffeine per day\nTypical Range: About 65 to 290 milligrams\n\n\n\nVariable stress\n\nDescription: Perceived stress score based on a short survey. Higher values indicate greater stress.\nScale Range: 0 to 40\nTypical Range in this sample: About 11 to 33\n\nStress is a common concern among adults and college students. Many people believe that lifestyle choices such as sleep and caffeine consumption can influence how stressed a person feels. Sleep is often thought to reduce stress because it allows the body and mind to recover. Caffeine is often used to stay alert, but higher caffeine intake may increase feelings of tension or restlessness.\nResearchers are interested in understanding how these everyday behaviors relate to stress levels. In real health studies, researchers often collect data on sleep habits, caffeine consumption, and stress scores to explore these relationships. Multiple linear regression provides a useful way to examine how two different predictors together relate to a single outcome.\nThis simulated data set reflects these ideas. It includes information on sleep hours, caffeine intake, and perceived stress scores for a group of individuals. The goal is to model stress as a function of sleep and caffeine and to understand how the two predictors together help explain variation in stress levels.\nYou can use the following questions in the Introduction section as motivation for your analysis:\n\nHow is sleep related to stress when we account for differences in caffeine intake?\nHow is caffeine intake related to stress when we account for differences in sleep?\nDo sleep and caffeine together help explain variation in perceived stress scores?\nWhich of the two predictors appears to have a stronger association with stress in this sample?\n\nThese questions help motivate why multiple linear regression is appropriate for this data set.",
    "crumbs": [
      "Learning Statistics using Generative AI"
    ]
  },
  {
    "objectID": "project/project.html#sec-reflection",
    "href": "project/project.html#sec-reflection",
    "title": "Learning Statistics using Generative AI",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nAfter completing your entire project, please write a reflection (~300 words) addressing the following points (you may expand beyond them):\n\nWhich GenAI tool you used\nHow AI influenced or improved your thinking throughout the project\nHow AI supported your understanding of multiple linear regression\nWhat challenges you encountered while using AI to self-learn or solve problems\nWhat you would do differently in the future when using AI for coursework or research, based on this experience\n\nThis reflection is part of your final project submission. Be honest and thoughtful. It will not be graded on whether you used AI ‚Äúcorrectly,‚Äù but on the depth of your reflection.\n\n\n\n\nPlease use the project template.",
    "crumbs": [
      "Learning Statistics using Generative AI"
    ]
  },
  {
    "objectID": "project/project.html#sec-ai",
    "href": "project/project.html#sec-ai",
    "title": "Learning Statistics using Generative AI",
    "section": "AI Usage",
    "text": "AI Usage\n\nYou use one of the following GenAI tools: ChatGPT, Google Gemini, and Microsoft Copilot.\nOther GenAI tools are not allowed.\nYou use generative AI as a thinking partner. This means the AI may help you think about concepts or understand the reasoning behind analysis steps.\nThe AI must not produce answers, interpretations, text for your report, or numerical results from your data set. Your work must represent your own thinking.\nYou must include your AI usage history after your data analysis report. Work submitted without this document will not receive full credit. See Section¬†2.3.2 for more details.\n\n\nInitial prompts\n‚Äº You must start your chat with GenAI using the following prompt:\n\nI am working on a multiple linear regression project using a small health-related dataset with three variables: sleep, caffeine, and stress. Act as my thinking partner. Do not give me full answers or report-ready text. Instead, guide me step by step, and after each step, ask me one clarifying or reflective question before moving on. Explain why each step matters and what ideas I should consider when making decisions. Use general examples if needed, but do not use my dataset values or provide final interpretations.\nHelp me understand the process by asking questions like: What does this coefficient mean in context? How would I explain R squared to someone with no statistics background? Why might these predictors be related? What assumptions should I check? What limitations might I need to note?\nYour role is to support my reasoning and help me understand how to build and interpret the model gradually, not to create my report content.\n\nThis prompt ensures the following learning outcomes:\n\nYou receive process guidance but must produce your own text.\nAI supports conceptual understanding rather than answering the assignment.\nYou learn to ask better questions and reflect on your reasoning.\nThe AI serves as a coach, not a solution generator.\n\n\n\nDocumenting AI usage\nYou must include your entire AI usage history at the end of your report. The purpose is to help you describe how you used generative AI as a thinking partner and demonstrate that the work you submitted reflects your own reasoning.\nPlease use the project template.\nFollow the step-by-step guide to share your AI chat conversation.",
    "crumbs": [
      "Learning Statistics using Generative AI"
    ]
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#why-comparing-two-populations",
    "href": "slides/11-infer-two-means-slides.html#why-comparing-two-populations",
    "title": "Comparing Two Population Means \n",
    "section": "Why Comparing Two Populations",
    "text": "Why Comparing Two Populations\n\n\nOften faced with a comparison of parameters from different populations.\n\n Comparing the mean annual income for Male and Female groups. \n Testing if a diet used for losing weight is effective from Placebo and New Diet samples. \n\n\n\n\n\nIf these two samples are drawn from populations with means \\(\\mu_1\\) and \\(\\mu_2\\) respectively,  \\[\\begin{align}\n&H_0: \\mu_1 = \\mu_2 \\\\\n&H_1: \\mu_1 &gt; \\mu_2\n\\end{align}\\] \n\n\n\\(\\mu_1\\): male mean annual income; \\(\\mu_2\\): female mean annual income\n\n\\(\\mu_1\\): mean weight loss from the New Diet group; \\(\\mu_2\\): mean weight loss from the Placebo group\n\n\n\n\n\nOften we are faced with an inference involving a comparison of parameters from different populations.\n\n Comparing the mean annual income for male and female groups. \n Testing if a diet used for losing weight is effective from Placebo samples and New Diet samples. \n\n\n\nIf these two samples are drawn from populations with means \\(\\mu_1\\) and \\(\\mu_2\\) respectively, then the testing problem can be formulated as  \\[\\begin{align}\n&H_0: \\mu_1 = \\mu_2 \\\\\n&H_1: \\mu_1 &gt; \\mu_2\n\\end{align}\\] \n\n\n\\(\\mu_1\\): male mean annual income; \\(\\mu_2\\): female mean annual income\n\n\\(\\mu_1\\): weight loss from the New Diet group; \\(\\mu_2\\): weight loss from the Placebo group"
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#dependent-and-independent-samples",
    "href": "slides/11-infer-two-means-slides.html#dependent-and-independent-samples",
    "title": "Comparing Two Population Means \n",
    "section": "Dependent and Independent Samples",
    "text": "Dependent and Independent Samples\n\nThe two samples can be independent or dependent.\n\n\n\nTwo samples are dependent or matched pairs if the sample values are matched, where the matching is based on some inherent relationship.\n\n Height data of fathers and daughters. The height of each dad is matched with the height of his daughter. \n Weights of subjects measure before and after some diet treatment. The subjects are the same before and after measurements. \n\n\n\n\n\n\n\n\n\n\n\n\nThe statistical methods are different for these two types of samples."
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#dependent-samples-matched-pairs-1",
    "href": "slides/11-infer-two-means-slides.html#dependent-samples-matched-pairs-1",
    "title": "Comparing Two Population Means \n",
    "section": "Dependent Samples (Matched Pairs)",
    "text": "Dependent Samples (Matched Pairs)\n\nSubject 1 may refer to\n\nthe first matched pair (dad-daughter)\nthe same person with two measurements (before and after)\n\n\n\n\n\n\n\n\nSubject\n(Dad) Before\n(Daughter) After\n\n\n\n1\n\\(x_{b1}\\)\n\\(x_{a1}\\)\n\n\n2\n\\(x_{b2}\\)\n\\(x_{a2}\\)\n\n\n3\n\\(x_{b3}\\)\n\\(x_{a3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(x_{bn}\\)\n\\(x_{an}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject 1 dad is 7 feet tall, subject 2 is 5 feet fall\nSubject 1 has weight 200 pounds, subject 2 weight 100 pounds\nIt makes more sense to tie the two samples together"
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#independent-samples-1",
    "href": "slides/11-infer-two-means-slides.html#independent-samples-1",
    "title": "Comparing Two Population Means \n",
    "section": "Independent Samples",
    "text": "Independent Samples\n\n\nTwo samples are independent if the sample values from one population are not related to the sample values from the other.\n\n Salary samples of men and women. Two samples are drawn independently from the male and female groups. \n\n\n\n\n\n\n\n\n\n\n\n\nSubject 1 of Group 1 has nothing to do with the subject 1 of Group 2.\n\n\n\n\n\n\n\n\n\n\nSubject of Group 1 (Male)\nMeasurement of Group 1\nSubject of Group 2 (Female)\nMeasurement of Group 2\n\n\n\n1\n\\(x_{11}\\)\n1\n\\(x_{21}\\)\n\n\n2\n\\(x_{12}\\)\n2\n\\(x_{22}\\)\n\n\n3\n\\(x_{13}\\)\n3\n\\(x_{23}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n_1\\)\n\\(x_{1n_1}\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\n\n\\(n_2\\)\n\\(x_{2n_2}\\)"
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#inference-from-two-samples",
    "href": "slides/11-infer-two-means-slides.html#inference-from-two-samples",
    "title": "Comparing Two Population Means \n",
    "section": "Inference from Two Samples",
    "text": "Inference from Two Samples\n\n\n\n\nThe statistical methods are different for these two types of samples.\n\n\n\n\nGood news: The concepts of CI and HT for one population can be applied to two-population cases.\n\n\n\n\n\\(\\text{CI = point estimate} \\pm \\text{margin of error (E)}\\), e.g., \\(\\overline{x} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\)\nMargin of error = critical value \\(\\times\\) standard error of the point estimator\n\n\n\n\nThe 6 testing steps are the same, and both critical value and \\(p\\)-value method can be applied too, e.g., \\(t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}}\\)\n\n\n\n\nTo compare two populations from two samples, we will learn and find\n\nthe point estimate for the parameter we are interested and its standard error\nwhich critical value and test statistic should be used in which cases"
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#hypothesis-testing-for-dependent-samples",
    "href": "slides/11-infer-two-means-slides.html#hypothesis-testing-for-dependent-samples",
    "title": "Comparing Two Population Means \n",
    "section": "Hypothesis Testing for Dependent Samples",
    "text": "Hypothesis Testing for Dependent Samples\n\nTo analyze a paired data set, simply analyze the differences!\n\n\n\n\n\n\n\n\n\n\n\nSubject\n\\(x_1\\)\n\\(x_2\\)\nDifference \\(d = x_1 - x_2\\)\n\n\n\n1\n\\(x_{11}\\)\n\\(x_{21}\\)\n\\(\\color{red}{d_1}\\)\n\n\n2\n\\(x_{12}\\)\n\\(x_{22}\\)\n\\(\\color{red}{d_2}\\)\n\n\n3\n\\(x_{13}\\)\n\\(x_{23}\\)\n\\(\\color{red}{d_3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\color{red}{\\vdots}\\)\n\n\n\\(n\\)\n\\(x_{1n}\\)\n\\(x_{2n}\\)\n\\(\\color{red}{d_n}\\)\n\n\n\n\n\n\\(\\mu_d = \\mu_1 - \\mu_2\\)\n \\(\\begin{align} & H_0: \\mu_1 - \\mu_2 = 0 \\iff \\mu_d = 0 \\\\ & H_1: \\mu_1 - \\mu_2 &gt; 0 \\iff \\mu_d &gt; 0 \\\\ & H_1: \\mu_1 - \\mu_2 &lt; 0 \\iff \\mu_d &lt; 0  \\\\ & H_1: \\mu_1 - \\mu_2 \\ne 0 \\iff \\mu_d \\ne 0 \\end{align}\\) \n\n\n\n\nThe point estimate of \\(\\mu_1 - \\mu_2\\) is \\(\\overline{x}_1 - \\overline{x}_2 = \\overline{d}\\).\n\n\nTransform two samples into one sample by taking the difference between paired measurements."
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#inference-for-paired-data",
    "href": "slides/11-infer-two-means-slides.html#inference-for-paired-data",
    "title": "Comparing Two Population Means \n",
    "section": "Inference for Paired Data",
    "text": "Inference for Paired Data\n\n\nRequirements: the sample differences \\(\\color{blue}{d_i}\\)s are\n\nrandom sample\nfrom a normal distribution and/or \\(n &gt; 30\\) (tested by QQ-plot of \\(d_i\\)s)\n\n\nFollow the same procedure as the one-sample \\(t\\)-test!\nThe test statistic is \\(\\color{blue}{t_{test} = \\frac{\\overline{d}-0}{s_d/\\sqrt{n}}} \\sim T_{n-1}\\) under \\(H_0\\) where \\(\\overline{d}\\) and \\(s_d\\) are the mean and SD of the difference samples \\((d_1, d_2, \\dots, d_n)\\).\nThe critical value \\(t_{\\alpha, n-1}\\) and \\(t_{\\alpha/2, n-1}\\).\n\n\n\n\n\n\n\n\n\nPaired \\(t\\)-test\nTest Statistic\nConfidence Interval for \\(\\mu_d = \\mu_1 - \\mu_2\\)\n\n\n\n\n\\(\\sigma_d\\) is unknown\n\\(\\large t_{test} = \\frac{\\overline{d}}{s_d/\\sqrt{n}}\\)\n\\(\\large \\overline{d} \\pm t_{\\alpha/2, n-1} \\frac{s_d}{\\sqrt{n}}\\)\n\n\n\nThe test from matched pairs is called a paired \\(t\\)-test.\n\n(Yes, the same as one-sample \\(t\\)-test)"
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#example",
    "href": "slides/11-infer-two-means-slides.html#example",
    "title": "Comparing Two Population Means \n",
    "section": "Example",
    "text": "Example\n\nConsider a capsule used to reduce blood pressure (BP) for the hypertensive individuals. Sample of 10 hypertensive individuals take the medicine for 4 weeks.\nDoes the data provide sufficient evidence that the treatment is effective in reducing BP?\n\n\n\n\n\n\n\n\n\n\n\n\nSubject\nBefore \\((x_b)\\)\n\nAfter \\((x_a)\\)\n\nDifference \\(d = x_b - x_a\\)\n\n\n\n1\n143\n124\n19\n\n\n2\n153\n129\n24\n\n\n3\n142\n131\n11\n\n\n4\n139\n145\n-6\n\n\n5\n172\n152\n20\n\n\n6\n176\n150\n26\n\n\n7\n155\n125\n30\n\n\n8\n149\n142\n7\n\n\n9\n140\n145\n-5\n\n\n10\n169\n160\n9"
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#example-contd",
    "href": "slides/11-infer-two-means-slides.html#example-contd",
    "title": "Comparing Two Population Means \n",
    "section": "Example Cont‚Äôd",
    "text": "Example Cont‚Äôd\n\n\\(\\overline{d} = 13.5\\), \\(s_d= 12.48\\).\n\\(\\mu_1 =\\) Mean Before, \\(\\mu_2 =\\) Mean After, and \\(\\mu_d = \\mu_1 - \\mu_2\\).\n\n\n\nStep 1:  \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\iff \\mu_d = 0\\\\ &H_1: \\mu_1 &gt; \\mu_2 \\iff \\mu_d &gt; 0 \\end{align}\\) \n\n\n\n\n\nStep 2:  \\(\\alpha = 0.05\\) \n\n\n\n\n\nStep 3:  \\(t_{test} = \\frac{\\overline{d}}{s_d/\\sqrt{n}} = \\frac{13.5}{12.48/\\sqrt{10}} = 3.42\\) \n\n\n\n\n\nStep 4-c:  \\(t_{\\alpha, n-1} = t_{0.05, 9} = 1.833\\).\n\n\n\n\n\nStep 5-c:  Since \\(\\small t_{test} = 3.42 &gt; 1.833  = t_{\\alpha, n-1}\\), we reject \\(H_0\\).\n\n\n\n\n\n\nStep 6:  There is sufficient evidence to support the claim that the drug is effective in reducing blood pressure. \n\n\nWe reject \\(H_0\\) if \\(\\small t_{test} &gt; t_{\\alpha, n-1}\\)."
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#example-contd-1",
    "href": "slides/11-infer-two-means-slides.html#example-contd-1",
    "title": "Comparing Two Population Means \n",
    "section": "Example Cont‚Äôd",
    "text": "Example Cont‚Äôd\n\nThe 95% CI for \\(\\mu_d = \\mu_1 - \\mu_2\\) is \\[\\begin{align}\\overline{d} \\pm t_{\\alpha/2, df} \\frac{s_d}{\\sqrt{n}} &= 13.5 \\pm t_{0.025, 9}\\frac{12.48}{\\sqrt{10}}\\\\ &= 13.5 \\pm 8.927 \\\\ &= (4.573, 22.427).\\end{align}\\]\n95% confident that the mean difference in blood pressure is between 4.57 and 22.43.\nSince the interval does NOT include 0, it leads to the same conclusion as rejection of \\(H_0\\)."
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#two-sample-paired-test-in-r",
    "href": "slides/11-infer-two-means-slides.html#two-sample-paired-test-in-r",
    "title": "Comparing Two Population Means \n",
    "section": "Two-Sample Paired Test in R",
    "text": "Two-Sample Paired Test in R\n\n\n\n\npair_data\n\n   before after\n1     143   124\n2     153   129\n3     142   131\n4     139   145\n5     172   152\n6     176   150\n7     155   125\n8     149   142\n9     140   145\n10    169   160\n\n\n\n\n\n(d &lt;- pair_data$before - pair_data$after)\n\n [1] 19 24 11 -6 20 26 30  7 -5  9\n\n(d_bar &lt;- mean(d))\n\n[1] 14\n\n\n\n\n\n(s_d &lt;- sd(d))\n\n[1] 12\n\n\n\n## t_test\n(t_test &lt;- d_bar/(s_d/sqrt(length(d))))\n\n[1] 3.4\n\n\n\n## t_cv\nqt(p = 0.95, df = length(d) - 1)\n\n[1] 1.8\n\n\n\n## p_value\npt(q = t_test, df = length(d) - 1, \n   lower.tail = FALSE)\n\n[1] 0.0038"
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#two-sample-paired-test-in-r-1",
    "href": "slides/11-infer-two-means-slides.html#two-sample-paired-test-in-r-1",
    "title": "Comparing Two Population Means \n",
    "section": "Two-Sample Paired Test in R",
    "text": "Two-Sample Paired Test in R\n\n## CI\nd_bar + c(-1, 1) * qt(p = 0.975, df = length(d) - 1) * (s_d / sqrt(length(d))) \n\n[1]  4.6 22.4\n\n\n\n\n\n\n## t.test() function\nt.test(x = pair_data$before, y = pair_data$after, alternative = \"greater\", mu = 0, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  pair_data$before and pair_data$after\nt = 3, df = 9, p-value = 0.004\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 6.3 Inf\nsample estimates:\nmean difference \n             14 \n\n\n\n\n\n\nBe careful about the one-sided CI! We should use the two-sided CI!"
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#compare-population-means-independent-samples",
    "href": "slides/11-infer-two-means-slides.html#compare-population-means-independent-samples",
    "title": "Comparing Two Population Means \n",
    "section": "Compare Population Means: Independent Samples",
    "text": "Compare Population Means: Independent Samples\n\nWhether stem cells can improve heart function.\nThe relationship between pregnant womens‚Äô smoking habits and newborns‚Äô weights.\nWhether one variation of an exam is harder than another variation."
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#testing-for-independent-samples-sigma_1-ne-sigma_2",
    "href": "slides/11-infer-two-means-slides.html#testing-for-independent-samples-sigma_1-ne-sigma_2",
    "title": "Comparing Two Population Means \n",
    "section": "Testing for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n",
    "text": "Testing for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n\n\n\nRequirements:\n\nThe two samples are independent.\nBoth samples are a random sample.\n\\(n_1 &gt; 30\\), \\(n_2 &gt; 30\\) and/or both samples are from a normally distributed population.\n\n\nInterested in whether the two population means \\(\\mu_1\\) and \\(\\mu_2\\) are equal or not, or one is larger than the other.\n\\(H_0: \\mu_1 = \\mu_2\\)\nIt is equivalent to testing if their difference is zero.\n\\(H_0: \\mu_1 - \\mu_2 = 0\\)\n\n\nWe start with finding a point estimate for \\(\\mu_1 - \\mu_2\\). What is the best point estimator for \\(\\mu_1 - \\mu_2\\)?\n\n\n\\(\\overline{X}_1 - \\overline{X}_2\\) is the best point estimator for \\(\\mu_1 - \\mu_2\\)!\nThe difference between the sample means ** \\(\\overline{X}_1 - \\overline{X}_2\\) is the best point estimator for \\(\\mu_1 - \\mu_2\\) **!"
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#sampling-distribution-of-overlinex_1---overlinex_2",
    "href": "slides/11-infer-two-means-slides.html#sampling-distribution-of-overlinex_1---overlinex_2",
    "title": "Comparing Two Population Means \n",
    "section": "Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\)\n",
    "text": "Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\)\n\nIf the two samples are from independent normally distributed populations or \\(n_1 &gt; 30\\) and \\(n_2 &gt; 30\\), \\[\\small \\overline{X}_1 \\sim N\\left(\\mu_1, \\frac{\\sigma_1^2}{n_1} \\right), \\quad \\overline{X}_2 \\sim N\\left(\\mu_2,\n\\frac{\\sigma_2^2}{n_2} \\right)\\]\n\n\\(\\overline{X}_1 - \\overline{X}_2\\) has the sampling distribution \\[\\small \\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} {\\color{red}{+}} \\frac{\\sigma_2^2}{n_2} \\right) \\]\n\n\n\\[\\small Z = \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\sim N(0, 1)\\]"
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#test-statistic-for-independent-samples-sigma_1-ne-sigma_2",
    "href": "slides/11-infer-two-means-slides.html#test-statistic-for-independent-samples-sigma_1-ne-sigma_2",
    "title": "Comparing Two Population Means \n",
    "section": "Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n",
    "text": "Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n\n\n\nWith \\(D_0\\) a hypothesized value (often 0),\n\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 \\le D_0\\\\ &H_1: \\mu_1 - \\mu_2 &gt; D_0 \\end{align}\\)  (right-tailed)\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 \\ge D_0\\\\ &H_1: \\mu_1 - \\mu_2 &lt; D_0 \\end{align}\\)  (left-tailed)\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 = D_0\\\\ &H_1: \\mu_1 - \\mu_2 \\ne D_0 \\end{align}\\)  (two-tailed)\n\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are known, the test statistic is the z-score of \\(\\small \\overline{X}_1 - \\overline{X}_2\\) under \\(H_0\\): \\[z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\]\nThen find \\(z_{\\alpha}\\) or \\(z_{\\alpha/2}\\) and follow our testing steps!"
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#test-statistic-for-independent-samples-sigma_1-ne-sigma_2-1",
    "href": "slides/11-infer-two-means-slides.html#test-statistic-for-independent-samples-sigma_1-ne-sigma_2-1",
    "title": "Comparing Two Population Means \n",
    "section": "Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n",
    "text": "Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, the test statistic becomes \\(t_{test}\\):\n\n\\[t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} \\]\n\n\n\nThe critical value \\(t_{\\alpha, df}\\) (one-tailed) and \\(t_{\\alpha/2, df}\\) (two-tailed), and the \\(t\\) distribution used to compute the \\(p\\)-value has the degrees of freedom \\[\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\] where \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\).\n\n\nIf the \\(df\\) is not an integer, we round it down to an integer. \n\n\nsimilar to the one sample case,"
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#inference-from-independent-samples-sigma_1-ne-sigma_2",
    "href": "slides/11-infer-two-means-slides.html#inference-from-independent-samples-sigma_1-ne-sigma_2",
    "title": "Comparing Two Population Means \n",
    "section": "Inference from Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n",
    "text": "Inference from Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n\n\n\n\n\n\n\n\n\\(\\large \\color{red}{\\sigma_1 \\ne \\sigma_2}\\)\nTest Statistic\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\n\n\n\nknown\n\\(\\large z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm z_{\\alpha/2} \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\)\n\n\nunknown\n\\(\\large t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}\\)\n\n\n\n\nUse \\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\) where \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\) to get the \\(p\\)-value, critical value, and CI.\nThe unequal-variance t-test is called Welch‚Äôs t-test."
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#example-two-sample-t-test",
    "href": "slides/11-infer-two-means-slides.html#example-two-sample-t-test",
    "title": "Comparing Two Population Means \n",
    "section": "Example: Two-Sample t-Test",
    "text": "Example: Two-Sample t-Test\n\n\nDoes an oversized tennis racket exert less stress/force on the elbow? The data show\n\nOversized: \\(n_1 = 33\\), \\(\\overline{x}_1 = 25.2\\), \\(s_1 = 8.6\\)\nConventional: \\(n_2 = 12\\), \\(\\overline{x}_2 = 33.9\\), \\(s_2 = 17.4\\)\nThe two populations are nearly normal.\nThe large difference in the sample SD suggests \\(\\sigma_1 \\ne \\sigma_2\\).\nForm a hypothesis test with \\(\\alpha = 0.05\\) and construct a 95% CI for the mean difference of force on the elbow.\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1:  \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 &lt; \\mu_2 \\end{align}\\) \n\n\n\n\nStep 2:  \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#example-two-sample-t-test-contd",
    "href": "slides/11-infer-two-means-slides.html#example-two-sample-t-test-contd",
    "title": "Comparing Two Population Means \n",
    "section": "Example: Two-Sample t-Test Cont‚Äôd",
    "text": "Example: Two-Sample t-Test Cont‚Äôd\n\nStep 3:  \\(t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(25.2 - 33.9) - 0}{\\sqrt{\\frac{\\color{red}{8.6^2}}{33} + \\frac{\\color{red}{17.4^2}}{12}}} = -1.66\\)\n\n\n\n\n\n\n\\(\\small A = \\dfrac{8.6^2}{33}\\), \\(\\small B = \\dfrac{17.4^2}{12}\\), \\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{33-1}+ \\dfrac{B^2}{12-1}} = 13.01\\)\n\nIf the computed value of \\(df\\) is not an integer, always round down to the nearest integer.\n\n\n\n\nStep 4-c:  \\(-t_{0.05, 13} = -1.77\\). \nStep 5-c:  We reject \\(H_0\\) if \\(\\small t_{test} &lt; -t_{\\alpha, df}\\). \\(\\small t_{test} = -1.66 &gt; -1.77 = -t_{\\alpha, df}\\), we fail to reject \\(H_0\\). \nStep 6:  There is insufficient evidence to support the claim that the oversized racket delivers less stress to the elbow."
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#example-two-sample-t-test-contd-1",
    "href": "slides/11-infer-two-means-slides.html#example-two-sample-t-test-contd-1",
    "title": "Comparing Two Population Means \n",
    "section": "Example: Two-Sample t-Test Cont‚Äôd",
    "text": "Example: Two-Sample t-Test Cont‚Äôd\n\nThe 95% CI for \\(\\mu_1 - \\mu_2\\) is\n\n\\[\\begin{align}(\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}} &= (25.2 - 33.9) \\pm t_{0.025,13}\\sqrt{\\frac{8.6^2}{33} + \\frac{17.4^2}{12}}\\\\&= -8.7 \\pm 11.32 = (-20.02, 2.62).\\end{align}\\]\n\nWe are 95% confident that the difference in the mean forces is between -20.02 and 2.62.\nSince the interval includes 0, it leads to the same conclusion as failing to reject \\(H_0\\)."
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#two-sample-t-test-in-r",
    "href": "slides/11-infer-two-means-slides.html#two-sample-t-test-in-r",
    "title": "Comparing Two Population Means \n",
    "section": "Two-Sample t-Test in R",
    "text": "Two-Sample t-Test in R\n\nn1 = 33; x1_bar = 25.2; s1 = 8.6\nn2 = 12; x2_bar = 33.9; s2 = 17.4\nA &lt;- s1^2 / n1; B &lt;- s2^2 / n2\ndf &lt;- (A + B)^2 / (A^2/(n1-1) + B^2/(n2-1))\n(df &lt;- floor(df))\n\n[1] 13\n\n## t_test\n(t_test &lt;- (x1_bar - x2_bar) / sqrt(s1^2/n1 + s2^2/n2))\n\n[1] -1.7\n\n## t_cv\nqt(p = 0.05, df = df)\n\n[1] -1.8\n\n## p_value\npt(q = t_test, df = df)\n\n[1] 0.06"
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#test-statistic-for-independent-samples-sigma_1-sigma_2-sigma",
    "href": "slides/11-infer-two-means-slides.html#test-statistic-for-independent-samples-sigma_1-sigma_2-sigma",
    "title": "Comparing Two Population Means \n",
    "section": "Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n",
    "text": "Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are known,\n\n\\[z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\]\n\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, similar to the one sample case, we use \\(t_{test}\\).\nAs \\(\\sigma_1 = \\sigma_2 = \\sigma\\), we don‚Äôt need two but one sample SD to replace the \\(\\sigma\\).\nUse the pooled sample variance to estimate the common \\(\\sigma^2\\):\n\n\\[ s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2} \\] which is the weighted average of \\(s_1^2\\) and \\(s_2^2\\).\n\nBut not so similar to the equal-variance case."
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#test-statistic-for-independent-samples-sigma_1-sigma_2-sigma-1",
    "href": "slides/11-infer-two-means-slides.html#test-statistic-for-independent-samples-sigma_1-sigma_2-sigma-1",
    "title": "Comparing Two Population Means \n",
    "section": "Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n",
    "text": "Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, \\[t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{ {\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\nHere, the critical value \\(t_{\\alpha, df}\\) (for one-tailed tests) and \\(t_{\\alpha/2, df}\\) (for two-tailed tests), and the \\(t\\) distribution used to compute the \\(p\\)-value have the degrees of freedom \\[df = n_1 + n_2 - 2\\]"
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#inference-from-independent-samples-sigma_1-sigma_2-sigma",
    "href": "slides/11-infer-two-means-slides.html#inference-from-independent-samples-sigma_1-sigma_2-sigma",
    "title": "Comparing Two Population Means \n",
    "section": "Inference from Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n",
    "text": "Inference from Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n\n\n\n\n\n\n\n\n\\(\\large \\color{red}{\\sigma_1 = \\sigma_2}\\)\nTest Statistic\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\n\n\n\nknown\n\\(\\large z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm z_{\\alpha/2} \\sigma \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\)\n\n\nunknown\n\\(\\large t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} {\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\)\n\n\n\n\n\\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\\)\nUse \\(df = n_1+n_2-2\\) get the \\(p\\)-value, critical value and CI.\nThe test from two independent samples with \\(\\sigma_1 = \\sigma_2  = \\sigma\\) is usually called two-sample pooled \\(z\\)-test or two-sample pooled \\(t\\)-test."
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#example-weight-loss",
    "href": "slides/11-infer-two-means-slides.html#example-weight-loss",
    "title": "Comparing Two Population Means \n",
    "section": "Example: Weight Loss",
    "text": "Example: Weight Loss\n\n\nA study was conducted to see the effectiveness of a weight loss program.\n\nTwo groups (Control and Experimental) of 10 subjects were selected.\nThe two populations are normally distributed and have the same SD.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data on weight loss was collected at the end of six months\n\n\nControl: \\(n_1 = 10\\), \\(\\overline{x}_1 = 2.1\\, lb\\), \\(s_1 = 0.5\\, lb\\)\n\n\nExperimental: \\(n_2 = 10\\), \\(\\overline{x}_2 = 4.2\\, lb\\), \\(s_2 = 0.7\\, lb\\)\n\n\n\nIs there a sufficient evidence at \\(\\alpha = 0.05\\) to conclude that the program is effective?\nIf yes, construct a 95% CI for \\(\\mu_1 - \\mu_2\\) to show how much effective it is.\n\n\n\n\n\n\n\n\n\n\n\nStep 1:  \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 &lt; \\mu_2 \\end{align}\\) \n\n\n\n\n\nStep 2:  \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#example-contd-2",
    "href": "slides/11-infer-two-means-slides.html#example-contd-2",
    "title": "Comparing Two Population Means \n",
    "section": "Example Cont‚Äôd",
    "text": "Example Cont‚Äôd\n\nStep 3:  \\(t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\). \n\\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}} = \\sqrt{\\frac{(10-1)0.5^2 + (10-1)0.7^2}{10+10-2}}=0.61\\)\n \\(t_{test} = \\frac{(2.1 - 4.2) - 0}{0.6083\\sqrt{\\frac{1}{10} + \\frac{1}{10}}} = -7.72\\)\n\n\n\nStep 4-c:  \\(df = n_1 + n_2 - 2 = 10 + 10 - 2 = 18\\). So \\(-t_{0.05, df = 18} = -1.734\\). \n\n\n\n\n\nStep 5-c:  We reject \\(H_0\\) if \\(\\small t_{test} &lt; -t_{\\alpha, df}\\). Since \\(\\small t_{test} = -7.72 &lt; -1.73  = -t_{\\alpha, df}\\), we reject \\(H_0\\).\n\n\n\n\n\nStep 4-p:  The \\(p\\)-value is \\(P(T_{df=18} &lt; t_{test}) \\approx 0\\) \n\n\n\n\n\nStep 5-p:  We reject \\(H_0\\) if \\(p\\)-value &lt; \\(\\alpha\\). Since \\(p\\)-value \\(\\approx 0 &lt; 0.05  = \\alpha\\), we reject \\(H_0\\).\n\n\n\n\n\nStep 6:  There is sufficient evidence to support the claim that the weight loss program is effective."
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#example-contd-3",
    "href": "slides/11-infer-two-means-slides.html#example-contd-3",
    "title": "Comparing Two Population Means \n",
    "section": "Example Cont‚Äôd",
    "text": "Example Cont‚Äôd\n\nThe 95% CI for \\(\\mu_1 - \\mu_2\\) is \\[\\begin{align}(\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} {\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}} &= (2.1 - 4.2) \\pm t_{0.025, 18} (0.61)\\sqrt{\\frac{1}{10} + \\frac{1}{10}}\\\\ &= -2.1 \\pm 0.57 = (-2.67, -1.53) \\end{align}\\]\nWe are 95% confident that the difference in the mean weight loss is between -2.67 and -1.53.\nSince the interval does not include 0, it leads to the same conclusion as rejection of \\(H_0\\)."
  },
  {
    "objectID": "slides/11-infer-two-means-slides.html#two-sample-pooled-t-test-in-r",
    "href": "slides/11-infer-two-means-slides.html#two-sample-pooled-t-test-in-r",
    "title": "Comparing Two Population Means \n",
    "section": "Two-Sample Pooled t-Test in R",
    "text": "Two-Sample Pooled t-Test in R\n\nn1 = 10; x1_bar = 2.1; s1 = 0.5\nn2 = 10; x2_bar = 4.2; s2 = 0.7\nsp &lt;- sqrt(((n1 - 1) * s1 ^ 2 + (n2 - 1) * s2 ^ 2) / (n1 + n2 - 2))\nsp\n\n[1] 0.61\n\ndf &lt;- n1 + n2 - 2\n## t_test\n(t_test &lt;- (x1_bar - x2_bar) / (sp * sqrt(1 / n1 + 1 / n2)))\n\n[1] -7.7\n\n## t_cv\nqt(p = 0.05, df = df)\n\n[1] -1.7\n\n## p_value\npt(q = t_test, df = df)\n\n[1] 2e-07"
  },
  {
    "objectID": "slides/11-infer-two-means.html#why-comparing-two-populations",
    "href": "slides/11-infer-two-means.html#why-comparing-two-populations",
    "title": "Comparing Two Population Means \n",
    "section": "Why Comparing Two Populations",
    "text": "Why Comparing Two Populations\n\n\nOften faced with a comparison of parameters from different populations.\n\n Comparing the mean annual income for Male and Female groups. \n Testing if a diet used for losing weight is effective from Placebo and New Diet samples. \n\n\n\n. . .\n\nIf these two samples are drawn from populations with means \\(\\mu_1\\) and \\(\\mu_2\\) respectively,  \\[\\begin{align}\n&H_0: \\mu_1 = \\mu_2 \\\\\n&H_1: \\mu_1 &gt; \\mu_2\n\\end{align}\\] \n\n\n\\(\\mu_1\\): male mean annual income; \\(\\mu_2\\): female mean annual income\n\n\\(\\mu_1\\): mean weight loss from the New Diet group; \\(\\mu_2\\): mean weight loss from the Placebo group\n\n\n\n\n\n\nOften we are faced with an inference involving a comparison of parameters from different populations.\n\n Comparing the mean annual income for male and female groups. \n Testing if a diet used for losing weight is effective from Placebo samples and New Diet samples. \n\n\n\nIf these two samples are drawn from populations with means \\(\\mu_1\\) and \\(\\mu_2\\) respectively, then the testing problem can be formulated as  \\[\\begin{align}\n&H_0: \\mu_1 = \\mu_2 \\\\\n&H_1: \\mu_1 &gt; \\mu_2\n\\end{align}\\] \n\n\n\\(\\mu_1\\): male mean annual income; \\(\\mu_2\\): female mean annual income\n\n\\(\\mu_1\\): weight loss from the New Diet group; \\(\\mu_2\\): weight loss from the Placebo group"
  },
  {
    "objectID": "slides/11-infer-two-means.html#dependent-and-independent-samples",
    "href": "slides/11-infer-two-means.html#dependent-and-independent-samples",
    "title": "Comparing Two Population Means \n",
    "section": "Dependent and Independent Samples",
    "text": "Dependent and Independent Samples\n\nThe two samples can be independent or dependent.\n\n\n\nTwo samples are dependent or matched pairs if the sample values are matched, where the matching is based on some inherent relationship.\n\n Height data of fathers and daughters. The height of each dad is matched with the height of his daughter. \n Weights of subjects measure before and after some diet treatment. The subjects are the same before and after measurements. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe statistical methods are different for these two types of samples."
  },
  {
    "objectID": "slides/11-infer-two-means.html#dependent-samples-matched-pairs-1",
    "href": "slides/11-infer-two-means.html#dependent-samples-matched-pairs-1",
    "title": "Comparing Two Population Means \n",
    "section": "Dependent Samples (Matched Pairs)",
    "text": "Dependent Samples (Matched Pairs)\n\nSubject 1 may refer to\n\nthe first matched pair (dad-daughter)\nthe same person with two measurements (before and after)\n\n\n\n\n\n\n\n\nSubject\n(Dad) Before\n(Daughter) After\n\n\n\n1\n\\(x_{b1}\\)\n\\(x_{a1}\\)\n\n\n2\n\\(x_{b2}\\)\n\\(x_{a2}\\)\n\n\n3\n\\(x_{b3}\\)\n\\(x_{a3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(x_{bn}\\)\n\\(x_{an}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject 1 dad is 7 feet tall, subject 2 is 5 feet fall\nSubject 1 has weight 200 pounds, subject 2 weight 100 pounds\nIt makes more sense to tie the two samples together"
  },
  {
    "objectID": "slides/11-infer-two-means.html#independent-samples-1",
    "href": "slides/11-infer-two-means.html#independent-samples-1",
    "title": "Comparing Two Population Means \n",
    "section": "Independent Samples",
    "text": "Independent Samples\n\n\nTwo samples are independent if the sample values from one population are not related to the sample values from the other.\n\n Salary samples of men and women. Two samples are drawn independently from the male and female groups. \n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject 1 of Group 1 has nothing to do with the subject 1 of Group 2.\n\n\n\n\n\n\n\n\n\n\nSubject of Group 1 (Male)\nMeasurement of Group 1\nSubject of Group 2 (Female)\nMeasurement of Group 2\n\n\n\n1\n\\(x_{11}\\)\n1\n\\(x_{21}\\)\n\n\n2\n\\(x_{12}\\)\n2\n\\(x_{22}\\)\n\n\n3\n\\(x_{13}\\)\n3\n\\(x_{23}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n_1\\)\n\\(x_{1n_1}\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\n\n\\(n_2\\)\n\\(x_{2n_2}\\)"
  },
  {
    "objectID": "slides/11-infer-two-means.html#inference-from-two-samples",
    "href": "slides/11-infer-two-means.html#inference-from-two-samples",
    "title": "Comparing Two Population Means \n",
    "section": "Inference from Two Samples",
    "text": "Inference from Two Samples\n\n\n\n\nThe statistical methods are different for these two types of samples.\n\n. . .\n\n\nGood news: The concepts of CI and HT for one population can be applied to two-population cases.\n\n. . .\n\n\\(\\text{CI = point estimate} \\pm \\text{margin of error (E)}\\), e.g., \\(\\overline{x} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\)\nMargin of error = critical value \\(\\times\\) standard error of the point estimator\n\n. . .\n\nThe 6 testing steps are the same, and both critical value and \\(p\\)-value method can be applied too, e.g., \\(t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}}\\)\n\n\n\n\n\nTo compare two populations from two samples, we will learn and find\n\nthe point estimate for the parameter we are interested and its standard error\nwhich critical value and test statistic should be used in which cases"
  },
  {
    "objectID": "slides/11-infer-two-means.html#hypothesis-testing-for-dependent-samples",
    "href": "slides/11-infer-two-means.html#hypothesis-testing-for-dependent-samples",
    "title": "Comparing Two Population Means \n",
    "section": "Hypothesis Testing for Dependent Samples",
    "text": "Hypothesis Testing for Dependent Samples\n\nTo analyze a paired data set, simply analyze the differences!\n\n\n\n\n\n\n\n\n\n\n\nSubject\n\\(x_1\\)\n\\(x_2\\)\nDifference \\(d = x_1 - x_2\\)\n\n\n\n1\n\\(x_{11}\\)\n\\(x_{21}\\)\n\\(\\color{red}{d_1}\\)\n\n\n2\n\\(x_{12}\\)\n\\(x_{22}\\)\n\\(\\color{red}{d_2}\\)\n\n\n3\n\\(x_{13}\\)\n\\(x_{23}\\)\n\\(\\color{red}{d_3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\color{red}{\\vdots}\\)\n\n\n\\(n\\)\n\\(x_{1n}\\)\n\\(x_{2n}\\)\n\\(\\color{red}{d_n}\\)\n\n\n\n\n\n\\(\\mu_d = \\mu_1 - \\mu_2\\)\n \\(\\begin{align} & H_0: \\mu_1 - \\mu_2 = 0 \\iff \\mu_d = 0 \\\\ & H_1: \\mu_1 - \\mu_2 &gt; 0 \\iff \\mu_d &gt; 0 \\\\ & H_1: \\mu_1 - \\mu_2 &lt; 0 \\iff \\mu_d &lt; 0  \\\\ & H_1: \\mu_1 - \\mu_2 \\ne 0 \\iff \\mu_d \\ne 0 \\end{align}\\) \n\n\n\n\n\nThe point estimate of \\(\\mu_1 - \\mu_2\\) is \\(\\overline{x}_1 - \\overline{x}_2 = \\overline{d}\\).\n\n\n\nTransform two samples into one sample by taking the difference between paired measurements."
  },
  {
    "objectID": "slides/11-infer-two-means.html#inference-for-paired-data",
    "href": "slides/11-infer-two-means.html#inference-for-paired-data",
    "title": "Comparing Two Population Means \n",
    "section": "Inference for Paired Data",
    "text": "Inference for Paired Data\n\n\nRequirements: the sample differences \\(\\color{blue}{d_i}\\)s are\n\nrandom sample\nfrom a normal distribution and/or \\(n &gt; 30\\) (tested by QQ-plot of \\(d_i\\)s)\n\n\nFollow the same procedure as the one-sample \\(t\\)-test!\nThe test statistic is \\(\\color{blue}{t_{test} = \\frac{\\overline{d}-0}{s_d/\\sqrt{n}}} \\sim T_{n-1}\\) under \\(H_0\\) where \\(\\overline{d}\\) and \\(s_d\\) are the mean and SD of the difference samples \\((d_1, d_2, \\dots, d_n)\\).\nThe critical value \\(t_{\\alpha, n-1}\\) and \\(t_{\\alpha/2, n-1}\\).\n\n. . .\n\n\n\n\n\n\n\nPaired \\(t\\)-test\nTest Statistic\nConfidence Interval for \\(\\mu_d = \\mu_1 - \\mu_2\\)\n\n\n\n\n\\(\\sigma_d\\) is unknown\n\\(\\large t_{test} = \\frac{\\overline{d}}{s_d/\\sqrt{n}}\\)\n\\(\\large \\overline{d} \\pm t_{\\alpha/2, n-1} \\frac{s_d}{\\sqrt{n}}\\)\n\n\n\nThe test from matched pairs is called a paired \\(t\\)-test.\n\n\n\n(Yes, the same as one-sample \\(t\\)-test)"
  },
  {
    "objectID": "slides/11-infer-two-means.html#example",
    "href": "slides/11-infer-two-means.html#example",
    "title": "Comparing Two Population Means \n",
    "section": "Example",
    "text": "Example\n\nConsider a capsule used to reduce blood pressure (BP) for the hypertensive individuals. Sample of 10 hypertensive individuals take the medicine for 4 weeks.\nDoes the data provide sufficient evidence that the treatment is effective in reducing BP?\n\n\n\n\n\n\n\n\n\n\n\n\nSubject\nBefore \\((x_b)\\)\n\nAfter \\((x_a)\\)\n\nDifference \\(d = x_b - x_a\\)\n\n\n\n1\n143\n124\n19\n\n\n2\n153\n129\n24\n\n\n3\n142\n131\n11\n\n\n4\n139\n145\n-6\n\n\n5\n172\n152\n20\n\n\n6\n176\n150\n26\n\n\n7\n155\n125\n30\n\n\n8\n149\n142\n7\n\n\n9\n140\n145\n-5\n\n\n10\n169\n160\n9"
  },
  {
    "objectID": "slides/11-infer-two-means.html#example-contd",
    "href": "slides/11-infer-two-means.html#example-contd",
    "title": "Comparing Two Population Means \n",
    "section": "Example Cont‚Äôd",
    "text": "Example Cont‚Äôd\n\n\\(\\overline{d} = 13.5\\), \\(s_d= 12.48\\).\n\\(\\mu_1 =\\) Mean Before, \\(\\mu_2 =\\) Mean After, and \\(\\mu_d = \\mu_1 - \\mu_2\\).\n\n. . .\n\nStep 1:  \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\iff \\mu_d = 0\\\\ &H_1: \\mu_1 &gt; \\mu_2 \\iff \\mu_d &gt; 0 \\end{align}\\) \n\n\n. . .\n\nStep 2:  \\(\\alpha = 0.05\\) \n\n\n. . .\n\nStep 3:  \\(t_{test} = \\frac{\\overline{d}}{s_d/\\sqrt{n}} = \\frac{13.5}{12.48/\\sqrt{10}} = 3.42\\) \n\n\n. . .\n\nStep 4-c:  \\(t_{\\alpha, n-1} = t_{0.05, 9} = 1.833\\).\n\n\n. . .\n\nStep 5-c:  Since \\(\\small t_{test} = 3.42 &gt; 1.833  = t_{\\alpha, n-1}\\), we reject \\(H_0\\).\n\n\n. . .\n\nStep 6:  There is sufficient evidence to support the claim that the drug is effective in reducing blood pressure. \n\n\n\nWe reject \\(H_0\\) if \\(\\small t_{test} &gt; t_{\\alpha, n-1}\\)."
  },
  {
    "objectID": "slides/11-infer-two-means.html#example-contd-1",
    "href": "slides/11-infer-two-means.html#example-contd-1",
    "title": "Comparing Two Population Means \n",
    "section": "Example Cont‚Äôd",
    "text": "Example Cont‚Äôd\n\nThe 95% CI for \\(\\mu_d = \\mu_1 - \\mu_2\\) is \\[\\begin{align}\\overline{d} \\pm t_{\\alpha/2, df} \\frac{s_d}{\\sqrt{n}} &= 13.5 \\pm t_{0.025, 9}\\frac{12.48}{\\sqrt{10}}\\\\ &= 13.5 \\pm 8.927 \\\\ &= (4.573, 22.427).\\end{align}\\]\n95% confident that the mean difference in blood pressure is between 4.57 and 22.43.\nSince the interval does NOT include 0, it leads to the same conclusion as rejection of \\(H_0\\)."
  },
  {
    "objectID": "slides/11-infer-two-means.html#two-sample-paired-test-in-r",
    "href": "slides/11-infer-two-means.html#two-sample-paired-test-in-r",
    "title": "Comparing Two Population Means \n",
    "section": "Two-Sample Paired Test in R",
    "text": "Two-Sample Paired Test in R\n\n\n\n\npair_data\n\n   before after\n1     143   124\n2     153   129\n3     142   131\n4     139   145\n5     172   152\n6     176   150\n7     155   125\n8     149   142\n9     140   145\n10    169   160\n\n\n\n\n\n(d &lt;- pair_data$before - pair_data$after)\n\n [1] 19 24 11 -6 20 26 30  7 -5  9\n\n(d_bar &lt;- mean(d))\n\n[1] 14\n\n\n\n\n\n(s_d &lt;- sd(d))\n\n[1] 12\n\n\n\n## t_test\n(t_test &lt;- d_bar/(s_d/sqrt(length(d))))\n\n[1] 3.4\n\n\n\n## t_cv\nqt(p = 0.95, df = length(d) - 1)\n\n[1] 1.8\n\n\n\n## p_value\npt(q = t_test, df = length(d) - 1, \n   lower.tail = FALSE)\n\n[1] 0.0038"
  },
  {
    "objectID": "slides/11-infer-two-means.html#two-sample-paired-test-in-r-1",
    "href": "slides/11-infer-two-means.html#two-sample-paired-test-in-r-1",
    "title": "Comparing Two Population Means \n",
    "section": "Two-Sample Paired Test in R",
    "text": "Two-Sample Paired Test in R\n\n## CI\nd_bar + c(-1, 1) * qt(p = 0.975, df = length(d) - 1) * (s_d / sqrt(length(d))) \n\n[1]  4.6 22.4\n\n\n. . .\n\n\n\n## t.test() function\nt.test(x = pair_data$before, y = pair_data$after, alternative = \"greater\", mu = 0, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  pair_data$before and pair_data$after\nt = 3, df = 9, p-value = 0.004\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 6.3 Inf\nsample estimates:\nmean difference \n             14 \n\n\n\n. . .\n\nBe careful about the one-sided CI! We should use the two-sided CI!"
  },
  {
    "objectID": "slides/11-infer-two-means.html#compare-population-means-independent-samples",
    "href": "slides/11-infer-two-means.html#compare-population-means-independent-samples",
    "title": "Comparing Two Population Means \n",
    "section": "Compare Population Means: Independent Samples",
    "text": "Compare Population Means: Independent Samples\n\nWhether stem cells can improve heart function.\nThe relationship between pregnant womens‚Äô smoking habits and newborns‚Äô weights.\nWhether one variation of an exam is harder than another variation."
  },
  {
    "objectID": "slides/11-infer-two-means.html#testing-for-independent-samples-sigma_1-ne-sigma_2",
    "href": "slides/11-infer-two-means.html#testing-for-independent-samples-sigma_1-ne-sigma_2",
    "title": "Comparing Two Population Means \n",
    "section": "Testing for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n",
    "text": "Testing for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n\n\n\nRequirements:\n\nThe two samples are independent.\nBoth samples are a random sample.\n\\(n_1 &gt; 30\\), \\(n_2 &gt; 30\\) and/or both samples are from a normally distributed population.\n\n\nInterested in whether the two population means \\(\\mu_1\\) and \\(\\mu_2\\) are equal or not, or one is larger than the other.\n\\(H_0: \\mu_1 = \\mu_2\\)\nIt is equivalent to testing if their difference is zero.\n\\(H_0: \\mu_1 - \\mu_2 = 0\\)\n\n\nWe start with finding a point estimate for \\(\\mu_1 - \\mu_2\\). What is the best point estimator for \\(\\mu_1 - \\mu_2\\)?\n\n. . .\n\\(\\overline{X}_1 - \\overline{X}_2\\) is the best point estimator for \\(\\mu_1 - \\mu_2\\)!\n\nThe difference between the sample means ** \\(\\overline{X}_1 - \\overline{X}_2\\) is the best point estimator for \\(\\mu_1 - \\mu_2\\) **!"
  },
  {
    "objectID": "slides/11-infer-two-means.html#sampling-distribution-of-overlinex_1---overlinex_2",
    "href": "slides/11-infer-two-means.html#sampling-distribution-of-overlinex_1---overlinex_2",
    "title": "Comparing Two Population Means \n",
    "section": "Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\)\n",
    "text": "Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\)\n\nIf the two samples are from independent normally distributed populations or \\(n_1 &gt; 30\\) and \\(n_2 &gt; 30\\), \\[\\small \\overline{X}_1 \\sim N\\left(\\mu_1, \\frac{\\sigma_1^2}{n_1} \\right), \\quad \\overline{X}_2 \\sim N\\left(\\mu_2,\n\\frac{\\sigma_2^2}{n_2} \\right)\\]\n. . .\n\\(\\overline{X}_1 - \\overline{X}_2\\) has the sampling distribution \\[\\small \\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} {\\color{red}{+}} \\frac{\\sigma_2^2}{n_2} \\right) \\]\n. . .\n\\[\\small Z = \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\sim N(0, 1)\\]"
  },
  {
    "objectID": "slides/11-infer-two-means.html#test-statistic-for-independent-samples-sigma_1-ne-sigma_2",
    "href": "slides/11-infer-two-means.html#test-statistic-for-independent-samples-sigma_1-ne-sigma_2",
    "title": "Comparing Two Population Means \n",
    "section": "Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n",
    "text": "Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n\n\n\nWith \\(D_0\\) a hypothesized value (often 0),\n\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 \\le D_0\\\\ &H_1: \\mu_1 - \\mu_2 &gt; D_0 \\end{align}\\)  (right-tailed)\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 \\ge D_0\\\\ &H_1: \\mu_1 - \\mu_2 &lt; D_0 \\end{align}\\)  (left-tailed)\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 = D_0\\\\ &H_1: \\mu_1 - \\mu_2 \\ne D_0 \\end{align}\\)  (two-tailed)\n\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are known, the test statistic is the z-score of \\(\\small \\overline{X}_1 - \\overline{X}_2\\) under \\(H_0\\): \\[z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\]\nThen find \\(z_{\\alpha}\\) or \\(z_{\\alpha/2}\\) and follow our testing steps!"
  },
  {
    "objectID": "slides/11-infer-two-means.html#test-statistic-for-independent-samples-sigma_1-ne-sigma_2-1",
    "href": "slides/11-infer-two-means.html#test-statistic-for-independent-samples-sigma_1-ne-sigma_2-1",
    "title": "Comparing Two Population Means \n",
    "section": "Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n",
    "text": "Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, the test statistic becomes \\(t_{test}\\):\n\n\\[t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} \\]\n\n. . .\n\nThe critical value \\(t_{\\alpha, df}\\) (one-tailed) and \\(t_{\\alpha/2, df}\\) (two-tailed), and the \\(t\\) distribution used to compute the \\(p\\)-value has the degrees of freedom \\[\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\] where \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\).\n\n\nIf the \\(df\\) is not an integer, we round it down to an integer. \n\n\n\nsimilar to the one sample case,"
  },
  {
    "objectID": "slides/11-infer-two-means.html#inference-from-independent-samples-sigma_1-ne-sigma_2",
    "href": "slides/11-infer-two-means.html#inference-from-independent-samples-sigma_1-ne-sigma_2",
    "title": "Comparing Two Population Means \n",
    "section": "Inference from Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n",
    "text": "Inference from Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n\n\n\n\n\n\n\n\n\\(\\large \\color{red}{\\sigma_1 \\ne \\sigma_2}\\)\nTest Statistic\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\n\n\n\nknown\n\\(\\large z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm z_{\\alpha/2} \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\)\n\n\nunknown\n\\(\\large t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}\\)\n\n\n\n\nUse \\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\) where \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\) to get the \\(p\\)-value, critical value, and CI.\nThe unequal-variance t-test is called Welch‚Äôs t-test."
  },
  {
    "objectID": "slides/11-infer-two-means.html#example-two-sample-t-test",
    "href": "slides/11-infer-two-means.html#example-two-sample-t-test",
    "title": "Comparing Two Population Means \n",
    "section": "Example: Two-Sample t-Test",
    "text": "Example: Two-Sample t-Test\n\n\nDoes an oversized tennis racket exert less stress/force on the elbow? The data show\n\nOversized: \\(n_1 = 33\\), \\(\\overline{x}_1 = 25.2\\), \\(s_1 = 8.6\\)\nConventional: \\(n_2 = 12\\), \\(\\overline{x}_2 = 33.9\\), \\(s_2 = 17.4\\)\nThe two populations are nearly normal.\nThe large difference in the sample SD suggests \\(\\sigma_1 \\ne \\sigma_2\\).\nForm a hypothesis test with \\(\\alpha = 0.05\\) and construct a 95% CI for the mean difference of force on the elbow.\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1:  \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 &lt; \\mu_2 \\end{align}\\) \n\n\n. . .\n\nStep 2:  \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "slides/11-infer-two-means.html#example-two-sample-t-test-contd",
    "href": "slides/11-infer-two-means.html#example-two-sample-t-test-contd",
    "title": "Comparing Two Population Means \n",
    "section": "Example: Two-Sample t-Test Cont‚Äôd",
    "text": "Example: Two-Sample t-Test Cont‚Äôd\n\nStep 3:  \\(t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(25.2 - 33.9) - 0}{\\sqrt{\\frac{\\color{red}{8.6^2}}{33} + \\frac{\\color{red}{17.4^2}}{12}}} = -1.66\\)\n\n\n. . .\n\n\n\n\\(\\small A = \\dfrac{8.6^2}{33}\\), \\(\\small B = \\dfrac{17.4^2}{12}\\), \\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{33-1}+ \\dfrac{B^2}{12-1}} = 13.01\\)\n\nIf the computed value of \\(df\\) is not an integer, always round down to the nearest integer.\n\n. . .\n\nStep 4-c:  \\(-t_{0.05, 13} = -1.77\\). \nStep 5-c:  We reject \\(H_0\\) if \\(\\small t_{test} &lt; -t_{\\alpha, df}\\). \\(\\small t_{test} = -1.66 &gt; -1.77 = -t_{\\alpha, df}\\), we fail to reject \\(H_0\\). \nStep 6:  There is insufficient evidence to support the claim that the oversized racket delivers less stress to the elbow."
  },
  {
    "objectID": "slides/11-infer-two-means.html#example-two-sample-t-test-contd-1",
    "href": "slides/11-infer-two-means.html#example-two-sample-t-test-contd-1",
    "title": "Comparing Two Population Means \n",
    "section": "Example: Two-Sample t-Test Cont‚Äôd",
    "text": "Example: Two-Sample t-Test Cont‚Äôd\n\nThe 95% CI for \\(\\mu_1 - \\mu_2\\) is\n\n\\[\\begin{align}(\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}} &= (25.2 - 33.9) \\pm t_{0.025,13}\\sqrt{\\frac{8.6^2}{33} + \\frac{17.4^2}{12}}\\\\&= -8.7 \\pm 11.32 = (-20.02, 2.62).\\end{align}\\]\n\nWe are 95% confident that the difference in the mean forces is between -20.02 and 2.62.\nSince the interval includes 0, it leads to the same conclusion as failing to reject \\(H_0\\)."
  },
  {
    "objectID": "slides/11-infer-two-means.html#two-sample-t-test-in-r",
    "href": "slides/11-infer-two-means.html#two-sample-t-test-in-r",
    "title": "Comparing Two Population Means \n",
    "section": "Two-Sample t-Test in R",
    "text": "Two-Sample t-Test in R\n\nn1 = 33; x1_bar = 25.2; s1 = 8.6\nn2 = 12; x2_bar = 33.9; s2 = 17.4\nA &lt;- s1^2 / n1; B &lt;- s2^2 / n2\ndf &lt;- (A + B)^2 / (A^2/(n1-1) + B^2/(n2-1))\n(df &lt;- floor(df))\n\n[1] 13\n\n## t_test\n(t_test &lt;- (x1_bar - x2_bar) / sqrt(s1^2/n1 + s2^2/n2))\n\n[1] -1.7\n\n## t_cv\nqt(p = 0.05, df = df)\n\n[1] -1.8\n\n## p_value\npt(q = t_test, df = df)\n\n[1] 0.06"
  },
  {
    "objectID": "slides/11-infer-two-means.html#test-statistic-for-independent-samples-sigma_1-sigma_2-sigma",
    "href": "slides/11-infer-two-means.html#test-statistic-for-independent-samples-sigma_1-sigma_2-sigma",
    "title": "Comparing Two Population Means \n",
    "section": "Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n",
    "text": "Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are known,\n\n\\[z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\]\n. . .\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, similar to the one sample case, we use \\(t_{test}\\).\nAs \\(\\sigma_1 = \\sigma_2 = \\sigma\\), we don‚Äôt need two but one sample SD to replace the \\(\\sigma\\).\nUse the pooled sample variance to estimate the common \\(\\sigma^2\\):\n\n\\[ s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2} \\] which is the weighted average of \\(s_1^2\\) and \\(s_2^2\\).\n\n\nBut not so similar to the equal-variance case."
  },
  {
    "objectID": "slides/11-infer-two-means.html#test-statistic-for-independent-samples-sigma_1-sigma_2-sigma-1",
    "href": "slides/11-infer-two-means.html#test-statistic-for-independent-samples-sigma_1-sigma_2-sigma-1",
    "title": "Comparing Two Population Means \n",
    "section": "Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n",
    "text": "Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, \\[t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{ {\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\nHere, the critical value \\(t_{\\alpha, df}\\) (for one-tailed tests) and \\(t_{\\alpha/2, df}\\) (for two-tailed tests), and the \\(t\\) distribution used to compute the \\(p\\)-value have the degrees of freedom \\[df = n_1 + n_2 - 2\\]"
  },
  {
    "objectID": "slides/11-infer-two-means.html#inference-from-independent-samples-sigma_1-sigma_2-sigma",
    "href": "slides/11-infer-two-means.html#inference-from-independent-samples-sigma_1-sigma_2-sigma",
    "title": "Comparing Two Population Means \n",
    "section": "Inference from Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n",
    "text": "Inference from Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n\n\n\n\n\n\n\n\n\\(\\large \\color{red}{\\sigma_1 = \\sigma_2}\\)\nTest Statistic\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\n\n\n\nknown\n\\(\\large z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm z_{\\alpha/2} \\sigma \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\)\n\n\nunknown\n\\(\\large t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} {\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\)\n\n\n\n\n\\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\\)\nUse \\(df = n_1+n_2-2\\) get the \\(p\\)-value, critical value and CI.\nThe test from two independent samples with \\(\\sigma_1 = \\sigma_2  = \\sigma\\) is usually called two-sample pooled \\(z\\)-test or two-sample pooled \\(t\\)-test."
  },
  {
    "objectID": "slides/11-infer-two-means.html#example-weight-loss",
    "href": "slides/11-infer-two-means.html#example-weight-loss",
    "title": "Comparing Two Population Means \n",
    "section": "Example: Weight Loss",
    "text": "Example: Weight Loss\n\n\nA study was conducted to see the effectiveness of a weight loss program.\n\nTwo groups (Control and Experimental) of 10 subjects were selected.\nThe two populations are normally distributed and have the same SD.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data on weight loss was collected at the end of six months\n\n\nControl: \\(n_1 = 10\\), \\(\\overline{x}_1 = 2.1\\, lb\\), \\(s_1 = 0.5\\, lb\\)\n\n\nExperimental: \\(n_2 = 10\\), \\(\\overline{x}_2 = 4.2\\, lb\\), \\(s_2 = 0.7\\, lb\\)\n\n\n\nIs there a sufficient evidence at \\(\\alpha = 0.05\\) to conclude that the program is effective?\nIf yes, construct a 95% CI for \\(\\mu_1 - \\mu_2\\) to show how much effective it is.\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\nStep 1:  \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 &lt; \\mu_2 \\end{align}\\) \n\n\n. . .\n\nStep 2:  \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "slides/11-infer-two-means.html#example-contd-2",
    "href": "slides/11-infer-two-means.html#example-contd-2",
    "title": "Comparing Two Population Means \n",
    "section": "Example Cont‚Äôd",
    "text": "Example Cont‚Äôd\n\nStep 3:  \\(t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\). \n\\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}} = \\sqrt{\\frac{(10-1)0.5^2 + (10-1)0.7^2}{10+10-2}}=0.61\\)\n \\(t_{test} = \\frac{(2.1 - 4.2) - 0}{0.6083\\sqrt{\\frac{1}{10} + \\frac{1}{10}}} = -7.72\\)\n\n. . .\n\nStep 4-c:  \\(df = n_1 + n_2 - 2 = 10 + 10 - 2 = 18\\). So \\(-t_{0.05, df = 18} = -1.734\\). \n\n\n. . .\n\nStep 5-c:  We reject \\(H_0\\) if \\(\\small t_{test} &lt; -t_{\\alpha, df}\\). Since \\(\\small t_{test} = -7.72 &lt; -1.73  = -t_{\\alpha, df}\\), we reject \\(H_0\\).\n\n\n. . .\n\nStep 4-p:  The \\(p\\)-value is \\(P(T_{df=18} &lt; t_{test}) \\approx 0\\) \n\n\n. . .\n\nStep 5-p:  We reject \\(H_0\\) if \\(p\\)-value &lt; \\(\\alpha\\). Since \\(p\\)-value \\(\\approx 0 &lt; 0.05  = \\alpha\\), we reject \\(H_0\\).\n\n\n. . .\n\nStep 6:  There is sufficient evidence to support the claim that the weight loss program is effective."
  },
  {
    "objectID": "slides/11-infer-two-means.html#example-contd-3",
    "href": "slides/11-infer-two-means.html#example-contd-3",
    "title": "Comparing Two Population Means \n",
    "section": "Example Cont‚Äôd",
    "text": "Example Cont‚Äôd\n\nThe 95% CI for \\(\\mu_1 - \\mu_2\\) is \\[\\begin{align}(\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} {\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}} &= (2.1 - 4.2) \\pm t_{0.025, 18} (0.61)\\sqrt{\\frac{1}{10} + \\frac{1}{10}}\\\\ &= -2.1 \\pm 0.57 = (-2.67, -1.53) \\end{align}\\]\nWe are 95% confident that the difference in the mean weight loss is between -2.67 and -1.53.\nSince the interval does not include 0, it leads to the same conclusion as rejection of \\(H_0\\)."
  },
  {
    "objectID": "slides/11-infer-two-means.html#two-sample-pooled-t-test-in-r",
    "href": "slides/11-infer-two-means.html#two-sample-pooled-t-test-in-r",
    "title": "Comparing Two Population Means \n",
    "section": "Two-Sample Pooled t-Test in R",
    "text": "Two-Sample Pooled t-Test in R\n\nn1 = 10; x1_bar = 2.1; s1 = 0.5\nn2 = 10; x2_bar = 4.2; s2 = 0.7\nsp &lt;- sqrt(((n1 - 1) * s1 ^ 2 + (n2 - 1) * s2 ^ 2) / (n1 + n2 - 2))\nsp\n\n[1] 0.61\n\ndf &lt;- n1 + n2 - 2\n## t_test\n(t_test &lt;- (x1_bar - x2_bar) / (sp * sqrt(1 / n1 + 1 / n2)))\n\n[1] -7.7\n\n## t_cv\nqt(p = 0.05, df = df)\n\n[1] -1.7\n\n## p_value\npt(q = t_test, df = df)\n\n[1] 2e-07"
  },
  {
    "objectID": "slides/02-overview-slides.html#statistics-as-numeric-records",
    "href": "slides/02-overview-slides.html#statistics-as-numeric-records",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Statistics as Numeric Records",
    "text": "Statistics as Numeric Records\n\nIn ordinary conversations, the word statistics is used as a term to indicate a set or collection of numeric records.\n\n\n\nFor example, some NBA star‚Äôs career statistics are right here.\nAnyone know who he is? Wanna guess?"
  },
  {
    "objectID": "slides/02-overview-slides.html#statistics-as-numeric-records-1",
    "href": "slides/02-overview-slides.html#statistics-as-numeric-records-1",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Statistics as Numeric Records",
    "text": "Statistics as Numeric Records\n\nIn ordinary conversations, the word statistics is used as a term to indicate a set or collection of numeric records.\n\n\n\n\n\n\nhttps://slamgoods.com/products/jordan-collectors-issue\n\n\n\n\n\nYes, Michael Jordan! My favorite NBA player.\nYou know I was born in 80‚Äôs.\nYou guys didn‚Äôt watch MJ playing, but I did. in TV. OK.\nYour idol may be Kobe, or LBJ or Stephen Curry, but for people born in 80‚Äôs, or your parents, most of them would agree MJ is the greatest of all time.\nOK I am gone too far.\nAgain, in ordinary conversations, the word statistics represents numeric records."
  },
  {
    "objectID": "slides/02-overview-slides.html#section-1",
    "href": "slides/02-overview-slides.html#section-1",
    "title": "Overview of Statistics and Data üìñ",
    "section": "",
    "text": "https://www.amazon.com/Funny-Statistics-Shirt-Definition/dp/B07JKMCDR2?customId=B07537PB8C&customizationToken=MC_Assembly_1%23B07537PB8C&th=1&psc=1\n\n\n\n\n\nAnd somebody defines statistics as the only field where two experts, using identical data, may come to completely opposite conclusions, which is true in some sense.\nAnd well see why later in this course.\nWith the same data, different statistical methods may produce different results and lead to difference conclusions."
  },
  {
    "objectID": "slides/02-overview-slides.html#statistics-as-a-discipline",
    "href": "slides/02-overview-slides.html#statistics-as-a-discipline",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Statistics as a Discipline",
    "text": "Statistics as a Discipline\n\n\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Statistics\n\n\n\n\n\nForget about that useless definition.\nFrom Wiki, Statistics is formally defined as the discipline that concerns the collection, organization, analysis, interpretation and presentation of data.\n\n\n\nStatistics is a subject dealing with data, or Science of Data.\n\n\nSo, Statistics is a Science of Data.\nThere might be another science of data. I‚Äôm not saying statistics is THE science of data.\n\n\n\n\nA science of data using statistical thinking, methods and models.\n\n\n\n\nü§î But wait, then what is DATA SCIENCE ‚ùì"
  },
  {
    "objectID": "slides/02-overview-slides.html#difference-between-statistics-and-data-science",
    "href": "slides/02-overview-slides.html#difference-between-statistics-and-data-science",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Difference between Statistics and Data Science",
    "text": "Difference between Statistics and Data Science\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen the term Data science just came out around 10 years ago, there is no formal definition of data science.\n\nMy ChatGPT says:\n\nStatistics is foundational to Data Science, but Data Science also includes programming, data engineering, machine learning, and business communication."
  },
  {
    "objectID": "slides/02-overview-slides.html#data-science-life-cycle",
    "href": "slides/02-overview-slides.html#data-science-life-cycle",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Data Science Life Cycle",
    "text": "Data Science Life Cycle\n\n\n\nData science is an discipline that helps us turn raw data into understanding, insight, and knowledge that includes multiple steps associated with data.\nThis pipeline is the entrie workflow of data science."
  },
  {
    "objectID": "slides/02-overview-slides.html#uc-santa-cruz-department-of-statistics-courses",
    "href": "slides/02-overview-slides.html#uc-santa-cruz-department-of-statistics-courses",
    "title": "Overview of Statistics and Data üìñ",
    "section": "UC Santa Cruz Department of Statistics Courses",
    "text": "UC Santa Cruz Department of Statistics Courses\n\n\n\nSTAT 5 ‚Äì Statistics\nSTAT 7 ‚Äì Statistical Methods for the Biological, Environmental, and Health Sciences\nSTAT 17 ‚Äì Statistical Methods for Business and Economics\nSTAT 80A ‚Äì Gambling and Gaming\nSTAT 80B ‚Äì The Art of Data Visualization\nSTAT 108 ‚Äì Linear Regression\nSTAT 131 ‚Äì Introduction to Probability Theory\nSTAT 132 ‚Äì Classical and Bayesian Inference\nSTAT 202 ‚Äì Linear Models in SAS\nSTAT 203 ‚Äì Introduction to Probability Theory\nSTAT 204 ‚Äì Introduction to Statistical Data Analysis\nSTAT 205 ‚Äì Introduction to Classical Statistical Learning\nSTAT 205B ‚Äì Intermediate Classical Inference\nSTAT 206 ‚Äì Applied Bayesian Statistics\nSTAT 206B ‚Äì Intermediate Bayesian Inference\nSTAT 207 ‚Äì Intermediate Bayesian Statistical Modeling\nSTAT 208 ‚Äì Linear Statistical Models\nSTAT 209 ‚Äì Generalized Linear Models\n\n\n\nSTAT 221 ‚Äì Statistical Machine Learning\nSTAT 222 ‚Äì Bayesian Nonparametric Methods\nSTAT 223 ‚Äì Time Series Analysis\nSTAT 224 ‚Äì Bayesian Survival Analysis and Clinical Design\nSTAT 225 ‚Äì Multivariate Statistical Methods\nSTAT 226 ‚Äì Spatial Statistics\nSTAT 227 ‚Äì Statistical Learning and High-Dimensional Data Analysis\nSTAT 229 ‚Äì Advanced Bayesian Computation\nSTAT 243 ‚Äì Stochastic Processes\nSTAT 244 ‚Äì Bayesian Decision Theory\nSTAT 246 ‚Äì Probability Theory with Markov Chains\nSTAT 266A ‚Äì Data Visualization and Statistical Programming in R\nSTAT 266B ‚Äì Advanced Statistical Programming in R\nSTAT 266C ‚Äì Introduction to Data Wrangling\n\n\nÔ∏è‚¨õ Methods and models\nüü© Other data science related topics"
  },
  {
    "objectID": "slides/02-overview-slides.html#data-science-may-now-be-a-broader-view-of-statistics",
    "href": "slides/02-overview-slides.html#data-science-may-now-be-a-broader-view-of-statistics",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Data Science May Now Be a Broader View of Statistics",
    "text": "Data Science May Now Be a Broader View of Statistics\nCollection, organization, analysis, interpretation and presentation of data.\n\n\nAlthough statistics deals with Collection, organization, analysis, interpretation and presentation of data, statistics focus much more on data analysis, methods and models.\nAnd now data science is more like a broader view of statistics."
  },
  {
    "objectID": "slides/02-overview-slides.html#what-we-learn-in-this-course",
    "href": "slides/02-overview-slides.html#what-we-learn-in-this-course",
    "title": "Overview of Statistics and Data üìñ",
    "section": "What We Learn In this Course",
    "text": "What We Learn In this Course\n\n\n\n\n\nOpenIntro Statistics Contents\n\n\n\n\n\nIn particular, we will spend most of the time talking about probability and statistical inference methods OK."
  },
  {
    "objectID": "slides/02-overview-slides.html#we-focus-on-statistical-inference",
    "href": "slides/02-overview-slides.html#we-focus-on-statistical-inference",
    "title": "Overview of Statistics and Data üìñ",
    "section": "We Focus On Statistical Inference",
    "text": "We Focus On Statistical Inference\n\nWe spend most of time on various statistical methods for analyzing data.\n\nLearn useful information\n\nabout the population we are interested (e.g.¬†All Marquette students)\nfrom our sample data (e.g.¬†Students in MATH 4720)\nthrough statistical inferential methods, including estimation and testing (e.g.¬†Confidence intervals)\n\n\n\n\n\nDon‚Äôt worry if you have no idea of these terms.\nThese are what we will discuss throughout the course, and I‚Äôll explain each term in detail later in class. OK."
  },
  {
    "objectID": "slides/02-overview-slides.html#statistics-is-a-science-of-data-so-what-is-data",
    "href": "slides/02-overview-slides.html#statistics-is-a-science-of-data-so-what-is-data",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Statistics is a Science of Data, so What is Data?",
    "text": "Statistics is a Science of Data, so What is Data?\n\nData: A set of objects on which we observe or measure one or more characteristics.\nObjects are individuals, observations, subjects or cases in statistical studies.\nA characteristic or attribute is called a variable because it varies from one to another.\n\n\nAll right. Statistics is a Science of Data, so What is Data?\nLet‚Äôs define Data.\nA data set is a set of objects on which we observe or measure one or more characteristics.\nObjects are individuals, observations, subjects or cases in statistical studies.\nA characteristic or attribute is called a variable because it varies from one to another.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nName\nClass\nPos\nHt\nWt\nHometown\nHigh_School\nPPG\nRPG\nAPG\n\n\n\n1\nKam Jones\nSr\nG\n6‚Äô5‚Äù\n205\nMemphis, TN\nEvangelical Christian School\n19.2\n4.5\n5.9\n\n\n2\nChase Ross\nJr\nG\n6‚Äô5‚Äù\n210\nDallas, TX\nCushing Academy\n10.5\n3.8\n2.1\n\n\n4\nStevie Mitchell\nSr\nG\n6‚Äô3‚Äù\n200\nReading, PA\nWilson HS\n10.7\n4.1\n1.6\n\n\n5\nTre Norman\nSo\nG\n6‚Äô4‚Äù\n210\nBoston, MA\nWorcester Academy\n1.9\n1.5\n0.5\n\n\n7\nZaide Lowery\nSo\nG\n6‚Äô5‚Äù\n200\nSpringfield, MO\nLa Lumiere School\n4.1\n3.0\n0.2\n\n\n8\nJoshua Clark\nFr\nF\n7‚Äô1‚Äù\n225\nVirginia Beach, VA\nClements HS\nNA\nNA\nNA\n\n\n10\nDamarius Owens\nFr\nF\n6‚Äô7‚Äù\n200\nRochester, NY\nWestern Reserve Academy\n2.6\n1.2\n0.5\n\n\n12\nBen Gold\nJr\nF\n6‚Äô11‚Äù\n235\nWellington, NZ\nNBA Global Academy\n7.4\n4.3\n0.9\n\n\n13\nRoyce Parham\nFr\nF\n6‚Äô8‚Äù\n230\nPittsburgh, PA\nWestern Reserve Academy\n5.1\n2.2\n0.4\n\n\n21\nAl Amadou\nSo\nF\n6‚Äô9‚Äù\n210\nPhiladelphia, PA\nChestnut Hill Academy\nNA\nNA\nNA\n\n\n22\nSean Jones\nJr (RS)\nG\n5‚Äô10‚Äù\n185\nColumbus, OH\nLincoln HS\nNA\nNA\nNA\n\n\n23\nDavid Joplin\nSr\nF\n6‚Äô8‚Äù\n225\nBrookfield, WI\nBrookfield Central HS\n14.2\n5.4\n1.3\n\n\n25\nJack Anderson\nSr\nG\n6‚Äô4‚Äù\n200\nDavie, FL\nWestern HS\n0.4\n0.4\n0.0\n\n\n35\nCaedin Hamilton\nR-Fr\nF\n6‚Äô9‚Äù\n250\nSanta Maria, CA\nSt.¬†Joseph HS\n1.5\n1.2\n0.6\n\n\n40\nCasey O‚ÄôMalley\nJr\nG\n6‚Äô3‚Äù\n200\nOmaha, NE\nCreighton Prep\n0.6\n0.2\n0.0\n\n\n41\nJonah Lucas\nJr\nG\n6‚Äô1‚Äù\n180\nWest Lafayette, IN\nHarrison HS\n0.0\n0.2\n0.0\n\n\n42\nLuke Jacobson\nFr\nF\n6‚Äô7‚Äù\n215\nSan Luis Obispo, CA\nMission Prep\nNA\nNA\nNA\n\n\n54\nJake Ciardo\nJr\nG\n6‚Äô2‚Äù\n185\nGermantown, WI\nGermantown HS\n0.4\n0.8\n0.0\n\n\n55\nCameron Brown\nSr\nG\n6‚Äô1‚Äù\n215\nPlano, TX\nJohn Paul II HS\n0.4\n0.2\n0.0\n\n\n\n\n\n\n\nFor example, the data set right here is a set of Marquette mens basketball players.\nSo objects are individuals or players in the data.\nAnd each player has several characteristics or attributes shown in columns associated with him.\nFor example, his #, class, position, height, weight, hometown, and high school.\nThese characteristics are called variables because they vary form one to another. Clear?"
  },
  {
    "objectID": "slides/02-overview-slides.html#data-matrix",
    "href": "slides/02-overview-slides.html#data-matrix",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Data Matrix",
    "text": "Data Matrix\n\nEach row corresponds to a unique case or observational unit.\nEach column represents a characteristic or variable.\nThis structure allows new cases to be added as rows or new variables as new columns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nName\nClass\nPos\nHt\nWt\nHometown\nHigh_School\nPPG\nRPG\nAPG\n\n\n\n1\nKam Jones\nSr\nG\n6‚Äô5‚Äù\n205\nMemphis, TN\nEvangelical Christian School\n19.2\n4.5\n5.9\n\n\n2\nChase Ross\nJr\nG\n6‚Äô5‚Äù\n210\nDallas, TX\nCushing Academy\n10.5\n3.8\n2.1\n\n\n4\nStevie Mitchell\nSr\nG\n6‚Äô3‚Äù\n200\nReading, PA\nWilson HS\n10.7\n4.1\n1.6\n\n\n5\nTre Norman\nSo\nG\n6‚Äô4‚Äù\n210\nBoston, MA\nWorcester Academy\n1.9\n1.5\n0.5\n\n\n7\nZaide Lowery\nSo\nG\n6‚Äô5‚Äù\n200\nSpringfield, MO\nLa Lumiere School\n4.1\n3.0\n0.2\n\n\n8\nJoshua Clark\nFr\nF\n7‚Äô1‚Äù\n225\nVirginia Beach, VA\nClements HS\nNA\nNA\nNA\n\n\n10\nDamarius Owens\nFr\nF\n6‚Äô7‚Äù\n200\nRochester, NY\nWestern Reserve Academy\n2.6\n1.2\n0.5\n\n\n12\nBen Gold\nJr\nF\n6‚Äô11‚Äù\n235\nWellington, NZ\nNBA Global Academy\n7.4\n4.3\n0.9\n\n\n13\nRoyce Parham\nFr\nF\n6‚Äô8‚Äù\n230\nPittsburgh, PA\nWestern Reserve Academy\n5.1\n2.2\n0.4\n\n\n21\nAl Amadou\nSo\nF\n6‚Äô9‚Äù\n210\nPhiladelphia, PA\nChestnut Hill Academy\nNA\nNA\nNA\n\n\n22\nSean Jones\nJr (RS)\nG\n5‚Äô10‚Äù\n185\nColumbus, OH\nLincoln HS\nNA\nNA\nNA\n\n\n23\nDavid Joplin\nSr\nF\n6‚Äô8‚Äù\n225\nBrookfield, WI\nBrookfield Central HS\n14.2\n5.4\n1.3\n\n\n25\nJack Anderson\nSr\nG\n6‚Äô4‚Äù\n200\nDavie, FL\nWestern HS\n0.4\n0.4\n0.0\n\n\n35\nCaedin Hamilton\nR-Fr\nF\n6‚Äô9‚Äù\n250\nSanta Maria, CA\nSt.¬†Joseph HS\n1.5\n1.2\n0.6\n\n\n40\nCasey O‚ÄôMalley\nJr\nG\n6‚Äô3‚Äù\n200\nOmaha, NE\nCreighton Prep\n0.6\n0.2\n0.0\n\n\n41\nJonah Lucas\nJr\nG\n6‚Äô1‚Äù\n180\nWest Lafayette, IN\nHarrison HS\n0.0\n0.2\n0.0\n\n\n42\nLuke Jacobson\nFr\nF\n6‚Äô7‚Äù\n215\nSan Luis Obispo, CA\nMission Prep\nNA\nNA\nNA\n\n\n54\nJake Ciardo\nJr\nG\n6‚Äô2‚Äù\n185\nGermantown, WI\nGermantown HS\n0.4\n0.8\n0.0\n\n\n55\nCameron Brown\nSr\nG\n6‚Äô1‚Äù\n215\nPlano, TX\nJohn Paul II HS\n0.4\n0.2\n0.0\n\n\n\n\n\n\n\nAnd we usually store a data set in a matrix form that has rows and columns.\nEach row corresponds to a unique case or observational unit, or the object.\nEach column represents a characteristic or variable.\nThis structure allows new cases to be added as rows or new variables as new columns."
  },
  {
    "objectID": "slides/02-overview-slides.html#target-population",
    "href": "slides/02-overview-slides.html#target-population",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Target Population",
    "text": "Target Population\n\nThe first step in conducting a study is to identify questions to be investigated.\n\nA clear research question is helpful in identifying\n\nwhat cases should be studied (row)\nwhat variables are important (column)\n\n\nTarget Population: the collection of all objects which we are interested in studying from.\n\n\n\n\nWhat is the average GPA of currently enrolled Marquette students?\n\n\n\n\n\n\n\n\n\n\n\n\nTarget Population: The complete collection of data we‚Äôd like to make inference about.\nSo the population is a set of all objects which we are interested in studying from.\nBecause All Marquette undergrads that are currently enrolled is the complete collection of data we‚Äôd like to make inference about.\nEach currently enrolled Marquette undergrad is an object.\nNote that students who are not currently enrolled or students that are already graduated are not our interest, and they shouldn‚Äôt be a part of target population.\nCan anybody tell me what variable associated with Marquette undergrads is our interest?\nSo average GPA is the variable or population property we like to make inference about.\n\n\n\nAll Marquette students that are currently enrolled."
  },
  {
    "objectID": "slides/02-overview-slides.html#target-population-1",
    "href": "slides/02-overview-slides.html#target-population-1",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Target Population",
    "text": "Target Population\n\nThe first step in conducting a study is to identify questions to be investigated.\n\nA clear research question is helpful in identifying\n\nwhat cases should be studied (row)\nwhat variables are important (column)\n\n\nTarget Population: the collection of all objects which we are interested in studying from.\n\n\n\n\nDoes a new drug reduce mortality in patients with severe heart disease?\n\n\n\n\n\n\n\n\n\n\n\n\nOK. Another example. If our research question is Does a new drug reduce mortality in patients with severe heart disease? What is our target population?\nA person with severe heart disease represents a case.\nThe population includes all patients with severe heart disease.\n\n\n\nAll people with severe heart disease."
  },
  {
    "objectID": "slides/02-overview-slides.html#sample-data",
    "href": "slides/02-overview-slides.html#sample-data",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Sample Data",
    "text": "Sample Data\n\n\n\nSometimes, it‚Äôs possible to collect data of all cases we are interested.\nMost of the time, it is too expensive to collect data for every case in a population.\nWhat about the average GPA of all students in Illinois? the U.S.? the world? üò± üò± üò±\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample: A subset of cases selected from a population.\nCompute the average GPA of the sample data\nHope sample avg GPA \\(\\approx\\) population avg GPA. üôè\n\n\nSo sampling is our solution to it.\nA Sample is A subset of cases selected from a population.\nAnd the idea is that OK, we are not able to compute the average GPA of a population, but we collect a sample from that population which has way less objects than the population.\nAnd then we compute the average GPA of the sample data.\nAnd we hope the sample average GPA can be close to the population average GPA because the population GPA is our main interest, not the sample GPA.\nTo have sample average GPA close to population GPA, we want the sample to look like the population so that the sample and the population share similar attribute including GPA."
  },
  {
    "objectID": "slides/02-overview-slides.html#good-sample-vs.-bad-sample",
    "href": "slides/02-overview-slides.html#good-sample-vs.-bad-sample",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Good Sample vs.¬†Bad Sample",
    "text": "Good Sample vs.¬†Bad Sample\n\nIs this 4720/5720 class a sample data of the target population Marquette students?\n\n\n\nIs this 4720/5720 class a ‚Äúgood‚Äù sample of the target population?"
  },
  {
    "objectID": "slides/02-overview-slides.html#good-sample-vs.-bad-sample-1",
    "href": "slides/02-overview-slides.html#good-sample-vs.-bad-sample-1",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Good Sample vs.¬†Bad Sample",
    "text": "Good Sample vs.¬†Bad Sample\n\nIs this 4720/5720 class a ‚Äúgood‚Äù sample of the target population?\n\n\nThe sample is convenient to be collected, but it may NOT be representative of the population.\nBiased sample: The average GPA of the class may be far from that of all Marquette undergrads.\n\n\n\nThe sample is convenient to be collected, but it is NOT representative of the population.\nYou are mostly STEM majors, and so with a high chance, your avg GPA is not the same as the GPA of humanities or business students,right.\n\nBiased sample: The average GPA of you guys may not be close to the average GPA of all Marquette undergrads.\nSample data must be collected in an appropriate way. If not GIGO."
  },
  {
    "objectID": "slides/02-overview-slides.html#how-and-why-a-representative-sample",
    "href": "slides/02-overview-slides.html#how-and-why-a-representative-sample",
    "title": "Overview of Statistics and Data üìñ",
    "section": "How and Why a Representative Sample?",
    "text": "How and Why a Representative Sample?\n\nWe always seek to randomly select a sample from a population.\nLots of statistical methods are based on randomness assumption.\n\n\n\nWe always seek to randomly select a sample from a population because random sampling usually give us a Representative Sample, as long as the sample size, or the number of objects in the sample is not too small.\nSo in the example, the proportion of major in our sample should look pretty similar to the proportion of our target population, which is all marquette students.\nIf data are not collected in a random framework, these statistical methods ‚Äì the estimates and errors associated with the estimates ‚Äì are not reliable."
  },
  {
    "objectID": "slides/02-overview-slides.html#two-types-of-studies-to-collect-sample-data",
    "href": "slides/02-overview-slides.html#two-types-of-studies-to-collect-sample-data",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Two Types of Studies to Collect Sample Data",
    "text": "Two Types of Studies to Collect Sample Data\n\n\nObservational Study: Observe and measure characteristics/variables, and do NOT attempt to modify or intervene with the subjects being studied.\n\n\n Sample from 1Ô∏è‚É£ the heart disease and 2Ô∏è‚É£ heart disease-free populations. Then record the fat content of the diets for the two groups. \n\n\n\n\n\n\nExperimental Study: Apply some treatment(s) and then proceed to observe its responses or effects on the individuals (experimental units).\n\nAssign volunteers to one of several diets with different levels of dietary fat (treatments). Then compare the treatments with respect to the incidence of hear disease after a period of time. \n\n\n\n\n\n\nObservational or Experimental?\n\nRandomly select 40 males and 40 females to see the difference in blood pressure levels between male and female. \n Test the effects of a new drug by randomly dividing patients into 3 groups (high dosage, low dosage, placebo)."
  },
  {
    "objectID": "slides/02-overview-slides.html#limitation-of-observational-studies-confounding",
    "href": "slides/02-overview-slides.html#limitation-of-observational-studies-confounding",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Limitation of Observational Studies: Confounding",
    "text": "Limitation of Observational Studies: Confounding\n\n\nConfounder: A variable NOT included in a study but affects the variables in the study.\nObserve past data show that increases in ice cream sales are associated with increases in drownings, and we conclude that eating ice cream causes drownings. üò± üòï ‚ÅâÔ∏è\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the confounder that is not in the data, but affects ice cream sales and the number of drownings?\n\n\n\n\n\nTemperature: as temperature increases, ice cream sales increase and the number of drownings goes up because more people swim.\n\n\n\n\n\n\n\n\n\n\nAs temperature increases (season), ice cream sales increase and the number of drownings goes up because more people swim."
  },
  {
    "objectID": "slides/02-overview-slides.html#causal-relationship",
    "href": "slides/02-overview-slides.html#causal-relationship",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Causal Relationship",
    "text": "Causal Relationship\n\nMaking causal conclusions based on experiments is often more reasonable than making the same causal conclusions based on observational data.\nObservational studies are generally only sufficient to show associations, not causality."
  },
  {
    "objectID": "slides/02-overview-slides.html#simple-random-sample",
    "href": "slides/02-overview-slides.html#simple-random-sample",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Simple Random Sample",
    "text": "Simple Random Sample\n\nRandom Sample: Each member of a population is equally likely to be selected.\nSimple Random Sample (SRS): Every possible sample of sample size \\(n\\) has the same chance to be chosen.\nExample: If sample 100 students from all, say 10,000 Marquette students, I would randomly assign each student a number (from 1 to 10,000), then randomly select 100 numbers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://research-methodology.net/sampling-in-primary-data-collection/random-sampling/"
  },
  {
    "objectID": "slides/02-overview-slides.html#stratified-random-sample",
    "href": "slides/02-overview-slides.html#stratified-random-sample",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Stratified Random Sample",
    "text": "Stratified Random Sample\n\nStratified Sampling: Subdivide the population into different subgroups (strata) that share the same characteristics, then draw a simple random sample from each subgroup.\nHomogeneous within strata; Non-homogeneous between strata"
  },
  {
    "objectID": "slides/02-overview-slides.html#stratified-random-sample-example",
    "href": "slides/02-overview-slides.html#stratified-random-sample-example",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Stratified Random Sample Example",
    "text": "Stratified Random Sample Example\n\n\nExample: Divide Marquette students into groups by colleges, then SRS for each group."
  },
  {
    "objectID": "slides/02-overview-slides.html#cluster-sampling",
    "href": "slides/02-overview-slides.html#cluster-sampling",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Cluster Sampling",
    "text": "Cluster Sampling\n\nCluster Sampling: Divide the population into clusters, then randomly select some of those clusters, and then choose all the members from those selected clusters.\nHomogeneous between clusters; Non-homogeneous within clusters"
  },
  {
    "objectID": "slides/02-overview-slides.html#cluster-sampling-example",
    "href": "slides/02-overview-slides.html#cluster-sampling-example",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Cluster Sampling Example",
    "text": "Cluster Sampling Example\n\n\nExample: Study 4720 student drinking habit by dividing the students into 9 groups, then randomly selecting 3 and interviewing all of the students in each of those clusters.\n\n\n\nclusters look similar each other, but members in a cluster are not very alike. They have different characteristics.\nHomogeneous between clusters because all classes are STEM classes.\nNon-homogeneous within clusters because students in the same class may have different majors or from different colleges"
  },
  {
    "objectID": "slides/02-overview-slides.html#section-3",
    "href": "slides/02-overview-slides.html#section-3",
    "title": "Overview of Statistics and Data üìñ",
    "section": "",
    "text": "This diagram tell us everything.\nWe are going to learn each data type."
  },
  {
    "objectID": "slides/02-overview-slides.html#categorical-vs.-numerical-variables",
    "href": "slides/02-overview-slides.html#categorical-vs.-numerical-variables",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Categorical vs.¬†Numerical Variables",
    "text": "Categorical vs.¬†Numerical Variables\n\n\nA categorical variable provides non-numerical information which can be placed in one (and only one) category from two or more categories.\n\nGender (Male üë®, Female üë©, Trans üè≥Ô∏è‚Äçüåà) \nClass (Freshman, Sophomore, Junior, Senior, Graduate) \nCountry (USA üá∫üá∏, Canada üá®üá¶, UK üá¨üáß, Germany üá©üá™, Japan üáØüáµ, Korea üá∞üá∑) \n\n\n\nA numerical variable is recorded in a numerical value representing counts or measurements.\n\n GPA \n The number of relationships you‚Äôve had \n Height"
  },
  {
    "objectID": "slides/02-overview-slides.html#numerical-variables-can-be-discrete-or-continuous",
    "href": "slides/02-overview-slides.html#numerical-variables-can-be-discrete-or-continuous",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Numerical Variables can be Discrete or Continuous",
    "text": "Numerical Variables can be Discrete or Continuous\n\nA discrete variable takes on values of a finite or countable number.\n\nA continuous variable takes on values anywhere over a particular range without gaps or jumps.\n\n GPA is continuous because it can be any value between 0 and 4. \n The number of relationships you‚Äôve had is discrete because you can count the number and it is finite.\n Height is continuous because it can be any number within a range."
  },
  {
    "objectID": "slides/02-overview-slides.html#categorical-variables-are-usually-recorded-as-numbers",
    "href": "slides/02-overview-slides.html#categorical-variables-are-usually-recorded-as-numbers",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Categorical Variables are Usually Recorded as Numbers",
    "text": "Categorical Variables are Usually Recorded as Numbers\n\nGender (Male = 0, Female = 1, Trans = 2) \nClass (Freshman = 1, Sophomore = 2, Junior = 3, Senior = 4, Graduate = 5) \nCountry (USA = 100, Canada = 101, UK = 200, Germany = 201, Japan = 300, Korea = 301) \nUnited Airlines boarding groups\n\nThe numbers represent categories only; differences between them are meaningless.\n\nCanada - USA = 101 - 100 = 1?\nGraduate - Sophomore = 5 - 2 = 3 = Junior?\n\n\nWe need to learn the level of measurements to know whether or which arithmetic operations are meaningful."
  },
  {
    "objectID": "slides/02-overview-slides.html#levels-of-measurements-nominal-and-ordinal-for-categorical-variables",
    "href": "slides/02-overview-slides.html#levels-of-measurements-nominal-and-ordinal-for-categorical-variables",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Levels of Measurements: Nominal and Ordinal for Categorical Variables",
    "text": "Levels of Measurements: Nominal and Ordinal for Categorical Variables\n\n\nNominal: The data can NOT be ordered in a meaningful or natural way.\n\n\nGender (Male = 0, Female = 1, Trans = 2)  is nominal because Male, Female and Trans cannot be ordered.\n\nCountry (USA = 100, Canada = 101, UK = 200, Germany = 201, Japan = 300, Korea = 301)  is nominal.\n\n\n\n\n\n\nOrdinal: The data can be arranged in some meaningful order, but differences between data values can NOT be determined or are meaningless.\n\n\nClass (Freshman = 1, Sophomore = 2, Junior = 3, Senior = 4, Graduate = 5)  is ordinal because Sophomore is one class higher than Freshman."
  },
  {
    "objectID": "slides/02-overview-slides.html#levels-of-measurements-interval-and-ratio-for-numerical-variables",
    "href": "slides/02-overview-slides.html#levels-of-measurements-interval-and-ratio-for-numerical-variables",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Levels of Measurements: Interval and Ratio for Numerical Variables",
    "text": "Levels of Measurements: Interval and Ratio for Numerical Variables\n\n\nInterval: The data have meaningful difference between any two values. But the data do NOT have a natural zero or starting point. The data can do \\(\\color{red} +\\) and \\(\\color{red} -\\), but can‚Äôt reasonably do \\(\\color{red} \\times\\) and \\(\\color{red} \\div\\).\n\n\nTemperature is interval because \\(80^{\\circ}\\)F is 40 degrees higher than \\(40^{\\circ}\\)F \\((80-40=40)\\), but \\(0^{\\circ}\\) does not mean NO heat and \\(80^{\\circ}\\)F is NOT twice as hot as \\(40^{\\circ}\\)F.\n\n\n\n\n\n\nRatio: The data have both meaningful differences and ratios, and there is a natural zero starting point that indicates none of the quantity. The data can do \\(\\color{red} +\\), \\(\\color{red} -\\), \\(\\color{red} \\times\\) and \\(\\color{red} \\div\\).\n\n\nDistance is ratio because \\(80\\) miles is twice as far as \\(40\\) miles \\((80/40 = 2)\\), and \\(0\\) mile means no distance."
  },
  {
    "objectID": "slides/02-overview-slides.html#converting-numerical-to-categorical",
    "href": "slides/02-overview-slides.html#converting-numerical-to-categorical",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Converting Numerical to Categorical",
    "text": "Converting Numerical to Categorical\n\nYou‚Äôve already seen an example.\n\n\n\n\n\n\nGrade\nPercentage\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[73, 77)\n\n\nC-\n[70, 73)\n\n\nD+\n[65, 70)\n\n\nD\n[60, 65)\n\n\nF\n[0, 60)"
  },
  {
    "objectID": "slides/02-overview-slides.html#section-5",
    "href": "slides/02-overview-slides.html#section-5",
    "title": "Overview of Statistics and Data üìñ",
    "section": "",
    "text": "Identify data type of each variable in the Marquette men‚Äôs basketball player data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nName\nClass\nPos\nHt\nWt\nHometown\nHigh_School\nPPG\nRPG\nAPG\n\n\n\n1\nKam Jones\nSr\nG\n6‚Äô5‚Äù\n205\nMemphis, TN\nEvangelical Christian School\n19.2\n4.5\n5.9\n\n\n2\nChase Ross\nJr\nG\n6‚Äô5‚Äù\n210\nDallas, TX\nCushing Academy\n10.5\n3.8\n2.1\n\n\n4\nStevie Mitchell\nSr\nG\n6‚Äô3‚Äù\n200\nReading, PA\nWilson HS\n10.7\n4.1\n1.6\n\n\n5\nTre Norman\nSo\nG\n6‚Äô4‚Äù\n210\nBoston, MA\nWorcester Academy\n1.9\n1.5\n0.5\n\n\n7\nZaide Lowery\nSo\nG\n6‚Äô5‚Äù\n200\nSpringfield, MO\nLa Lumiere School\n4.1\n3.0\n0.2\n\n\n8\nJoshua Clark\nFr\nF\n7‚Äô1‚Äù\n225\nVirginia Beach, VA\nClements HS\nNA\nNA\nNA\n\n\n10\nDamarius Owens\nFr\nF\n6‚Äô7‚Äù\n200\nRochester, NY\nWestern Reserve Academy\n2.6\n1.2\n0.5\n\n\n12\nBen Gold\nJr\nF\n6‚Äô11‚Äù\n235\nWellington, NZ\nNBA Global Academy\n7.4\n4.3\n0.9\n\n\n13\nRoyce Parham\nFr\nF\n6‚Äô8‚Äù\n230\nPittsburgh, PA\nWestern Reserve Academy\n5.1\n2.2\n0.4\n\n\n21\nAl Amadou\nSo\nF\n6‚Äô9‚Äù\n210\nPhiladelphia, PA\nChestnut Hill Academy\nNA\nNA\nNA\n\n\n22\nSean Jones\nJr (RS)\nG\n5‚Äô10‚Äù\n185\nColumbus, OH\nLincoln HS\nNA\nNA\nNA\n\n\n23\nDavid Joplin\nSr\nF\n6‚Äô8‚Äù\n225\nBrookfield, WI\nBrookfield Central HS\n14.2\n5.4\n1.3\n\n\n25\nJack Anderson\nSr\nG\n6‚Äô4‚Äù\n200\nDavie, FL\nWestern HS\n0.4\n0.4\n0.0\n\n\n35\nCaedin Hamilton\nR-Fr\nF\n6‚Äô9‚Äù\n250\nSanta Maria, CA\nSt.¬†Joseph HS\n1.5\n1.2\n0.6\n\n\n40\nCasey O‚ÄôMalley\nJr\nG\n6‚Äô3‚Äù\n200\nOmaha, NE\nCreighton Prep\n0.6\n0.2\n0.0\n\n\n41\nJonah Lucas\nJr\nG\n6‚Äô1‚Äù\n180\nWest Lafayette, IN\nHarrison HS\n0.0\n0.2\n0.0\n\n\n42\nLuke Jacobson\nFr\nF\n6‚Äô7‚Äù\n215\nSan Luis Obispo, CA\nMission Prep\nNA\nNA\nNA\n\n\n54\nJake Ciardo\nJr\nG\n6‚Äô2‚Äù\n185\nGermantown, WI\nGermantown HS\n0.4\n0.8\n0.0\n\n\n55\nCameron Brown\nSr\nG\n6‚Äô1‚Äù\n215\nPlano, TX\nJohn Paul II HS\n0.4\n0.2\n0.0\n\n\n\n\n\n\n\n  \n    ‚àí\n    +\n \n 03:00"
  },
  {
    "objectID": "slides/02-overview.html#statistics-as-numeric-records",
    "href": "slides/02-overview.html#statistics-as-numeric-records",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Statistics as Numeric Records",
    "text": "Statistics as Numeric Records\n\nIn ordinary conversations, the word statistics is used as a term to indicate a set or collection of numeric records.\n\n\n\n\n\n\n\n\n\n\n\nFor example, some NBA star‚Äôs career statistics are right here.\nAnyone know who he is? Wanna guess?"
  },
  {
    "objectID": "slides/02-overview.html#statistics-as-numeric-records-1",
    "href": "slides/02-overview.html#statistics-as-numeric-records-1",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Statistics as Numeric Records",
    "text": "Statistics as Numeric Records\n\nIn ordinary conversations, the word statistics is used as a term to indicate a set or collection of numeric records.\n\n\n\n\n\n\nhttps://slamgoods.com/products/jordan-collectors-issue\n\n\n\n\n\n\nYes, Michael Jordan! My favorite NBA player.\nYou know I was born in 80‚Äôs.\nYou guys didn‚Äôt watch MJ playing, but I did. in TV. OK.\nYour idol may be Kobe, or LBJ or Stephen Curry, but for people born in 80‚Äôs, or your parents, most of them would agree MJ is the greatest of all time.\nOK I am gone too far.\nAgain, in ordinary conversations, the word statistics represents numeric records."
  },
  {
    "objectID": "slides/02-overview.html#section-1",
    "href": "slides/02-overview.html#section-1",
    "title": "Overview of Statistics and Data üìñ",
    "section": "",
    "text": "https://www.amazon.com/Funny-Statistics-Shirt-Definition/dp/B07JKMCDR2?customId=B07537PB8C&customizationToken=MC_Assembly_1%23B07537PB8C&th=1&psc=1\n\n\n\n\n\n\nAnd somebody defines statistics as the only field where two experts, using identical data, may come to completely opposite conclusions, which is true in some sense.\nAnd well see why later in this course.\nWith the same data, different statistical methods may produce different results and lead to difference conclusions."
  },
  {
    "objectID": "slides/02-overview.html#statistics-as-a-discipline",
    "href": "slides/02-overview.html#statistics-as-a-discipline",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Statistics as a Discipline",
    "text": "Statistics as a Discipline\n\n\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Statistics\n\n\n\n\n\n\nForget about that useless definition.\nFrom Wiki, Statistics is formally defined as the discipline that concerns the collection, organization, analysis, interpretation and presentation of data.\n\n\n. . .\n\n\nStatistics is a subject dealing with data, or Science of Data.\n\n\n\nSo, Statistics is a Science of Data.\nThere might be another science of data. I‚Äôm not saying statistics is THE science of data.\n\n\n. . .\n\nA science of data using statistical thinking, methods and models.\n\n. . .\n\nü§î But wait, then what is DATA SCIENCE ‚ùì"
  },
  {
    "objectID": "slides/02-overview.html#difference-between-statistics-and-data-science",
    "href": "slides/02-overview.html#difference-between-statistics-and-data-science",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Difference between Statistics and Data Science",
    "text": "Difference between Statistics and Data Science\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen the term Data science just came out around 10 years ago, there is no formal definition of data science.\n\n\n. . .\nMy ChatGPT says:\n\nStatistics is foundational to Data Science, but Data Science also includes programming, data engineering, machine learning, and business communication."
  },
  {
    "objectID": "slides/02-overview.html#data-science-life-cycle",
    "href": "slides/02-overview.html#data-science-life-cycle",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Data Science Life Cycle",
    "text": "Data Science Life Cycle\n\n\n\n\n\n\n\n\n\n\n\nData science is an discipline that helps us turn raw data into understanding, insight, and knowledge that includes multiple steps associated with data.\nThis pipeline is the entrie workflow of data science."
  },
  {
    "objectID": "slides/02-overview.html#uc-santa-cruz-department-of-statistics-courses",
    "href": "slides/02-overview.html#uc-santa-cruz-department-of-statistics-courses",
    "title": "Overview of Statistics and Data üìñ",
    "section": "UC Santa Cruz Department of Statistics Courses",
    "text": "UC Santa Cruz Department of Statistics Courses\n\n\n\nSTAT 5 ‚Äì Statistics\nSTAT 7 ‚Äì Statistical Methods for the Biological, Environmental, and Health Sciences\nSTAT 17 ‚Äì Statistical Methods for Business and Economics\nSTAT 80A ‚Äì Gambling and Gaming\nSTAT 80B ‚Äì The Art of Data Visualization\nSTAT 108 ‚Äì Linear Regression\nSTAT 131 ‚Äì Introduction to Probability Theory\nSTAT 132 ‚Äì Classical and Bayesian Inference\nSTAT 202 ‚Äì Linear Models in SAS\nSTAT 203 ‚Äì Introduction to Probability Theory\nSTAT 204 ‚Äì Introduction to Statistical Data Analysis\nSTAT 205 ‚Äì Introduction to Classical Statistical Learning\nSTAT 205B ‚Äì Intermediate Classical Inference\nSTAT 206 ‚Äì Applied Bayesian Statistics\nSTAT 206B ‚Äì Intermediate Bayesian Inference\nSTAT 207 ‚Äì Intermediate Bayesian Statistical Modeling\nSTAT 208 ‚Äì Linear Statistical Models\nSTAT 209 ‚Äì Generalized Linear Models\n\n\n\nSTAT 221 ‚Äì Statistical Machine Learning\nSTAT 222 ‚Äì Bayesian Nonparametric Methods\nSTAT 223 ‚Äì Time Series Analysis\nSTAT 224 ‚Äì Bayesian Survival Analysis and Clinical Design\nSTAT 225 ‚Äì Multivariate Statistical Methods\nSTAT 226 ‚Äì Spatial Statistics\nSTAT 227 ‚Äì Statistical Learning and High-Dimensional Data Analysis\nSTAT 229 ‚Äì Advanced Bayesian Computation\nSTAT 243 ‚Äì Stochastic Processes\nSTAT 244 ‚Äì Bayesian Decision Theory\nSTAT 246 ‚Äì Probability Theory with Markov Chains\nSTAT 266A ‚Äì Data Visualization and Statistical Programming in R\nSTAT 266B ‚Äì Advanced Statistical Programming in R\nSTAT 266C ‚Äì Introduction to Data Wrangling\n\n\nÔ∏è‚¨õ Methods and models\nüü© Other data science related topics"
  },
  {
    "objectID": "slides/02-overview.html#data-science-may-now-be-a-broader-view-of-statistics",
    "href": "slides/02-overview.html#data-science-may-now-be-a-broader-view-of-statistics",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Data Science May Now Be a Broader View of Statistics",
    "text": "Data Science May Now Be a Broader View of Statistics\nCollection, organization, analysis, interpretation and presentation of data.\n\n\n\n\n\n\n\n\n\n\nAlthough statistics deals with Collection, organization, analysis, interpretation and presentation of data, statistics focus much more on data analysis, methods and models.\nAnd now data science is more like a broader view of statistics."
  },
  {
    "objectID": "slides/02-overview.html#what-we-learn-in-this-course",
    "href": "slides/02-overview.html#what-we-learn-in-this-course",
    "title": "Overview of Statistics and Data üìñ",
    "section": "What We Learn In this Course",
    "text": "What We Learn In this Course\n\n\n\n\n\nOpenIntro Statistics Contents\n\n\n\n\n\n\nIn particular, we will spend most of the time talking about probability and statistical inference methods OK."
  },
  {
    "objectID": "slides/02-overview.html#we-focus-on-statistical-inference",
    "href": "slides/02-overview.html#we-focus-on-statistical-inference",
    "title": "Overview of Statistics and Data üìñ",
    "section": "We Focus On Statistical Inference",
    "text": "We Focus On Statistical Inference\n\nWe spend most of time on various statistical methods for analyzing data.\n\nLearn useful information\n\nabout the population we are interested (e.g.¬†All Marquette students)\nfrom our sample data (e.g.¬†Students in MATH 4720)\nthrough statistical inferential methods, including estimation and testing (e.g.¬†Confidence intervals)\n\n\n\n\n\n\n\n\n\n\n\n\n\nDon‚Äôt worry if you have no idea of these terms.\nThese are what we will discuss throughout the course, and I‚Äôll explain each term in detail later in class. OK."
  },
  {
    "objectID": "slides/02-overview.html#statistics-is-a-science-of-data-so-what-is-data",
    "href": "slides/02-overview.html#statistics-is-a-science-of-data-so-what-is-data",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Statistics is a Science of Data, so What is Data?",
    "text": "Statistics is a Science of Data, so What is Data?\n\nData: A set of objects on which we observe or measure one or more characteristics.\nObjects are individuals, observations, subjects or cases in statistical studies.\nA characteristic or attribute is called a variable because it varies from one to another.\n\n\n\nAll right. Statistics is a Science of Data, so What is Data?\nLet‚Äôs define Data.\nA data set is a set of objects on which we observe or measure one or more characteristics.\nObjects are individuals, observations, subjects or cases in statistical studies.\nA characteristic or attribute is called a variable because it varies from one to another.\n\n\n. . .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nName\nClass\nPos\nHt\nWt\nHometown\nHigh_School\nPPG\nRPG\nAPG\n\n\n\n1\nKam Jones\nSr\nG\n6‚Äô5‚Äù\n205\nMemphis, TN\nEvangelical Christian School\n19.2\n4.5\n5.9\n\n\n2\nChase Ross\nJr\nG\n6‚Äô5‚Äù\n210\nDallas, TX\nCushing Academy\n10.5\n3.8\n2.1\n\n\n4\nStevie Mitchell\nSr\nG\n6‚Äô3‚Äù\n200\nReading, PA\nWilson HS\n10.7\n4.1\n1.6\n\n\n5\nTre Norman\nSo\nG\n6‚Äô4‚Äù\n210\nBoston, MA\nWorcester Academy\n1.9\n1.5\n0.5\n\n\n7\nZaide Lowery\nSo\nG\n6‚Äô5‚Äù\n200\nSpringfield, MO\nLa Lumiere School\n4.1\n3.0\n0.2\n\n\n8\nJoshua Clark\nFr\nF\n7‚Äô1‚Äù\n225\nVirginia Beach, VA\nClements HS\nNA\nNA\nNA\n\n\n10\nDamarius Owens\nFr\nF\n6‚Äô7‚Äù\n200\nRochester, NY\nWestern Reserve Academy\n2.6\n1.2\n0.5\n\n\n12\nBen Gold\nJr\nF\n6‚Äô11‚Äù\n235\nWellington, NZ\nNBA Global Academy\n7.4\n4.3\n0.9\n\n\n13\nRoyce Parham\nFr\nF\n6‚Äô8‚Äù\n230\nPittsburgh, PA\nWestern Reserve Academy\n5.1\n2.2\n0.4\n\n\n21\nAl Amadou\nSo\nF\n6‚Äô9‚Äù\n210\nPhiladelphia, PA\nChestnut Hill Academy\nNA\nNA\nNA\n\n\n22\nSean Jones\nJr (RS)\nG\n5‚Äô10‚Äù\n185\nColumbus, OH\nLincoln HS\nNA\nNA\nNA\n\n\n23\nDavid Joplin\nSr\nF\n6‚Äô8‚Äù\n225\nBrookfield, WI\nBrookfield Central HS\n14.2\n5.4\n1.3\n\n\n25\nJack Anderson\nSr\nG\n6‚Äô4‚Äù\n200\nDavie, FL\nWestern HS\n0.4\n0.4\n0.0\n\n\n35\nCaedin Hamilton\nR-Fr\nF\n6‚Äô9‚Äù\n250\nSanta Maria, CA\nSt.¬†Joseph HS\n1.5\n1.2\n0.6\n\n\n40\nCasey O‚ÄôMalley\nJr\nG\n6‚Äô3‚Äù\n200\nOmaha, NE\nCreighton Prep\n0.6\n0.2\n0.0\n\n\n41\nJonah Lucas\nJr\nG\n6‚Äô1‚Äù\n180\nWest Lafayette, IN\nHarrison HS\n0.0\n0.2\n0.0\n\n\n42\nLuke Jacobson\nFr\nF\n6‚Äô7‚Äù\n215\nSan Luis Obispo, CA\nMission Prep\nNA\nNA\nNA\n\n\n54\nJake Ciardo\nJr\nG\n6‚Äô2‚Äù\n185\nGermantown, WI\nGermantown HS\n0.4\n0.8\n0.0\n\n\n55\nCameron Brown\nSr\nG\n6‚Äô1‚Äù\n215\nPlano, TX\nJohn Paul II HS\n0.4\n0.2\n0.0\n\n\n\n\n\n\n\n\nFor example, the data set right here is a set of Marquette mens basketball players.\nSo objects are individuals or players in the data.\nAnd each player has several characteristics or attributes shown in columns associated with him.\nFor example, his #, class, position, height, weight, hometown, and high school.\nThese characteristics are called variables because they vary form one to another. Clear?"
  },
  {
    "objectID": "slides/02-overview.html#data-matrix",
    "href": "slides/02-overview.html#data-matrix",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Data Matrix",
    "text": "Data Matrix\n\nEach row corresponds to a unique case or observational unit.\nEach column represents a characteristic or variable.\nThis structure allows new cases to be added as rows or new variables as new columns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nName\nClass\nPos\nHt\nWt\nHometown\nHigh_School\nPPG\nRPG\nAPG\n\n\n\n1\nKam Jones\nSr\nG\n6‚Äô5‚Äù\n205\nMemphis, TN\nEvangelical Christian School\n19.2\n4.5\n5.9\n\n\n2\nChase Ross\nJr\nG\n6‚Äô5‚Äù\n210\nDallas, TX\nCushing Academy\n10.5\n3.8\n2.1\n\n\n4\nStevie Mitchell\nSr\nG\n6‚Äô3‚Äù\n200\nReading, PA\nWilson HS\n10.7\n4.1\n1.6\n\n\n5\nTre Norman\nSo\nG\n6‚Äô4‚Äù\n210\nBoston, MA\nWorcester Academy\n1.9\n1.5\n0.5\n\n\n7\nZaide Lowery\nSo\nG\n6‚Äô5‚Äù\n200\nSpringfield, MO\nLa Lumiere School\n4.1\n3.0\n0.2\n\n\n8\nJoshua Clark\nFr\nF\n7‚Äô1‚Äù\n225\nVirginia Beach, VA\nClements HS\nNA\nNA\nNA\n\n\n10\nDamarius Owens\nFr\nF\n6‚Äô7‚Äù\n200\nRochester, NY\nWestern Reserve Academy\n2.6\n1.2\n0.5\n\n\n12\nBen Gold\nJr\nF\n6‚Äô11‚Äù\n235\nWellington, NZ\nNBA Global Academy\n7.4\n4.3\n0.9\n\n\n13\nRoyce Parham\nFr\nF\n6‚Äô8‚Äù\n230\nPittsburgh, PA\nWestern Reserve Academy\n5.1\n2.2\n0.4\n\n\n21\nAl Amadou\nSo\nF\n6‚Äô9‚Äù\n210\nPhiladelphia, PA\nChestnut Hill Academy\nNA\nNA\nNA\n\n\n22\nSean Jones\nJr (RS)\nG\n5‚Äô10‚Äù\n185\nColumbus, OH\nLincoln HS\nNA\nNA\nNA\n\n\n23\nDavid Joplin\nSr\nF\n6‚Äô8‚Äù\n225\nBrookfield, WI\nBrookfield Central HS\n14.2\n5.4\n1.3\n\n\n25\nJack Anderson\nSr\nG\n6‚Äô4‚Äù\n200\nDavie, FL\nWestern HS\n0.4\n0.4\n0.0\n\n\n35\nCaedin Hamilton\nR-Fr\nF\n6‚Äô9‚Äù\n250\nSanta Maria, CA\nSt.¬†Joseph HS\n1.5\n1.2\n0.6\n\n\n40\nCasey O‚ÄôMalley\nJr\nG\n6‚Äô3‚Äù\n200\nOmaha, NE\nCreighton Prep\n0.6\n0.2\n0.0\n\n\n41\nJonah Lucas\nJr\nG\n6‚Äô1‚Äù\n180\nWest Lafayette, IN\nHarrison HS\n0.0\n0.2\n0.0\n\n\n42\nLuke Jacobson\nFr\nF\n6‚Äô7‚Äù\n215\nSan Luis Obispo, CA\nMission Prep\nNA\nNA\nNA\n\n\n54\nJake Ciardo\nJr\nG\n6‚Äô2‚Äù\n185\nGermantown, WI\nGermantown HS\n0.4\n0.8\n0.0\n\n\n55\nCameron Brown\nSr\nG\n6‚Äô1‚Äù\n215\nPlano, TX\nJohn Paul II HS\n0.4\n0.2\n0.0\n\n\n\n\n\n\n\n\nAnd we usually store a data set in a matrix form that has rows and columns.\nEach row corresponds to a unique case or observational unit, or the object.\nEach column represents a characteristic or variable.\nThis structure allows new cases to be added as rows or new variables as new columns."
  },
  {
    "objectID": "slides/02-overview.html#target-population",
    "href": "slides/02-overview.html#target-population",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Target Population",
    "text": "Target Population\n\nThe first step in conducting a study is to identify questions to be investigated.\n\nA clear research question is helpful in identifying\n\nwhat cases should be studied (row)\nwhat variables are important (column)\n\n\nTarget Population: the collection of all objects which we are interested in studying from.\n\n\n\n\nWhat is the average GPA of currently enrolled Marquette students?\n\n\n\n\n\n\n\n\n\n\n\n\n\nTarget Population: The complete collection of data we‚Äôd like to make inference about.\nSo the population is a set of all objects which we are interested in studying from.\nBecause All Marquette undergrads that are currently enrolled is the complete collection of data we‚Äôd like to make inference about.\nEach currently enrolled Marquette undergrad is an object.\nNote that students who are not currently enrolled or students that are already graduated are not our interest, and they shouldn‚Äôt be a part of target population.\nCan anybody tell me what variable associated with Marquette undergrads is our interest?\nSo average GPA is the variable or population property we like to make inference about.\n\n\n\n\nAll Marquette students that are currently enrolled."
  },
  {
    "objectID": "slides/02-overview.html#target-population-1",
    "href": "slides/02-overview.html#target-population-1",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Target Population",
    "text": "Target Population\n\nThe first step in conducting a study is to identify questions to be investigated.\n\nA clear research question is helpful in identifying\n\nwhat cases should be studied (row)\nwhat variables are important (column)\n\n\nTarget Population: the collection of all objects which we are interested in studying from.\n\n\n\n\nDoes a new drug reduce mortality in patients with severe heart disease?\n\n\n\n\n\n\n\n\n\n\n\n\n\nOK. Another example. If our research question is Does a new drug reduce mortality in patients with severe heart disease? What is our target population?\nA person with severe heart disease represents a case.\nThe population includes all patients with severe heart disease.\n\n\n\n\nAll people with severe heart disease."
  },
  {
    "objectID": "slides/02-overview.html#sample-data",
    "href": "slides/02-overview.html#sample-data",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Sample Data",
    "text": "Sample Data\n\n\n\nSometimes, it‚Äôs possible to collect data of all cases we are interested.\nMost of the time, it is too expensive to collect data for every case in a population.\nWhat about the average GPA of all students in Illinois? the U.S.? the world? üò± üò± üò±\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\nSample: A subset of cases selected from a population.\nCompute the average GPA of the sample data\nHope sample avg GPA \\(\\approx\\) population avg GPA. üôè\n\n\n\nSo sampling is our solution to it.\nA Sample is A subset of cases selected from a population.\nAnd the idea is that OK, we are not able to compute the average GPA of a population, but we collect a sample from that population which has way less objects than the population.\nAnd then we compute the average GPA of the sample data.\nAnd we hope the sample average GPA can be close to the population average GPA because the population GPA is our main interest, not the sample GPA.\nTo have sample average GPA close to population GPA, we want the sample to look like the population so that the sample and the population share similar attribute including GPA."
  },
  {
    "objectID": "slides/02-overview.html#good-sample-vs.-bad-sample",
    "href": "slides/02-overview.html#good-sample-vs.-bad-sample",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Good Sample vs.¬†Bad Sample",
    "text": "Good Sample vs.¬†Bad Sample\n\nIs this 4720/5720 class a sample data of the target population Marquette students?\n\n. . .\n\nIs this 4720/5720 class a ‚Äúgood‚Äù sample of the target population?\n\n. . ."
  },
  {
    "objectID": "slides/02-overview.html#good-sample-vs.-bad-sample-1",
    "href": "slides/02-overview.html#good-sample-vs.-bad-sample-1",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Good Sample vs.¬†Bad Sample",
    "text": "Good Sample vs.¬†Bad Sample\n\nIs this 4720/5720 class a ‚Äúgood‚Äù sample of the target population?\n\n\nThe sample is convenient to be collected, but it may NOT be representative of the population.\nBiased sample: The average GPA of the class may be far from that of all Marquette undergrads.\n\n\n\n\n\n\n\n\n\n\n\nThe sample is convenient to be collected, but it is NOT representative of the population.\nYou are mostly STEM majors, and so with a high chance, your avg GPA is not the same as the GPA of humanities or business students,right.\n\nBiased sample: The average GPA of you guys may not be close to the average GPA of all Marquette undergrads.\nSample data must be collected in an appropriate way. If not GIGO."
  },
  {
    "objectID": "slides/02-overview.html#how-and-why-a-representative-sample",
    "href": "slides/02-overview.html#how-and-why-a-representative-sample",
    "title": "Overview of Statistics and Data üìñ",
    "section": "How and Why a Representative Sample?",
    "text": "How and Why a Representative Sample?\n\nWe always seek to randomly select a sample from a population.\nLots of statistical methods are based on randomness assumption.\n\n\n\n\n\n\n\n\n\n\n\nWe always seek to randomly select a sample from a population because random sampling usually give us a Representative Sample, as long as the sample size, or the number of objects in the sample is not too small.\nSo in the example, the proportion of major in our sample should look pretty similar to the proportion of our target population, which is all marquette students.\nIf data are not collected in a random framework, these statistical methods ‚Äì the estimates and errors associated with the estimates ‚Äì are not reliable."
  },
  {
    "objectID": "slides/02-overview.html#two-types-of-studies-to-collect-sample-data",
    "href": "slides/02-overview.html#two-types-of-studies-to-collect-sample-data",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Two Types of Studies to Collect Sample Data",
    "text": "Two Types of Studies to Collect Sample Data\n\n\nObservational Study: Observe and measure characteristics/variables, and do NOT attempt to modify or intervene with the subjects being studied.\n\n\n Sample from 1Ô∏è‚É£ the heart disease and 2Ô∏è‚É£ heart disease-free populations. Then record the fat content of the diets for the two groups. \n\n\n\n. . .\n\n\nExperimental Study: Apply some treatment(s) and then proceed to observe its responses or effects on the individuals (experimental units).\n\nAssign volunteers to one of several diets with different levels of dietary fat (treatments). Then compare the treatments with respect to the incidence of hear disease after a period of time. \n\n\n\n. . .\n\nObservational or Experimental?\n\nRandomly select 40 males and 40 females to see the difference in blood pressure levels between male and female. \n Test the effects of a new drug by randomly dividing patients into 3 groups (high dosage, low dosage, placebo)."
  },
  {
    "objectID": "slides/02-overview.html#limitation-of-observational-studies-confounding",
    "href": "slides/02-overview.html#limitation-of-observational-studies-confounding",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Limitation of Observational Studies: Confounding",
    "text": "Limitation of Observational Studies: Confounding\n\n\nConfounder: A variable NOT included in a study but affects the variables in the study.\nObserve past data show that increases in ice cream sales are associated with increases in drownings, and we conclude that eating ice cream causes drownings. üò± üòï ‚ÅâÔ∏è\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\nWhat is the confounder that is not in the data, but affects ice cream sales and the number of drownings?\n\n. . .\n\n\nTemperature: as temperature increases, ice cream sales increase and the number of drownings goes up because more people swim.\n\n\n\n\n\n\n\n\n\n\n\n\nAs temperature increases (season), ice cream sales increase and the number of drownings goes up because more people swim."
  },
  {
    "objectID": "slides/02-overview.html#causal-relationship",
    "href": "slides/02-overview.html#causal-relationship",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Causal Relationship",
    "text": "Causal Relationship\n\nMaking causal conclusions based on experiments is often more reasonable than making the same causal conclusions based on observational data.\nObservational studies are generally only sufficient to show associations, not causality."
  },
  {
    "objectID": "slides/02-overview.html#simple-random-sample",
    "href": "slides/02-overview.html#simple-random-sample",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Simple Random Sample",
    "text": "Simple Random Sample\n\nRandom Sample: Each member of a population is equally likely to be selected.\nSimple Random Sample (SRS): Every possible sample of sample size \\(n\\) has the same chance to be chosen.\nExample: If sample 100 students from all, say 10,000 Marquette students, I would randomly assign each student a number (from 1 to 10,000), then randomly select 100 numbers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://research-methodology.net/sampling-in-primary-data-collection/random-sampling/"
  },
  {
    "objectID": "slides/02-overview.html#stratified-random-sample",
    "href": "slides/02-overview.html#stratified-random-sample",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Stratified Random Sample",
    "text": "Stratified Random Sample\n\nStratified Sampling: Subdivide the population into different subgroups (strata) that share the same characteristics, then draw a simple random sample from each subgroup.\nHomogeneous within strata; Non-homogeneous between strata"
  },
  {
    "objectID": "slides/02-overview.html#stratified-random-sample-example",
    "href": "slides/02-overview.html#stratified-random-sample-example",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Stratified Random Sample Example",
    "text": "Stratified Random Sample Example\n\n\nExample: Divide Marquette students into groups by colleges, then SRS for each group."
  },
  {
    "objectID": "slides/02-overview.html#cluster-sampling",
    "href": "slides/02-overview.html#cluster-sampling",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Cluster Sampling",
    "text": "Cluster Sampling\n\nCluster Sampling: Divide the population into clusters, then randomly select some of those clusters, and then choose all the members from those selected clusters.\nHomogeneous between clusters; Non-homogeneous within clusters"
  },
  {
    "objectID": "slides/02-overview.html#cluster-sampling-example",
    "href": "slides/02-overview.html#cluster-sampling-example",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Cluster Sampling Example",
    "text": "Cluster Sampling Example\n\n\nExample: Study 4720 student drinking habit by dividing the students into 9 groups, then randomly selecting 3 and interviewing all of the students in each of those clusters.\n\n\n\n\n\n\n\n\n\n\n\nclusters look similar each other, but members in a cluster are not very alike. They have different characteristics.\nHomogeneous between clusters because all classes are STEM classes.\nNon-homogeneous within clusters because students in the same class may have different majors or from different colleges"
  },
  {
    "objectID": "slides/02-overview.html#section-3",
    "href": "slides/02-overview.html#section-3",
    "title": "Overview of Statistics and Data üìñ",
    "section": "",
    "text": "This diagram tell us everything.\nWe are going to learn each data type."
  },
  {
    "objectID": "slides/02-overview.html#categorical-vs.-numerical-variables",
    "href": "slides/02-overview.html#categorical-vs.-numerical-variables",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Categorical vs.¬†Numerical Variables",
    "text": "Categorical vs.¬†Numerical Variables\n\n\nA categorical variable provides non-numerical information which can be placed in one (and only one) category from two or more categories.\n\nGender (Male üë®, Female üë©, Trans üè≥Ô∏è‚Äçüåà) \nClass (Freshman, Sophomore, Junior, Senior, Graduate) \nCountry (USA üá∫üá∏, Canada üá®üá¶, UK üá¨üáß, Germany üá©üá™, Japan üáØüáµ, Korea üá∞üá∑) \n\n\n\nA numerical variable is recorded in a numerical value representing counts or measurements.\n\n GPA \n The number of relationships you‚Äôve had \n Height"
  },
  {
    "objectID": "slides/02-overview.html#numerical-variables-can-be-discrete-or-continuous",
    "href": "slides/02-overview.html#numerical-variables-can-be-discrete-or-continuous",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Numerical Variables can be Discrete or Continuous",
    "text": "Numerical Variables can be Discrete or Continuous\n\nA discrete variable takes on values of a finite or countable number.\n\nA continuous variable takes on values anywhere over a particular range without gaps or jumps.\n\n GPA is continuous because it can be any value between 0 and 4. \n The number of relationships you‚Äôve had is discrete because you can count the number and it is finite.\n Height is continuous because it can be any number within a range."
  },
  {
    "objectID": "slides/02-overview.html#categorical-variables-are-usually-recorded-as-numbers",
    "href": "slides/02-overview.html#categorical-variables-are-usually-recorded-as-numbers",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Categorical Variables are Usually Recorded as Numbers",
    "text": "Categorical Variables are Usually Recorded as Numbers\n\nGender (Male = 0, Female = 1, Trans = 2) \nClass (Freshman = 1, Sophomore = 2, Junior = 3, Senior = 4, Graduate = 5) \nCountry (USA = 100, Canada = 101, UK = 200, Germany = 201, Japan = 300, Korea = 301) \nUnited Airlines boarding groups\n\nThe numbers represent categories only; differences between them are meaningless.\n\nCanada - USA = 101 - 100 = 1?\nGraduate - Sophomore = 5 - 2 = 3 = Junior?\n\n\nWe need to learn the level of measurements to know whether or which arithmetic operations are meaningful."
  },
  {
    "objectID": "slides/02-overview.html#levels-of-measurements-nominal-and-ordinal-for-categorical-variables",
    "href": "slides/02-overview.html#levels-of-measurements-nominal-and-ordinal-for-categorical-variables",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Levels of Measurements: Nominal and Ordinal for Categorical Variables",
    "text": "Levels of Measurements: Nominal and Ordinal for Categorical Variables\n\n\nNominal: The data can NOT be ordered in a meaningful or natural way.\n\n\nGender (Male = 0, Female = 1, Trans = 2)  is nominal because Male, Female and Trans cannot be ordered.\n\nCountry (USA = 100, Canada = 101, UK = 200, Germany = 201, Japan = 300, Korea = 301)  is nominal.\n\n\n\n\n\n\nOrdinal: The data can be arranged in some meaningful order, but differences between data values can NOT be determined or are meaningless.\n\n\nClass (Freshman = 1, Sophomore = 2, Junior = 3, Senior = 4, Graduate = 5)  is ordinal because Sophomore is one class higher than Freshman."
  },
  {
    "objectID": "slides/02-overview.html#levels-of-measurements-interval-and-ratio-for-numerical-variables",
    "href": "slides/02-overview.html#levels-of-measurements-interval-and-ratio-for-numerical-variables",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Levels of Measurements: Interval and Ratio for Numerical Variables",
    "text": "Levels of Measurements: Interval and Ratio for Numerical Variables\n\n\nInterval: The data have meaningful difference between any two values. But the data do NOT have a natural zero or starting point. The data can do \\(\\color{red} +\\) and \\(\\color{red} -\\), but can‚Äôt reasonably do \\(\\color{red} \\times\\) and \\(\\color{red} \\div\\).\n\n\nTemperature is interval because \\(80^{\\circ}\\)F is 40 degrees higher than \\(40^{\\circ}\\)F \\((80-40=40)\\), but \\(0^{\\circ}\\) does not mean NO heat and \\(80^{\\circ}\\)F is NOT twice as hot as \\(40^{\\circ}\\)F.\n\n\n\n. . .\n\n\nRatio: The data have both meaningful differences and ratios, and there is a natural zero starting point that indicates none of the quantity. The data can do \\(\\color{red} +\\), \\(\\color{red} -\\), \\(\\color{red} \\times\\) and \\(\\color{red} \\div\\).\n\n\nDistance is ratio because \\(80\\) miles is twice as far as \\(40\\) miles \\((80/40 = 2)\\), and \\(0\\) mile means no distance."
  },
  {
    "objectID": "slides/02-overview.html#converting-numerical-to-categorical",
    "href": "slides/02-overview.html#converting-numerical-to-categorical",
    "title": "Overview of Statistics and Data üìñ",
    "section": "Converting Numerical to Categorical",
    "text": "Converting Numerical to Categorical\n\nYou‚Äôve already seen an example.\n\n. . .\n\n\n\n\nGrade\nPercentage\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[73, 77)\n\n\nC-\n[70, 73)\n\n\nD+\n[65, 70)\n\n\nD\n[60, 65)\n\n\nF\n[0, 60)"
  },
  {
    "objectID": "slides/02-overview.html#section-5",
    "href": "slides/02-overview.html#section-5",
    "title": "Overview of Statistics and Data üìñ",
    "section": "",
    "text": "Identify data type of each variable in the Marquette men‚Äôs basketball player data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nName\nClass\nPos\nHt\nWt\nHometown\nHigh_School\nPPG\nRPG\nAPG\n\n\n\n1\nKam Jones\nSr\nG\n6‚Äô5‚Äù\n205\nMemphis, TN\nEvangelical Christian School\n19.2\n4.5\n5.9\n\n\n2\nChase Ross\nJr\nG\n6‚Äô5‚Äù\n210\nDallas, TX\nCushing Academy\n10.5\n3.8\n2.1\n\n\n4\nStevie Mitchell\nSr\nG\n6‚Äô3‚Äù\n200\nReading, PA\nWilson HS\n10.7\n4.1\n1.6\n\n\n5\nTre Norman\nSo\nG\n6‚Äô4‚Äù\n210\nBoston, MA\nWorcester Academy\n1.9\n1.5\n0.5\n\n\n7\nZaide Lowery\nSo\nG\n6‚Äô5‚Äù\n200\nSpringfield, MO\nLa Lumiere School\n4.1\n3.0\n0.2\n\n\n8\nJoshua Clark\nFr\nF\n7‚Äô1‚Äù\n225\nVirginia Beach, VA\nClements HS\nNA\nNA\nNA\n\n\n10\nDamarius Owens\nFr\nF\n6‚Äô7‚Äù\n200\nRochester, NY\nWestern Reserve Academy\n2.6\n1.2\n0.5\n\n\n12\nBen Gold\nJr\nF\n6‚Äô11‚Äù\n235\nWellington, NZ\nNBA Global Academy\n7.4\n4.3\n0.9\n\n\n13\nRoyce Parham\nFr\nF\n6‚Äô8‚Äù\n230\nPittsburgh, PA\nWestern Reserve Academy\n5.1\n2.2\n0.4\n\n\n21\nAl Amadou\nSo\nF\n6‚Äô9‚Äù\n210\nPhiladelphia, PA\nChestnut Hill Academy\nNA\nNA\nNA\n\n\n22\nSean Jones\nJr (RS)\nG\n5‚Äô10‚Äù\n185\nColumbus, OH\nLincoln HS\nNA\nNA\nNA\n\n\n23\nDavid Joplin\nSr\nF\n6‚Äô8‚Äù\n225\nBrookfield, WI\nBrookfield Central HS\n14.2\n5.4\n1.3\n\n\n25\nJack Anderson\nSr\nG\n6‚Äô4‚Äù\n200\nDavie, FL\nWestern HS\n0.4\n0.4\n0.0\n\n\n35\nCaedin Hamilton\nR-Fr\nF\n6‚Äô9‚Äù\n250\nSanta Maria, CA\nSt.¬†Joseph HS\n1.5\n1.2\n0.6\n\n\n40\nCasey O‚ÄôMalley\nJr\nG\n6‚Äô3‚Äù\n200\nOmaha, NE\nCreighton Prep\n0.6\n0.2\n0.0\n\n\n41\nJonah Lucas\nJr\nG\n6‚Äô1‚Äù\n180\nWest Lafayette, IN\nHarrison HS\n0.0\n0.2\n0.0\n\n\n42\nLuke Jacobson\nFr\nF\n6‚Äô7‚Äù\n215\nSan Luis Obispo, CA\nMission Prep\nNA\nNA\nNA\n\n\n54\nJake Ciardo\nJr\nG\n6‚Äô2‚Äù\n185\nGermantown, WI\nGermantown HS\n0.4\n0.8\n0.0\n\n\n55\nCameron Brown\nSr\nG\n6‚Äô1‚Äù\n215\nPlano, TX\nJohn Paul II HS\n0.4\n0.2\n0.0"
  },
  {
    "objectID": "slides/04-programming-slides.html#r-is-a-calculator---arithmetic-operators",
    "href": "slides/04-programming-slides.html#r-is-a-calculator---arithmetic-operators",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "R is a Calculator - Arithmetic Operators",
    "text": "R is a Calculator - Arithmetic Operators\n\n\nWe are already equipped with the tools we need for doing statistics in this course. Now it‚Äôs time to program in R.\nWe are going to go through basic R syntax and its commonly used data structures.\nBecause some of you have no experience in R, this introduction is a must, because I want to make sure everyone is on the same page.\nIf you are already familiar with basic R syntax, please bear with me. You can learn more advanced stuff by yourself. OK.\nFirst, as many other languages, R is a calculator. We can do basic arithmetic operations using R.\nTo get the remainder of division, we use two percentage symbols.\nTo get the quotient of division, we use percent, slash and percent symbol."
  },
  {
    "objectID": "slides/04-programming-slides.html#r-is-a-calculator---examples",
    "href": "slides/04-programming-slides.html#r-is-a-calculator---examples",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "R is a Calculator - Examples",
    "text": "R is a Calculator - Examples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have to do the operation in the parenthesis first"
  },
  {
    "objectID": "slides/04-programming-slides.html#r-does-comparisons---logical-operators",
    "href": "slides/04-programming-slides.html#r-does-comparisons---logical-operators",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "R Does Comparisons - Logical Operators",
    "text": "R Does Comparisons - Logical Operators\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 &lt;= 5\n\n[1] TRUE\n\n5 &lt;= 4\n\n[1] FALSE\n\n# Is 5 NOT equal to 5? FALSE\n5 != 5\n\n[1] FALSE"
  },
  {
    "objectID": "slides/04-programming-slides.html#build-in-functions",
    "href": "slides/04-programming-slides.html#build-in-functions",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Build-in Functions",
    "text": "Build-in Functions\n\nR has lots of built-in functions, especially for mathematics, probability and statistics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsqrt(144)\n\n[1] 12\n\nexp(1)  ## Euler's number\n\n[1] 2.718282\n\nsin(pi/2)\n\n[1] 1\n\nabs(-7)\n\n[1] 7"
  },
  {
    "objectID": "slides/04-programming-slides.html#creating-variables",
    "href": "slides/04-programming-slides.html#creating-variables",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Creating Variables",
    "text": "Creating Variables\n\nA variable stores a value that can be changed according to our need.\nUse &lt;- operator to assign a value to the variable. (Highly recommendedüëç)\n\n\nx &lt;- 5  ## we create an object, value 5, and call it x, which is a variable.\nx  ## type the variable name to see the value stored in the object x\n\n[1] 5\n\n\n\n\n(x &lt;- x + 6)  # We can reassign any value to the variable we created\n\n[1] 11\n\nx == 5  # We can perform any operations on variables\n\n[1] FALSE\n\nlog(x) # Variables can also be used in any built-in functions\n\n[1] 2.397895"
  },
  {
    "objectID": "slides/04-programming-slides.html#types-of-variables",
    "href": "slides/04-programming-slides.html#types-of-variables",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Types of Variables",
    "text": "Types of Variables\n\nUse typeof() to check which type a variable belongs to.\nCommon types include character, double, integer and logical.\nCheck if it‚Äôs of a specific type: is.character(), is.double(), is.integer(), is.logical(). \n\n\n\n\ntypeof(5)\n\n[1] \"double\"\n\ntypeof(5L)\n\n[1] \"integer\"\n\ntypeof(\"I_love_stat\")\n\n[1] \"character\"\n\n\n\n\ntypeof(1 &gt; 3)\n\n[1] \"logical\"\n\nis.double(5L)\n\n[1] FALSE"
  },
  {
    "objectID": "slides/04-programming-slides.html#variable-types-in-r-and-in-statistics",
    "href": "slides/04-programming-slides.html#variable-types-in-r-and-in-statistics",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Variable Types in R and in Statistics",
    "text": "Variable Types in R and in Statistics\n\nType character and logical correspond to categorical variables.\nType logical is a special type of categorical variables that has only two categories (binary).\n\n\nType double and integer correspond to numerical variables. (an exception later)\n\nType double is for continuous variables\nType integer is for discrete variables."
  },
  {
    "objectID": "slides/04-programming-slides.html#section-3",
    "href": "slides/04-programming-slides.html#section-3",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "",
    "text": "Create a variable age that stores your age. Check what type it is.\nCreate a variable name that stores your name. Check its type.\nCreate a variable is_male that stores whether you are male (true/false). Check its type.\n\n\n\n\n\n\n\n\n\n\n\n  \n    ‚àí\n    +\n \n 02:00"
  },
  {
    "objectID": "slides/04-programming-slides.html#atomic-vector",
    "href": "slides/04-programming-slides.html#atomic-vector",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "(Atomic) Vector",
    "text": "(Atomic) Vector\n\nTo create a vector, use c(), short for concatenate or combine.\nAll elements of a vector must be of the same type.\n\n\n\n\n(dbl_vec &lt;- c(1, 2.5, 4.5)) \n\n[1] 1.0 2.5 4.5\n\n(chr_vec &lt;- c(\"pretty\", \"girl\"))\n\n[1] \"pretty\" \"girl\"  \n\n\n\n\n## check how many elements in a vector\nlength(dbl_vec) \n\n[1] 3\n\n## check a compact description of \n## any R data structure\nstr(dbl_vec) \n\n num [1:3] 1 2.5 4.5"
  },
  {
    "objectID": "slides/04-programming-slides.html#operations-on-vectors",
    "href": "slides/04-programming-slides.html#operations-on-vectors",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Operations on Vectors",
    "text": "Operations on Vectors\n\nWe can do any operations on vectors as we do on a scalar variable (vector of length 1).\n\n\n\n\n# Create two vectors\nv1 &lt;- c(3, 8)\nv2 &lt;- c(4, 100) \n\n## All operations happen element-wisely\n# Vector addition\nv1 + v2\n\n[1]   7 108\n\n# Vector subtraction\nv1 - v2\n\n[1]  -1 -92\n\n\n\n\n# Vector multiplication\nv1 * v2\n\n[1]  12 800\n\n# Vector division\nv1 / v2\n\n[1] 0.75 0.08\n\nsqrt(v2)\n\n[1]  2 10"
  },
  {
    "objectID": "slides/04-programming-slides.html#recycling-of-vectors",
    "href": "slides/04-programming-slides.html#recycling-of-vectors",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Recycling of Vectors",
    "text": "Recycling of Vectors\n\nIf we apply arithmetic operations to two vectors of unequal length, the elements of the shorter vector will be recycled to complete the operations.  \n\n\n\nv1 &lt;- c(3, 8, 4, 5)\n# The following 2 operations are the same\nv1 * 2\n\n[1]  6 16  8 10\n\nv1 * c(2, 2, 2, 2)\n\n[1]  6 16  8 10\n\nv3 &lt;- c(4, 11)\nv1 + v3  ## v3 becomes c(4, 11, 4, 11) when doing the operation\n\n[1]  7 19  8 16"
  },
  {
    "objectID": "slides/04-programming-slides.html#subsetting-vectors",
    "href": "slides/04-programming-slides.html#subsetting-vectors",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Subsetting Vectors",
    "text": "Subsetting Vectors\n\nTo extract element(s) in a vector, use a pair of brackets [] with element indexing.\nThe indexing starts with 1.\n\n\n\n\nv1\n\n[1] 3 8 4 5\n\nv2\n\n[1]   4 100\n\n## The 3rd element\nv1[3]  \n\n[1] 4\n\n\n\n\nv1[c(1, 3)]\n\n[1] 3 4\n\n## extract all except a few elements\n## put a negative sign before the vector of \n## indices\nv1[-c(2, 3)] \n\n[1] 3 5"
  },
  {
    "objectID": "slides/04-programming-slides.html#factor-1",
    "href": "slides/04-programming-slides.html#factor-1",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Factor",
    "text": "Factor\n\nA vector of type factor can be ordered in a meaningful way. Create a factor by factor().\n\n\n## Create a factor from a character vector using function factor()\n(fac &lt;- factor(c(\"med\", \"high\", \"low\")))\n\n[1] med  high low \nLevels: high low med\n\n\n\n\nIt is a type of integer, not character. üò≤ üôÑ\n\n\ntypeof(fac)  ## The type is integer.\n\n[1] \"integer\"\n\nstr(fac)  ## The integers show the level each element in vector fac belongs to.\n\n Factor w/ 3 levels \"high\",\"low\",\"med\": 3 1 2\n\n\n\n\n\norder_fac &lt;- factor(c(\"med\", \"high\", \"low\"), levels = c(\"low\", \"med\", \"high\"))\nstr(order_fac)\n\n Factor w/ 3 levels \"low\",\"med\",\"high\": 2 3 1\n\n\nlevels(fac) ## Each level represents an integer, ordered from the vector alphabetically."
  },
  {
    "objectID": "slides/04-programming-slides.html#matrix-1",
    "href": "slides/04-programming-slides.html#matrix-1",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Matrix",
    "text": "Matrix\n\nA matrix is a two-dimensional analog of a vector with attribute dim.\nUse command matrix() to create a matrix.\n\n\n## Create a 3 by 2 matrix called mat\n(mat &lt;- matrix(data = 1:6, nrow = 3, ncol = 2)) \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\ndim(mat); nrow(mat); ncol(mat)\n\n[1] 3 2\n\n\n[1] 3\n\n\n[1] 2\n\n\n\n# elements are arranged by row\nmatrix(data = 1:6, \n       nrow = 3, \n       ncol = 2, \n       byrow = TRUE) #&lt;&lt;\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\nattributes(mat)\n\n$dim\n[1] 3 2"
  },
  {
    "objectID": "slides/04-programming-slides.html#subsetting-a-matrix",
    "href": "slides/04-programming-slides.html#subsetting-a-matrix",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Subsetting a Matrix",
    "text": "Subsetting a Matrix\n\nUse the same indexing approach as vectors on rows and columns.\nUse comma , to separate row and column index.\n\nmat[2, 2] extracts the element of the second row and second column.\n\n\n\n\nmat\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n## all rows and 2nd column\n## leave row index blank\n## specify 2 in coln index\nmat[, 2]\n\n[1] 4 5 6\n\n\n\n\n## 2nd row and all columns\nmat[2, ] \n\n[1] 2 5\n\n## The 1st and 3rd rows and the 1st column\nmat[c(1, 3), 1] \n\n[1] 1 3"
  },
  {
    "objectID": "slides/04-programming-slides.html#data-frame-the-most-common-way-of-storing-datasets",
    "href": "slides/04-programming-slides.html#data-frame-the-most-common-way-of-storing-datasets",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Data Frame: The Most Common Way of Storing Datasets",
    "text": "Data Frame: The Most Common Way of Storing Datasets\n\nA data frame is of type list of equal-length vectors, having a 2-dimensional structure.\nMore general than matrix: Different columns can have different types.\nUse data.frame() that takes named vectors as input ‚Äúelement‚Äù.\n\n\n## data frame w/ an dbl column named age and char columns gen and col.\n(df &lt;- data.frame(age = c(19, 21, 40), gen = c(\"m\", \"f\", \"m\"), col = c(\"r\",\"b\",\"g\")))\n\n  age gen col\n1  19   m   r\n2  21   f   b\n3  40   m   g\n\nstr(df)  ## use $ to represent column elements \n\n'data.frame':   3 obs. of  3 variables:\n $ age: num  19 21 40\n $ gen: chr  \"m\" \"f\" \"m\"\n $ col: chr  \"r\" \"b\" \"g\"\n\n\n\n\nWhat happen if we create a data frame without column names?"
  },
  {
    "objectID": "slides/04-programming-slides.html#data-structure-comparison",
    "href": "slides/04-programming-slides.html#data-structure-comparison",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Data Structure Comparison",
    "text": "Data Structure Comparison\n\n\n\n\n\nhttps://environmentalcomputing.net/getting-started-with-r/data-types-structure/"
  },
  {
    "objectID": "slides/04-programming-slides.html#properties-of-data-frames",
    "href": "slides/04-programming-slides.html#properties-of-data-frames",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Properties of Data Frames",
    "text": "Properties of Data Frames\nData frame has properties of matrix.\n\n\n\ndf\n\n  age gen col\n1  19   m   r\n2  21   f   b\n3  40   m   g\n\ncolnames(df)  ## df as a matrix\n\n[1] \"age\" \"gen\" \"col\"\n\nncol(df) ## df as a matrix\n\n[1] 3\n\ndim(df) ## df as a matrix\n\n[1] 3 3"
  },
  {
    "objectID": "slides/04-programming-slides.html#subsetting-a-data-frame",
    "href": "slides/04-programming-slides.html#subsetting-a-data-frame",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Subsetting a Data Frame",
    "text": "Subsetting a Data Frame\n\n\n\n\ndf\n\n  age gen col\n1  19   m   r\n2  21   f   b\n3  40   m   g\n\n## Subset rows\ndf[c(1, 3), ]\n\n  age gen col\n1  19   m   r\n3  40   m   g\n\n## Subset columns\ndf[, c(\"age\", \"col\")]\n\n  age col\n1  19   r\n2  21   b\n3  40   g\n\n\n\n\n## select the row where age == 21\ndf[df$age == 21, ]\n\n  age gen col\n2  21   f   b\n\n\n\n\nstr(df[\"age\"])  ## a data frame with one column\n\n'data.frame':   3 obs. of  1 variable:\n $ age: num  19 21 40\n\nstr(df[, \"age\"])  ## becomes a vector by default\n\n num [1:3] 19 21 40\n\n## like a list\ndf$age\n\n[1] 19 21 40\n\ndf[c(\"age\", \"gen\")] \n\n  age gen\n1  19   m\n2  21   f\n3  40   m"
  },
  {
    "objectID": "slides/04-programming-slides.html#section-6",
    "href": "slides/04-programming-slides.html#section-6",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "",
    "text": "Create a vector object called x that has 5 elements 3, 6, 2, 9, 14.\nCompute the average of elements of x.\nSubset the mtcars data set by selecting variables mpg and disp.\nSelect the cars (rows) in mtcars that have 4 cylinders.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    ‚àí\n    +\n \n 05:00\n \n\nx &lt;- c(3, 6, 2, 9, 14)\nmean(x)\n\n[1] 6.8\n\nmtcars[, c(\"mpg\", \"disp\")]\n\n                     mpg  disp\nMazda RX4           21.0 160.0\nMazda RX4 Wag       21.0 160.0\nDatsun 710          22.8 108.0\nHornet 4 Drive      21.4 258.0\nHornet Sportabout   18.7 360.0\nValiant             18.1 225.0\nDuster 360          14.3 360.0\nMerc 240D           24.4 146.7\nMerc 230            22.8 140.8\nMerc 280            19.2 167.6\nMerc 280C           17.8 167.6\nMerc 450SE          16.4 275.8\nMerc 450SL          17.3 275.8\nMerc 450SLC         15.2 275.8\nCadillac Fleetwood  10.4 472.0\nLincoln Continental 10.4 460.0\nChrysler Imperial   14.7 440.0\nFiat 128            32.4  78.7\nHonda Civic         30.4  75.7\nToyota Corolla      33.9  71.1\nToyota Corona       21.5 120.1\nDodge Challenger    15.5 318.0\nAMC Javelin         15.2 304.0\nCamaro Z28          13.3 350.0\nPontiac Firebird    19.2 400.0\nFiat X1-9           27.3  79.0\nPorsche 914-2       26.0 120.3\nLotus Europa        30.4  95.1\nFord Pantera L      15.8 351.0\nFerrari Dino        19.7 145.0\nMaserati Bora       15.0 301.0\nVolvo 142E          21.4 121.0\n\nmtcars[mtcars$cyl == 4, ]\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2"
  },
  {
    "objectID": "slides/04-programming.html",
    "href": "slides/04-programming.html",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "",
    "text": "We are already equipped with the tools we need for doing statistics in this course. Now it‚Äôs time to program in R.\nWe are going to go through basic R syntax and its commonly used data structures.\nBecause some of you have no experience in R, this introduction is a must, because I want to make sure everyone is on the same page.\nIf you are already familiar with basic R syntax, please bear with me. You can learn more advanced stuff by yourself. OK.\nFirst, as many other languages, R is a calculator. We can do basic arithmetic operations using R.\nTo get the remainder of division, we use two percentage symbols.\nTo get the quotient of division, we use percent, slash and percent symbol.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have to do the operation in the parenthesis first\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 &lt;= 5\n\n[1] TRUE\n\n5 &lt;= 4\n\n[1] FALSE\n\n# Is 5 NOT equal to 5? FALSE\n5 != 5\n\n[1] FALSE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR has lots of built-in functions, especially for mathematics, probability and statistics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsqrt(144)\n\n[1] 12\n\nexp(1)  ## Euler's number\n\n[1] 2.718282\n\nsin(pi/2)\n\n[1] 1\n\nabs(-7)\n\n[1] 7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou‚Äôve seen comments a lot! How do we write a comment in R?\n\n. . .\n\nUse # to add a comment so that the text after # is not read as an R command.\nWriting (good) comments is highly recommended: help readers and more importantly yourself understand what the code is doing.\nComments should explain the why, not the what.\n\n\n\n\n\n\n\n\nhttps://www.reddit.com/r/ProgrammerHumor/comments/8w54mx/code_comments_be_like/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Everything that exists is an object.   Everything that happens is a function call. \n‚Äì John Chambers, the creator of the S programming language.\n\n\nWe have made lots of things happened!\nEven arithmetic and logical operators are functions!\n\n\n`+`(x = 2, y = 3)\n\n[1] 5\n\n`&`(TRUE, FALSE)\n\n[1] FALSE\n\n\n\n\nA variable stores a value that can be changed according to our need.\nUse &lt;- operator to assign a value to the variable. (Highly recommendedüëç)\n\n\nx &lt;- 5  ## we create an object, value 5, and call it x, which is a variable.\nx  ## type the variable name to see the value stored in the object x\n\n[1] 5\n\n\n\n\n(x &lt;- x + 6)  # We can reassign any value to the variable we created\n\n[1] 11\n\nx == 5  # We can perform any operations on variables\n\n[1] FALSE\n\nlog(x) # Variables can also be used in any built-in functions\n\n[1] 2.397895"
  },
  {
    "objectID": "slides/04-programming.html#r-is-a-calculator---arithmetic-operators",
    "href": "slides/04-programming.html#r-is-a-calculator---arithmetic-operators",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "",
    "text": "We are already equipped with the tools we need for doing statistics in this course. Now it‚Äôs time to program in R.\nWe are going to go through basic R syntax and its commonly used data structures.\nBecause some of you have no experience in R, this introduction is a must, because I want to make sure everyone is on the same page.\nIf you are already familiar with basic R syntax, please bear with me. You can learn more advanced stuff by yourself. OK.\nFirst, as many other languages, R is a calculator. We can do basic arithmetic operations using R.\nTo get the remainder of division, we use two percentage symbols.\nTo get the quotient of division, we use percent, slash and percent symbol."
  },
  {
    "objectID": "slides/04-programming.html#r-is-a-calculator---examples",
    "href": "slides/04-programming.html#r-is-a-calculator---examples",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "",
    "text": "We have to do the operation in the parenthesis first"
  },
  {
    "objectID": "slides/04-programming.html#r-does-comparisons---logical-operators",
    "href": "slides/04-programming.html#r-does-comparisons---logical-operators",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "",
    "text": "5 &lt;= 5\n\n[1] TRUE\n\n5 &lt;= 4\n\n[1] FALSE\n\n# Is 5 NOT equal to 5? FALSE\n5 != 5\n\n[1] FALSE"
  },
  {
    "objectID": "slides/04-programming.html#build-in-functions",
    "href": "slides/04-programming.html#build-in-functions",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "",
    "text": "R has lots of built-in functions, especially for mathematics, probability and statistics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsqrt(144)\n\n[1] 12\n\nexp(1)  ## Euler's number\n\n[1] 2.718282\n\nsin(pi/2)\n\n[1] 1\n\nabs(-7)\n\n[1] 7"
  },
  {
    "objectID": "slides/04-programming.html#commenting",
    "href": "slides/04-programming.html#commenting",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "",
    "text": "You‚Äôve seen comments a lot! How do we write a comment in R?\n\n. . .\n\nUse # to add a comment so that the text after # is not read as an R command.\nWriting (good) comments is highly recommended: help readers and more importantly yourself understand what the code is doing.\nComments should explain the why, not the what."
  },
  {
    "objectID": "slides/04-programming.html#section-1",
    "href": "slides/04-programming.html#section-1",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "",
    "text": "https://www.reddit.com/r/ProgrammerHumor/comments/8w54mx/code_comments_be_like/"
  },
  {
    "objectID": "slides/04-programming.html#objects-and-funtions-in-r",
    "href": "slides/04-programming.html#objects-and-funtions-in-r",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "",
    "text": "Everything that exists is an object.   Everything that happens is a function call. \n‚Äì John Chambers, the creator of the S programming language.\n\n\nWe have made lots of things happened!\nEven arithmetic and logical operators are functions!\n\n\n`+`(x = 2, y = 3)\n\n[1] 5\n\n`&`(TRUE, FALSE)\n\n[1] FALSE"
  },
  {
    "objectID": "slides/04-programming.html#creating-variables",
    "href": "slides/04-programming.html#creating-variables",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "",
    "text": "A variable stores a value that can be changed according to our need.\nUse &lt;- operator to assign a value to the variable. (Highly recommendedüëç)\n\n\nx &lt;- 5  ## we create an object, value 5, and call it x, which is a variable.\nx  ## type the variable name to see the value stored in the object x\n\n[1] 5\n\n\n\n\n(x &lt;- x + 6)  # We can reassign any value to the variable we created\n\n[1] 11\n\nx == 5  # We can perform any operations on variables\n\n[1] FALSE\n\nlog(x) # Variables can also be used in any built-in functions\n\n[1] 2.397895"
  },
  {
    "objectID": "slides/04-programming.html#bad-naming",
    "href": "slides/04-programming.html#bad-naming",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Bad Naming",
    "text": "Bad Naming\n\n‚ùå Unless you have a very good reason, don‚Äôt create a variable whose name is the same as any R built-in constant or function!\nüòü It causes lots of confusion when your code is long and when others read your code.\n\n\n## THIS IS BAD CODING! DON'T DO THIS!\npi  ## pi is a built-in constant\n\n[1] 3.141593\n\n(pi &lt;- 20)\n\n[1] 20\n\nabs ## abs is a built-in function\n\nfunction (x)  .Primitive(\"abs\")\n\n(abs &lt;- abs(pi))\n\n[1] 20"
  },
  {
    "objectID": "slides/04-programming.html#types-of-variables",
    "href": "slides/04-programming.html#types-of-variables",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Types of Variables",
    "text": "Types of Variables\n\nUse typeof() to check which type a variable belongs to.\nCommon types include character, double, integer and logical.\nCheck if it‚Äôs of a specific type: is.character(), is.double(), is.integer(), is.logical(). \n\n\n\n\ntypeof(5)\n\n[1] \"double\"\n\ntypeof(5L)\n\n[1] \"integer\"\n\ntypeof(\"I_love_stat\")\n\n[1] \"character\"\n\n\n\n\ntypeof(1 &gt; 3)\n\n[1] \"logical\"\n\nis.double(5L)\n\n[1] FALSE"
  },
  {
    "objectID": "slides/04-programming.html#variable-types-in-r-and-in-statistics",
    "href": "slides/04-programming.html#variable-types-in-r-and-in-statistics",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Variable Types in R and in Statistics",
    "text": "Variable Types in R and in Statistics\n\nType character and logical correspond to categorical variables.\nType logical is a special type of categorical variables that has only two categories (binary).\n\n\nType double and integer correspond to numerical variables. (an exception later)\n\nType double is for continuous variables\nType integer is for discrete variables."
  },
  {
    "objectID": "slides/04-programming.html#section-3",
    "href": "slides/04-programming.html#section-3",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "",
    "text": "Create a variable age that stores your age. Check what type it is.\nCreate a variable name that stores your name. Check its type.\nCreate a variable is_male that stores whether you are male (true/false). Check its type."
  },
  {
    "objectID": "slides/04-programming.html#atomic-vector",
    "href": "slides/04-programming.html#atomic-vector",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "(Atomic) Vector",
    "text": "(Atomic) Vector\n\nTo create a vector, use c(), short for concatenate or combine.\nAll elements of a vector must be of the same type.\n\n\n\n\n(dbl_vec &lt;- c(1, 2.5, 4.5)) \n\n[1] 1.0 2.5 4.5\n\n(chr_vec &lt;- c(\"pretty\", \"girl\"))\n\n[1] \"pretty\" \"girl\"  \n\n\n\n\n## check how many elements in a vector\nlength(dbl_vec) \n\n[1] 3\n\n## check a compact description of \n## any R data structure\nstr(dbl_vec) \n\n num [1:3] 1 2.5 4.5"
  },
  {
    "objectID": "slides/04-programming.html#operations-on-vectors",
    "href": "slides/04-programming.html#operations-on-vectors",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Operations on Vectors",
    "text": "Operations on Vectors\n\nWe can do any operations on vectors as we do on a scalar variable (vector of length 1).\n\n\n\n\n# Create two vectors\nv1 &lt;- c(3, 8)\nv2 &lt;- c(4, 100) \n\n## All operations happen element-wisely\n# Vector addition\nv1 + v2\n\n[1]   7 108\n\n# Vector subtraction\nv1 - v2\n\n[1]  -1 -92\n\n\n\n\n# Vector multiplication\nv1 * v2\n\n[1]  12 800\n\n# Vector division\nv1 / v2\n\n[1] 0.75 0.08\n\nsqrt(v2)\n\n[1]  2 10"
  },
  {
    "objectID": "slides/04-programming.html#recycling-of-vectors",
    "href": "slides/04-programming.html#recycling-of-vectors",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Recycling of Vectors",
    "text": "Recycling of Vectors\n\nIf we apply arithmetic operations to two vectors of unequal length, the elements of the shorter vector will be recycled to complete the operations.  \n\n\n\nv1 &lt;- c(3, 8, 4, 5)\n# The following 2 operations are the same\nv1 * 2\n\n[1]  6 16  8 10\n\nv1 * c(2, 2, 2, 2)\n\n[1]  6 16  8 10\n\nv3 &lt;- c(4, 11)\nv1 + v3  ## v3 becomes c(4, 11, 4, 11) when doing the operation\n\n[1]  7 19  8 16"
  },
  {
    "objectID": "slides/04-programming.html#subsetting-vectors",
    "href": "slides/04-programming.html#subsetting-vectors",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Subsetting Vectors",
    "text": "Subsetting Vectors\n\nTo extract element(s) in a vector, use a pair of brackets [] with element indexing.\nThe indexing starts with 1.\n\n\n\n\nv1\n\n[1] 3 8 4 5\n\nv2\n\n[1]   4 100\n\n## The 3rd element\nv1[3]  \n\n[1] 4\n\n\n\n\nv1[c(1, 3)]\n\n[1] 3 4\n\n## extract all except a few elements\n## put a negative sign before the vector of \n## indices\nv1[-c(2, 3)] \n\n[1] 3 5"
  },
  {
    "objectID": "slides/04-programming.html#factor-1",
    "href": "slides/04-programming.html#factor-1",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Factor",
    "text": "Factor\n\nA vector of type factor can be ordered in a meaningful way. Create a factor by factor().\n\n\n## Create a factor from a character vector using function factor()\n(fac &lt;- factor(c(\"med\", \"high\", \"low\")))\n\n[1] med  high low \nLevels: high low med\n\n\n. . .\n\nIt is a type of integer, not character. üò≤ üôÑ\n\n\ntypeof(fac)  ## The type is integer.\n\n[1] \"integer\"\n\nstr(fac)  ## The integers show the level each element in vector fac belongs to.\n\n Factor w/ 3 levels \"high\",\"low\",\"med\": 3 1 2\n\n\n. . .\n\norder_fac &lt;- factor(c(\"med\", \"high\", \"low\"), levels = c(\"low\", \"med\", \"high\"))\nstr(order_fac)\n\n Factor w/ 3 levels \"low\",\"med\",\"high\": 2 3 1\n\n\n\nlevels(fac) ## Each level represents an integer, ordered from the vector alphabetically."
  },
  {
    "objectID": "slides/04-programming.html#list-generic-vectors",
    "href": "slides/04-programming.html#list-generic-vectors",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "List (Generic Vectors)",
    "text": "List (Generic Vectors)\n\nLists are different from (atomic) vectors: Elements can be of any type, including lists.\nConstruct a list by using list().\n\n\n\n\n## a list of 3 elements of different types\nx_lst &lt;- list(idx = 1:3, \n              \"a\", \n              c(TRUE, FALSE))\n\n\n\n$idx\n[1] 1 2 3\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1]  TRUE FALSE\n\n\n\n\nstr(x_lst)\n\nList of 3\n $ idx: int [1:3] 1 2 3\n $    : chr \"a\"\n $    : logi [1:2] TRUE FALSE\n\nnames(x_lst)\n\n[1] \"idx\" \"\"    \"\"   \n\nlength(x_lst)\n\n[1] 3"
  },
  {
    "objectID": "slides/04-programming.html#subsetting-a-list",
    "href": "slides/04-programming.html#subsetting-a-list",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Subsetting a List",
    "text": "Subsetting a List\n\n\nReturn an  element  of a list\n\n## subset by name (a vector)\nx_lst$idx  \n\n[1] 1 2 3\n\n## subset by indexing (a vector)\nx_lst[[1]]  \n\n[1] 1 2 3\n\ntypeof(x_lst$idx)\n\n[1] \"integer\"\n\n\n\n\nReturn a  sub-list  of a list\n\n## subset by name (still a list)\nx_lst[\"idx\"]  \n\n$idx\n[1] 1 2 3\n\n## subset by indexing (still a list)\nx_lst[1]  \n\n$idx\n[1] 1 2 3\n\ntypeof(x_lst[\"idx\"])\n\n[1] \"list\"\n\n\n\n\n\n\n\nThis is where we should pay more attention to. When we subset a list, it may return an element of the list, or it returns a sub-list of the list.\nLet‚Äôs see how it happens.\nThis is our x_lst. We can subset a list by name or by indexing.\nSuppose we want the first element of the list, we can get it by its name using x_lst$idx.\nWe can also obtain it by using indexing like x_lst[[1]] because we want the first element.\nNotice that the way we subset a list returns an integer vector, the real first element of the list, not a list.\nLet‚Äôs see another case on the right.\nWe can also subset by name using single pair of brackets, and put the name inside the brackets.\nOr we can subset by indexing, again using single pair of brackets.\nAnd you see what happened? The way we subset a list here returns a sub-list, not the element itself.\nSo please be careful when subsetting a list.\nIf you want a vector, use these ways. If you want to keep it as a list, use these ways."
  },
  {
    "objectID": "slides/04-programming.html#section-4",
    "href": "slides/04-programming.html#section-4",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "",
    "text": "pepper packet pepper shaker"
  },
  {
    "objectID": "slides/04-programming.html#section-5",
    "href": "slides/04-programming.html#section-5",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "",
    "text": "If list x is a train carrying objects, then x[[5]] is the object in car 5; x[4:6] is a train of cars 4-6.\n‚Äî @RLangTip, https://twitter.com/RLangTip/status/268375867468681216"
  },
  {
    "objectID": "slides/04-programming.html#matrix-1",
    "href": "slides/04-programming.html#matrix-1",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Matrix",
    "text": "Matrix\n\nA matrix is a two-dimensional analog of a vector with attribute dim.\nUse command matrix() to create a matrix.\n\n\n## Create a 3 by 2 matrix called mat\n(mat &lt;- matrix(data = 1:6, nrow = 3, ncol = 2)) \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\ndim(mat); nrow(mat); ncol(mat)\n\n[1] 3 2\n\n\n[1] 3\n\n\n[1] 2\n\n\n\n\n# elements are arranged by row\nmatrix(data = 1:6, \n       nrow = 3, \n       ncol = 2, \n       byrow = TRUE) #&lt;&lt;\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\nattributes(mat)\n\n$dim\n[1] 3 2"
  },
  {
    "objectID": "slides/04-programming.html#subsetting-a-matrix",
    "href": "slides/04-programming.html#subsetting-a-matrix",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Subsetting a Matrix",
    "text": "Subsetting a Matrix\n\nUse the same indexing approach as vectors on rows and columns.\nUse comma , to separate row and column index.\n\nmat[2, 2] extracts the element of the second row and second column.\n\n\n\n\nmat\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n## all rows and 2nd column\n## leave row index blank\n## specify 2 in coln index\nmat[, 2]\n\n[1] 4 5 6\n\n\n\n\n## 2nd row and all columns\nmat[2, ] \n\n[1] 2 5\n\n## The 1st and 3rd rows and the 1st column\nmat[c(1, 3), 1] \n\n[1] 1 3"
  },
  {
    "objectID": "slides/04-programming.html#data-frame-the-most-common-way-of-storing-datasets",
    "href": "slides/04-programming.html#data-frame-the-most-common-way-of-storing-datasets",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Data Frame: The Most Common Way of Storing Datasets",
    "text": "Data Frame: The Most Common Way of Storing Datasets\n\nA data frame is of type list of equal-length vectors, having a 2-dimensional structure.\nMore general than matrix: Different columns can have different types.\nUse data.frame() that takes named vectors as input ‚Äúelement‚Äù.\n\n\n## data frame w/ an dbl column named age and char columns gen and col.\n(df &lt;- data.frame(age = c(19, 21, 40), gen = c(\"m\", \"f\", \"m\"), col = c(\"r\",\"b\",\"g\")))\n\n  age gen col\n1  19   m   r\n2  21   f   b\n3  40   m   g\n\nstr(df)  ## use $ to represent column elements \n\n'data.frame':   3 obs. of  3 variables:\n $ age: num  19 21 40\n $ gen: chr  \"m\" \"f\" \"m\"\n $ col: chr  \"r\" \"b\" \"g\"\n\n\n. . .\n\nWhat happen if we create a data frame without column names?"
  },
  {
    "objectID": "slides/04-programming.html#data-structure-comparison",
    "href": "slides/04-programming.html#data-structure-comparison",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Data Structure Comparison",
    "text": "Data Structure Comparison\n\n\n\n\n\nhttps://environmentalcomputing.net/getting-started-with-r/data-types-structure/"
  },
  {
    "objectID": "slides/04-programming.html#properties-of-data-frames",
    "href": "slides/04-programming.html#properties-of-data-frames",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Properties of Data Frames",
    "text": "Properties of Data Frames\nData frame has properties of matrix.\n\n\n\ndf\n\n  age gen col\n1  19   m   r\n2  21   f   b\n3  40   m   g\n\ncolnames(df)  ## df as a matrix\n\n[1] \"age\" \"gen\" \"col\"\n\nncol(df) ## df as a matrix\n\n[1] 3\n\ndim(df) ## df as a matrix\n\n[1] 3 3"
  },
  {
    "objectID": "slides/04-programming.html#subsetting-a-data-frame",
    "href": "slides/04-programming.html#subsetting-a-data-frame",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "Subsetting a Data Frame",
    "text": "Subsetting a Data Frame\n\n\n\n\ndf\n\n  age gen col\n1  19   m   r\n2  21   f   b\n3  40   m   g\n\n## Subset rows\ndf[c(1, 3), ]\n\n  age gen col\n1  19   m   r\n3  40   m   g\n\n## Subset columns\ndf[, c(\"age\", \"col\")]\n\n  age col\n1  19   r\n2  21   b\n3  40   g\n\n\n\n\n## select the row where age == 21\ndf[df$age == 21, ]\n\n  age gen col\n2  21   f   b\n\n\n\n\n\n\nstr(df[\"age\"])  ## a data frame with one column\n\n'data.frame':   3 obs. of  1 variable:\n $ age: num  19 21 40\n\nstr(df[, \"age\"])  ## becomes a vector by default\n\n num [1:3] 19 21 40\n\n## like a list\ndf$age\n\n[1] 19 21 40\n\ndf[c(\"age\", \"gen\")] \n\n  age gen\n1  19   m\n2  21   f\n3  40   m"
  },
  {
    "objectID": "slides/04-programming.html#section-6",
    "href": "slides/04-programming.html#section-6",
    "title": "You RRR a Beginner: Data Types and Structures üë®‚Äçüíª",
    "section": "",
    "text": "Create a vector object called x that has 5 elements 3, 6, 2, 9, 14.\nCompute the average of elements of x.\nSubset the mtcars data set by selecting variables mpg and disp.\nSelect the cars (rows) in mtcars that have 4 cylinders.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- c(3, 6, 2, 9, 14)\nmean(x)\n\n[1] 6.8\n\nmtcars[, c(\"mpg\", \"disp\")]\n\n                     mpg  disp\nMazda RX4           21.0 160.0\nMazda RX4 Wag       21.0 160.0\nDatsun 710          22.8 108.0\nHornet 4 Drive      21.4 258.0\nHornet Sportabout   18.7 360.0\nValiant             18.1 225.0\nDuster 360          14.3 360.0\nMerc 240D           24.4 146.7\nMerc 230            22.8 140.8\nMerc 280            19.2 167.6\nMerc 280C           17.8 167.6\nMerc 450SE          16.4 275.8\nMerc 450SL          17.3 275.8\nMerc 450SLC         15.2 275.8\nCadillac Fleetwood  10.4 472.0\nLincoln Continental 10.4 460.0\nChrysler Imperial   14.7 440.0\nFiat 128            32.4  78.7\nHonda Civic         30.4  75.7\nToyota Corolla      33.9  71.1\nToyota Corona       21.5 120.1\nDodge Challenger    15.5 318.0\nAMC Javelin         15.2 304.0\nCamaro Z28          13.3 350.0\nPontiac Firebird    19.2 400.0\nFiat X1-9           27.3  79.0\nPorsche 914-2       26.0 120.3\nLotus Europa        30.4  95.1\nFord Pantera L      15.8 351.0\nFerrari Dino        19.7 145.0\nMaserati Bora       15.0 301.0\nVolvo 142E          21.4 121.0\n\nmtcars[mtcars$cyl == 4, ]\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2"
  },
  {
    "objectID": "slides/07-distribution-slides.html#random-variables-1",
    "href": "slides/07-distribution-slides.html#random-variables-1",
    "title": "Probability Distributions \n",
    "section": "Random Variables",
    "text": "Random Variables\n\nRecap: A variable in a data set is a characteristic that varies from one to another.\n\nA variable can be either categorical or numerical.\nNumerical variables can be either discrete or continuous.\n\n\n\n\n\nA random variable, usually written as \\(X\\) 1, is a variable whose possible values are numerical outcomes determined by chance or randomness of a procedure or experiment.\n\n Toss a coin 2 times. \\(X\\) = # of heads. \n \\(X\\) = # of accidents in W. Wisconsin Ave. per day.\n\n\n\nA random variable has a probability distribution associated with it, accounting for its randomness.\n\n\nSimilar to relative frequency table (distribution)\nA probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment in terms of values of r.v. X. Yes. The possible values of \\(X\\) are numerical \\(X = 0, 1, 2\\) that are determined by randomness of the tossing-coin experiment.\n\n\nUsually in statistics, a capital \\(X\\) represents a random variable and a small \\(x\\) represents a realized value of \\(X\\)."
  },
  {
    "objectID": "slides/07-distribution-slides.html#discrete-and-continuous-random-variables",
    "href": "slides/07-distribution-slides.html#discrete-and-continuous-random-variables",
    "title": "Probability Distributions \n",
    "section": "Discrete and Continuous Random Variables",
    "text": "Discrete and Continuous Random Variables\n\nA discrete random variable takes on a finite or countable number of values.\nA continuous random variable has infinitely many values, and the collection of values is uncountable.\n\n\n\n\n The number of relationships you‚Äôve ever had is discrete variable because we can count the number and it is finite.\n\nIf we can further determine the probability that the number is 0, 1, 2, or any possible number, it is a discrete random variable.\n\n\n\n Height is continuous because it can be any number within a range. \n\nIf we have a way to quantify the probability that the height is from any value \\(a\\) to any value \\(b\\), it is a continuous random variable."
  },
  {
    "objectID": "slides/07-distribution-slides.html#a-statistician-should-know",
    "href": "slides/07-distribution-slides.html#a-statistician-should-know",
    "title": "Probability Distributions \n",
    "section": "A Statistician Should Know",
    "text": "A Statistician Should Know\n\n\n\n\n\n\n\n\n\n\n\nThere are lots of well-known probability distributions out there.\nThe distributions shown here are just a small part of probability distributions\nIn fact there are infinitely many probability distributions.\nA function that satisfies probability distribution conditions is a probability distribution.\nYou can construct our own probability distribution."
  },
  {
    "objectID": "slides/07-distribution-slides.html#discrete-probability-distribution",
    "href": "slides/07-distribution-slides.html#discrete-probability-distribution",
    "title": "Probability Distributions \n",
    "section": "Discrete Probability Distribution",
    "text": "Discrete Probability Distribution\n\nThe probability (mass) function (pf, pmf) of a discrete random variable (r.v.) \\(X\\) is a function \\(P(X = x)\\) (or \\(p(x)\\)) that assigns a probability for every possible number \\(x\\).\nThe probability distribution for a discrete r.v. \\(X\\) displays its probability function.\nThe display can be a table, graph, or mathematical formula of \\(P(X = x)\\).\n\n\n Example: ü™ôü™ô Toss a fair coin twice independently and \\(X\\) is the number of heads. \n\nThe probability distribution of \\(X\\) as a table is\n\n\n\n\n\nx\n0\n1\n2\n\n\nP(X = x)\n0.25\n0.5\n0.25\n\n\n\n\n\n\n\nüëâ \\(\\{X = x\\}\\) is an event corresponding to an event of some experiment.\n\n\n\n\nWhat is the event that \\(\\{X = 0\\}\\) corresponds to?\n\n\n\n\nHow do we get \\(P(X = 0)\\), \\(P(X=1)\\) and \\(P(X=2)\\) ?"
  },
  {
    "objectID": "slides/07-distribution-slides.html#discrete-probability-distribution-as-a-graph",
    "href": "slides/07-distribution-slides.html#discrete-probability-distribution-as-a-graph",
    "title": "Probability Distributions \n",
    "section": "Discrete Probability Distribution as a Graph",
    "text": "Discrete Probability Distribution as a Graph\n\n\n\n\\(0 \\le P(X = x) \\le 1\\) for every value \\(x\\) of \\(X\\).\n\n \\(x = 0, 1, 2\\) \n\n\n\n\n\n\n\\(\\sum_{x}P(X=x) = 1\\), where \\(x\\) assumes all possible values.\n\n \\(P(X=0) + P(X = 1) + P(X = 2) = 1\\) \n\n\n\n\n\n\nThe probabilities for a discrete r.v. are additive because \\(\\{X = a\\}\\) and \\(\\{X = b\\}\\) are disjoint for any possible values \\(a \\ne b\\).\n\n \\(P(X = 1 \\text{ or } 2) = P(\\{X = 1\\} \\cup \\{X = 2\\}) = P(X = 1) + P(X = 2)\\)."
  },
  {
    "objectID": "slides/07-distribution-slides.html#mean-of-a-discrete-random-variable",
    "href": "slides/07-distribution-slides.html#mean-of-a-discrete-random-variable",
    "title": "Probability Distributions \n",
    "section": "Mean of a Discrete Random Variable",
    "text": "Mean of a Discrete Random Variable\n\nSuppose \\(X\\) takes values \\(x_1, \\dots, x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\).\nThe mean or expected value of \\(X\\) is the sum of each outcome multiplied by its corresponding probability: \\[E(X) := x_1 \\times P(X = x_1) + \\dots + x_k \\times P(X = x_k) = \\sum_{i=1}^kx_iP(X=x_i)\\]\nThe Greek letter \\(\\mu\\) may be used in place of the notation \\(E(X)\\).\n\n\nGive an example\n\n\nüëâ The mean of a discrete random variable \\(X\\) is the weighted average of possible values \\(x\\) weighted by their corresponding probability.\n\n\n\n\nWhat is the mean of \\(X\\) (the number of heads) in the previous example?\n\n\nOn average, we will see one heads comes up when we toss a fair coin twice."
  },
  {
    "objectID": "slides/07-distribution-slides.html#variance-of-a-discrete-random-variable",
    "href": "slides/07-distribution-slides.html#variance-of-a-discrete-random-variable",
    "title": "Probability Distributions \n",
    "section": "Variance of a Discrete Random Variable",
    "text": "Variance of a Discrete Random Variable\n\nSuppose \\(X\\) takes values \\(x_1, \\dots , x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\) and expected value \\(\\mu = E(X)\\).\nThe variance of \\(X\\), denoted by \\(\\mathrm{Var}(X)\\) or \\(\\sigma^2\\), is \\[\\small \\mathrm{Var}(X) := (x_1 - \\mu)^2 \\times P(X = x_1) + \\dots + (x_k - \\mu)^2 \\times P(X = x_k) = \\sum_{i=1}^k(x_i - \\mu)^2P(X=x_i)\\]\nThe standard deviation of \\(X\\), \\(\\sigma\\), is the square root of the variance.\n\n\n\nüëâ The variance of a discrete random variable \\(X\\) is the weighted sum of squared deviation from the mean weighted by probability values.\n\n\n\n\nWhat is the variance of \\(X\\) (the number of heads) in the previous example?"
  },
  {
    "objectID": "slides/07-distribution-slides.html#binomial-experiment-and-random-variable",
    "href": "slides/07-distribution-slides.html#binomial-experiment-and-random-variable",
    "title": "Probability Distributions \n",
    "section": "Binomial Experiment and Random Variable",
    "text": "Binomial Experiment and Random Variable\n\nA binomial experiment is the one having the following properties:\n\nüëâ The experiment consists of a fixed number of identical trials \\(n\\).\nüëâ Each trial results in one of exactly two outcomes (success (S) and failure (F)).\nüëâ Trials are independent, meaning that the outcome of any trial does not affect the outcome of any other trial.\nüëâ The probability of success is constant for all trials.\n\n\nIf \\(X\\) is defined as  the number of successes observed in \\(n\\) trials , \\(X\\) is a binomial random variable.\n\n\n\n\nThe word success just means one of the two outcomes, and does not necessarily mean something good. \n\nüò≤ Can define Drug abuse as success and No drug abuse as failure.\n\n\nif smoking and non-smoking are the only two outcomes of some binomial experiment."
  },
  {
    "objectID": "slides/07-distribution-slides.html#binomial-distribution-2",
    "href": "slides/07-distribution-slides.html#binomial-distribution-2",
    "title": "Probability Distributions \n",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\nThe probability function \\(P(X = x)\\) of a binomial r.v. \\(X\\) can be fully determined by\n\nthe number of trials \\(n\\)\n\nprobability of success \\(\\pi\\)\n\n\n\nDifferent \\((n, \\pi)\\) pairs generate different binomial probability distributions.\n\\(X\\) is said to follow a binomial distribution with parameters \\(n\\) and \\(\\pi\\), written as \\(\\color{blue}{X \\sim binomial(n, \\pi)}\\).\nThe binomial probability function is \\[ \\color{blue}{P(X = x \\mid n, \\pi) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x}, \\quad x = 0, 1, 2, \\dots, n}\\] with mean \\(\\mu = E(X) = n\\pi\\) and variance \\(\\sigma^2 = \\mathrm{Var}(X) = n\\pi(1-\\pi)\\).\n\n\nOnce \\((n, \\pi)\\) is fixed, we know exactly what the distribution looks like, and we can form a table, graph, and provide a mathematical formula of the binomial distribution.\n\n\nTossing a fair coin two times independently. Let \\(X =\\) # of heads. Is \\(X\\) a binomial r.v.?\n\n\nExample: Toss a fair coin two times independently.\n\n\n\\(X =\\) # of heads\n\\(X \\sim binomial(n=2, \\pi=1/2)\\)"
  },
  {
    "objectID": "slides/07-distribution-slides.html#binomial-distribution-example",
    "href": "slides/07-distribution-slides.html#binomial-distribution-example",
    "title": "Probability Distributions \n",
    "section": "Binomial Distribution Example",
    "text": "Binomial Distribution Example\n\n\nAssume that 20% of all drivers have a blood alcohol level above the legal limit. For a random sample of 15 vehicles, compute the probability that:\n\nExactly 6 of the 15 drivers will exceed the legal limit.\nOf the 15 drivers, 6 or more will exceed the legal limit.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose it is a binomial experiment with \\(n = 15\\) and \\(\\pi = 0.2\\).\nLet \\(X\\) be the number of drivers exceeding limit.\n\\(X \\sim binomial(15, 0.2)\\).\n\n\\[ \\color{blue}{P(X = x \\mid n=15, \\pi=0.2) = \\frac{15!}{x!(15-x)!}(0.2)^x(1-0.2)^{15-x}, \\quad x = 0, 1, 2, \\dots, 15}\\]\n\nOn a particular roadway, assume that 20% of all drivers have a blood alcohol level above the legal limit.\n\nAll 15 drivers will have a blood alcohol level within the legal limit."
  },
  {
    "objectID": "slides/07-distribution-slides.html#binomial-distribution-example-x-sim-binomial15-0.2",
    "href": "slides/07-distribution-slides.html#binomial-distribution-example-x-sim-binomial15-0.2",
    "title": "Probability Distributions \n",
    "section": "Binomial Distribution Example \\(X \\sim binomial(15, 0.2)\\)\n",
    "text": "Binomial Distribution Example \\(X \\sim binomial(15, 0.2)\\)"
  },
  {
    "objectID": "slides/07-distribution-slides.html#binomial-distribution-example-1",
    "href": "slides/07-distribution-slides.html#binomial-distribution-example-1",
    "title": "Probability Distributions \n",
    "section": "Binomial Distribution Example",
    "text": "Binomial Distribution Example\n\n\nAssume that 20% of all drivers have a blood alcohol level above the legal limit. For a random sample of 15 vehicles, compute the probability that:\n\nExactly 6 of the 15 drivers will exceed the legal limit.\nOf the 15 drivers, 6 or more will exceed the legal limit.\n\n\n\n\n\n\n\n\n\n\n\n\nOn a particular roadway, assume that 20% of all drivers have a blood alcohol level above the legal limit.\nSuppose it is a binomial experiment with \\(n = 15\\) and \\(\\pi = 0.2\\).\nLet \\(X\\) be the number of drivers exceeding limit. Then \\(X \\sim binomial(15, 0.2)\\).\n\n\n\\(\\small P(X = 6) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x} = \\frac{15!}{6!(15-6)!}(0.2)^6(1-0.2)^{15-6} = 0.043\\)\n\n\n\n\n\\(\\small P(X \\ge 6) = p(6) + \\dots + p(15) = 1 - P(X \\le 5) = 1 - (p(0) + p(1) + \\dots + p(5)) = 0.0611\\)\n\n\nAll 15 drivers will have a blood alcohol level within the legal limit.\n\\(\\small P(X = 0) = \\frac{15!}{0!(15-0)!}(0.2)^0(1-0.2)^{15-0} = 0.0352\\)\n\n\n\n\\(P(X \\ge 6) = 1 - P(X \\le 5)\\) very useful trick\nCalculation is super tedious, even you use a calculator!\n\n\n\n\nNever do this by hand. We compute them using R!"
  },
  {
    "objectID": "slides/07-distribution-slides.html#binomial-example-computation-in-r",
    "href": "slides/07-distribution-slides.html#binomial-example-computation-in-r",
    "title": "Probability Distributions \n",
    "section": "Binomial Example Computation in R",
    "text": "Binomial Example Computation in R\n\n\nWith size the number of trials and prob the probability of success,\n\n\ndbinom(x, size, prob) to compute \\(P(X = x)\\)\n\n\npbinom(q, size, prob) to compute \\(P(X \\le q)\\)\n\n\npbinom(q, size, prob, lower.tail = FALSE) to compute \\(P(X &gt; q)\\)\n\n\n\n\n\n\n\n## 1. P(X = 6)\ndbinom(x = 6, size = 15, prob = 0.2) \n\n[1] 0.043\n\n## 2. P(X &gt;= 6) = 1 - P(X &lt;= 5)\n1 - pbinom(q = 5, size = 15, prob = 0.2) \n\n[1] 0.0611\n\n\n\n\n## 2. P(X &gt;= 6) = P(X &gt; 5)\npbinom(q = 5, size = 15, prob = 0.2, \n       lower.tail = FALSE)  \n\n[1] 0.0611"
  },
  {
    "objectID": "slides/07-distribution-slides.html#binomial15-0.2",
    "href": "slides/07-distribution-slides.html#binomial15-0.2",
    "title": "Probability Distributions \n",
    "section": "Binomial(15, 0.2)",
    "text": "Binomial(15, 0.2)\n\nplot(x = 0:15, y = dbinom(0:15, size = 15, prob = 0.2), type = 'h', xlab = \"x\", \n     ylab = \"P(X = x)\", lwd = 5, main = \"Binomial(15, 0.2)\")\n\n\n\nSince \\(n = 15\\) and \\(\\pi = 0.2\\), mean is 3.\nMost of the probabilities are around \\(x = 3\\).\nIt‚Äôs very uncommon to see that more than 10 drivers have alcohol level above the legal limit."
  },
  {
    "objectID": "slides/07-distribution-slides.html#poisson-random-variables",
    "href": "slides/07-distribution-slides.html#poisson-random-variables",
    "title": "Probability Distributions \n",
    "section": "Poisson Random Variables",
    "text": "Poisson Random Variables\n\nIf we‚Äôd like to count the number of occurrences of some event over a unit of time period or space (region) and calculate its associated probability, we could consider the Poisson distribution.\n\nNumber of COVID patients arriving at ICU in one hour\nNumber of Marquette students logging onto D2L in one day\nNumber of dandelions per square meter in Marquette campus\n\n\n\n\n\nLet \\(X\\) be a Poisson r.v. Then \\(\\color{blue}{X \\sim Poisson(\\lambda)}\\), where \\(\\lambda\\) is the parameter representing the mean number of occurrences of the event in the interval. \\[\\color{blue}{P(X = x \\mid \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad x = 0, 1, 2, \\dots}\\] with both mean and variance being equal to \\(\\lambda\\).\n\n\nLet \\(X\\) be a Poisson r.v. representing the number of occurrences of some event over a unit of interval (time, space, volume)."
  },
  {
    "objectID": "slides/07-distribution-slides.html#assumptions-and-properties-of-poisson-variables",
    "href": "slides/07-distribution-slides.html#assumptions-and-properties-of-poisson-variables",
    "title": "Probability Distributions \n",
    "section": "Assumptions and Properties of Poisson Variables",
    "text": "Assumptions and Properties of Poisson Variables\n\nüëâ Events occur one at a time; two or more events do not occur at the same time or in the same space or spot.\nüëâ The occurrence of an event in a given period of time or region of space is independent of the occurrence of the event in a nonoverlapping time period or region of space.\nüëâ \\(\\lambda\\) is constant of any period or region.\n\n\nYou cannot say two patients arrived at ICU at the same time.\nYou can always know which patient arrives at ICU earlier.\nYou can always know which students log in D2L earlier\n\n\nCan you find the difference between binomial and Poisson distributions?\n\n\n\n\nThe Poisson distribution\n\nis determined by one single parameter \\(\\lambda\\)\n\nhas possible values \\(x = 0, 1, 2, \\dots\\) with no upper limit (countable), while a binomial variable has possible values \\(0, 1, 2, \\dots, n\\) (finite)"
  },
  {
    "objectID": "slides/07-distribution-slides.html#poisson-distribution-example",
    "href": "slides/07-distribution-slides.html#poisson-distribution-example",
    "title": "Probability Distributions \n",
    "section": "Poisson Distribution Example",
    "text": "Poisson Distribution Example\n\n\nLast year there were 4200 births at the University of Wisconsin Hospital. Assume \\(X\\) be the number of births in a given day at the center, and \\(X \\sim Poisson(\\lambda)\\). Find\n\n\n\\(\\lambda\\), the mean number of births per day.\nthe probability that on a randomly selected day, there are exactly 10 births.\n\n\\(P(X &gt; 10)\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\small \\lambda = \\frac{\\text{Number of birth in a year}}{\\text{Number of days}} = \\frac{4200}{365} = 11.5\\)\n\n\n\n\n\\(\\small P(X = 10 \\mid \\lambda = 11.5) = \\frac{\\lambda^x e^{-\\lambda}}{x!} = \\frac{11.5^{10} e^{-11.5}}{10!} = 0.113\\)\n\n\n\n\n\n\\(\\small P(X &gt; 10) = p(11) + p(12) + \\cdots + p(20) + \\cdots\\) (No end!) \\(\\small P(X &gt; 10) = 1 - P(X \\le 10) = 1 - (p(0) + p(1) + p(2) + \\dots + p(10))\\).\n\n.tip[ I know you are waiting for R implementation!"
  },
  {
    "objectID": "slides/07-distribution-slides.html#poisson-example-compuatation-in-r",
    "href": "slides/07-distribution-slides.html#poisson-example-compuatation-in-r",
    "title": "Probability Distributions \n",
    "section": "Poisson Example Compuatation in R",
    "text": "Poisson Example Compuatation in R\n\n\n\nWith lambda the mean of Poisson distribution,\n\n\ndpois(x, lambda) to compute \\(P(X = x)\\)\n\n\nppois(q, lambda) to compute \\(P(X \\le q)\\)\n\n\nppois(q, lambda, lower.tail = FALSE) to compute \\(P(X &gt; q)\\)\n\n\n\n\n\n\n\n(lam &lt;- 4200 / 365)\n\n[1] 11.5\n\n## P(X = 10)\ndpois(x = 10, lambda = lam)  \n\n[1] 0.113\n\n\n\n\n## P(X &gt; 10) = 1 - P(X &lt;= 10)\n1 - ppois(q = 10, lambda = lam)  \n\n[1] 0.599\n\n## P(X &gt; 10)\nppois(q = 10, lambda = lam, \n      lower.tail = FALSE) \n\n[1] 0.599"
  },
  {
    "objectID": "slides/07-distribution-slides.html#poisson11.5",
    "href": "slides/07-distribution-slides.html#poisson11.5",
    "title": "Probability Distributions \n",
    "section": "Poisson(11.5)",
    "text": "Poisson(11.5)\n\n\n\\(X\\) has no upper limit. The graph is truncated at \\(x = 24\\).\n\n\nCodepar(mar = c(3.8, 3.8, 1, 0), mgp = c(2.5, 0.5, 0))\nplot(0:24, dpois(0:24, lambda = lam), type = 'h', lwd = 5, las = 1,\n     ylab = \"P(X = x)\", xlab = \"x\", main = \"Poisson(11.5)\")"
  },
  {
    "objectID": "slides/07-distribution-slides.html#continuous-probability-distributions-1",
    "href": "slides/07-distribution-slides.html#continuous-probability-distributions-1",
    "title": "Probability Distributions \n",
    "section": "Continuous Probability Distributions",
    "text": "Continuous Probability Distributions\n\nA continuous r.v. can take on any values from an interval of the real line.\nInstead of probability functions, a continuous r.v. \\(X\\) has the probability density function (pdf) \\(f(x)\\) such that for any real value \\(a &lt; b\\), \\[P(a &lt; X &lt; b) = \\int_{a}^b f(x) dx\\]\nThe cumulative distribution function (cdf) of \\(X\\) is defined as \\[F(x) := P(X \\le x) = \\int_{-\\infty}^x f(t)dt\\]\n\n\n\nEvery pdf must satisfy  (1) \\(f(x) \\ge 0\\) for all \\(x\\); (2) \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\) \n\n\n\n\nüòé Luckily we don‚Äôt deal with integrals in this course."
  },
  {
    "objectID": "slides/07-distribution-slides.html#density-curve",
    "href": "slides/07-distribution-slides.html#density-curve",
    "title": "Probability Distributions \n",
    "section": "Density Curve",
    "text": "Density Curve\n\nA pdf generates a graph called the density curve that shows the likelihood of a random variable at all possible values.\n\\(P(a &lt; X &lt; b) = \\int_{a}^b f(x) dx\\): The area under the density curve between \\(a\\) and \\(b\\).\n\\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\): The total area under any density curve is equal to 1.\n\n\n\n** \\(P(a &lt; X &lt; b)\\) is the area under the density curve between \\(a\\) and \\(b\\).**\nThe total area under any density curve is equal to 1."
  },
  {
    "objectID": "slides/07-distribution-slides.html#commonly-used-continuous-distributions",
    "href": "slides/07-distribution-slides.html#commonly-used-continuous-distributions",
    "title": "Probability Distributions \n",
    "section": "Commonly Used Continuous Distributions",
    "text": "Commonly Used Continuous Distributions\n\nDistribution Applet\nIn this course, we will touch normal (Gaussian), Student‚Äôs t, chi-squared, F\nSome other common distributions include uniform, exponential, gamma, beta, inverse gamma, Cauchy, etc. (MATH 4700)"
  },
  {
    "objectID": "slides/07-distribution-slides.html#normal-gaussian-distribution",
    "href": "slides/07-distribution-slides.html#normal-gaussian-distribution",
    "title": "Probability Distributions \n",
    "section": "Normal (Gaussian) Distribution",
    "text": "Normal (Gaussian) Distribution\n\nThe normal distribution, \\(N(\\mu, \\sigma^2\\)), has the pdf given by \\[\\small f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}, \\quad -\\infty &lt; x &lt; \\infty\\]\n\nTwo parameters mean \\(\\mu\\) and variance \\(\\sigma^2\\) (standard deviation \\(\\sigma\\))\nAlways bell shaped, and symmetric about the mean \\(\\mu\\)\n\nWhen \\(\\mu = 0\\) and \\(\\sigma = 1\\), \\(N(0, 1)\\) is called standard normal."
  },
  {
    "objectID": "slides/07-distribution-slides.html#normal-density-curves",
    "href": "slides/07-distribution-slides.html#normal-density-curves",
    "title": "Probability Distributions \n",
    "section": "Normal Density Curves",
    "text": "Normal Density Curves"
  },
  {
    "objectID": "slides/07-distribution-slides.html#standardization-and-z-scores",
    "href": "slides/07-distribution-slides.html#standardization-and-z-scores",
    "title": "Probability Distributions \n",
    "section": "Standardization and Z-Scores",
    "text": "Standardization and Z-Scores\n\nStandardization: Convert \\(N(\\mu, \\sigma^2)\\) to \\(N(0, 1)\\).\nWhy standardization: Put data onto a standardized scale, making comparisons easier!\n\n\n\n\nMeasure\nSAT\nACT\n\n\n\nMean\n1100\n21\n\n\nSD\n200\n6\n\n\n\n\nThe distribution of SAT and ACT scores are both nearly normal.\nSuppose Anna scored 1300 on her SAT and Tommy scored 24 on his ACT. Who performed better?\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor normal distribution, we usually do the so-called Standardization.\nThe idea is to Make a normal distribution standard normal, i.e., convert \\(N(\\mu, \\sigma^2)\\) to \\(N(0, 1)\\).\nBut why do we want to do that?\nWell, by doing so, we can put any data onto a standardized scale, making comparisons easier and reasonable.\nThe distribution of SAT and ACT scores are both nearly normal.\nSuppose Anna scored 1300 on her SAT and Tommy scored 24 on his ACT. Who performed better?\nIt‚Äôs hard to compare the two scores, right? because they are on the different scales.\nAnd standardization helps us compare Anna and Tommy‚Äôs performance because we are gonna put their scores on the same scale. Let‚Äôs see how."
  },
  {
    "objectID": "slides/07-distribution-slides.html#standardization-and-z-scores-1",
    "href": "slides/07-distribution-slides.html#standardization-and-z-scores-1",
    "title": "Probability Distributions \n",
    "section": "Standardization and Z-Scores",
    "text": "Standardization and Z-Scores\n\nIf \\(x\\) is an observation from a distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the standardized value of \\(x\\) is so-called \\(z\\)-score: \\[z = \\frac{x - \\mu}{\\sigma}\\]\n\nA \\(z\\)-score tells us how many standard deviations \\(x\\) falls away from the mean, and in which direction.\n\nObservations larger (smaller) than the mean have positive (negative) \\(z\\)-scores.\nA \\(z\\)-score -1.2 means that \\(x\\) is 1.2 standard deviations to the left of (below) the mean.\nA \\(z\\)-score 1.8 means that \\(x\\) is 1.8 standard deviations to the right of (above) the mean.\n\n\n If \\(X \\sim N(\\mu, \\sigma^2)\\), \\(Z = \\frac{X - \\mu}{\\sigma}\\) follows the standard normal distribution, i.e., \\(Z \\sim N(0, 1)\\). \n\n\nIf z-score is positive, it means that the original x is greater the mean.\nSince we divided by sigma, the standardized value is measured using SD as the unit.\nA \\(z\\)-score tells us how many standard deviations the original observation \\(x\\) falls away from the mean, and in which direction.\n\nObservations larger (smaller) than the mean have positive (negative) \\(z\\)-scores.\nA \\(z\\)-score -1.2 means that \\(x\\) is 1.2 standard deviations to the left of (below) the mean.\nA \\(z\\)-score 1.8 means that the original observation \\(x\\) is 1.8 standard deviations to the right of (above) the mean.\n\n\n If \\(X \\sim N(\\mu, \\sigma^2)\\), \\(Z = \\frac{X - \\mu}{\\sigma}\\) follows the standard normal distribution, i.e., \\(Z \\sim N(0, 1)\\)."
  },
  {
    "objectID": "slides/07-distribution-slides.html#standardization-illustration",
    "href": "slides/07-distribution-slides.html#standardization-illustration",
    "title": "Probability Distributions \n",
    "section": "Standardization Illustration",
    "text": "Standardization Illustration\n\n\n\\(X - \\mu\\) shifts the mean from \\(\\mu\\) to 0\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\frac{X - \\mu}{\\sigma}\\) scales the variation from 4 to 1"
  },
  {
    "objectID": "slides/07-distribution-slides.html#standardization-illustration-1",
    "href": "slides/07-distribution-slides.html#standardization-illustration-1",
    "title": "Probability Distributions \n",
    "section": "Standardization Illustration",
    "text": "Standardization Illustration\n\nA value of \\(x\\) that is 2 standard deviation below \\(\\mu\\) corresponds to \\(z = -2\\).\n\\(z = \\frac{x  -\\mu}{\\sigma} \\iff x = \\mu + z\\sigma\\). If \\(z = -2\\), \\(x = \\mu - 2\\sigma\\)."
  },
  {
    "objectID": "slides/07-distribution-slides.html#sat-and-act-example",
    "href": "slides/07-distribution-slides.html#sat-and-act-example",
    "title": "Probability Distributions \n",
    "section": "SAT and ACT Example",
    "text": "SAT and ACT Example\n\n\n\\(z_{A} = \\frac{x_{A} - \\mu_{SAT}}{\\sigma_{SAT}} = \\frac{1300-1100}{200} = 1\\); \\(z_{T} = \\frac{x_{T} - \\mu_{ACT}}{\\sigma_{ACT}} = \\frac{24-21}{6} = 0.5\\).\n\n\n\nAfter standardizing SAT and ACT distributions, we can compare SAT and ACT scores together.\nThe z-score for Anna is 1, so her SAT score is 1 SD above the mean of SAT.\nThe z-score for Tommy is 0.5, so his ACT score is 0.5 SD above the mean of ACT.\nOriginally, SAT and ACT are two different distributions with different scales.\nBut now we convert both distributions to standard normal distributions, so that they can be compared.\nBecause Anna‚Äôs score is 1 sd higher than the mean, and Tommy‚Äôs socre is 0.5 sd higher than the mean, we conclude that Anna performed better on the test.\n\nAn observation x1 is said to be more unusual than another observation x2 if the absolute value of its Zscore is larger than the absolute value of the other observation‚Äôs Z-score."
  },
  {
    "objectID": "slides/07-distribution-slides.html#finding-tail-areas-px-x",
    "href": "slides/07-distribution-slides.html#finding-tail-areas-px-x",
    "title": "Probability Distributions \n",
    "section": "Finding Tail Areas \\(P(X < x)\\)\n",
    "text": "Finding Tail Areas \\(P(X &lt; x)\\)\n\n\nWhat fraction of students have an SAT score below Anna‚Äôs score of 1300?\n\n\nThis is the same as the percentile Anna is at, which is the percentage of cases that have lower scores than Anna.\nNeed \\(P(X &lt; 1300 \\mid \\mu = 1100, \\sigma = 200)\\) or \\(P(Z &lt; 1 \\mid \\mu = 0, \\sigma = 1)\\).\n\n\n\nIf there are 100 test takers, how many students score are lower than Anna‚Äôs score? It‚Äôs very useful in statistics to be able to identify tail areas of distributions. For instance, what fraction of people have an SAT score below Anna‚Äôs score of 1300? This is the same as the percentile Anna is at, which is the percentage of cases that have lower scores than Anna. We can visualize such a tail area like the curve and shading shown in Figure 4.6.\nThe area to the left of Z represents the fraction of people who scored lower than Anna."
  },
  {
    "objectID": "slides/07-distribution-slides.html#finding-tail-areas-px-x-in-r",
    "href": "slides/07-distribution-slides.html#finding-tail-areas-px-x-in-r",
    "title": "Probability Distributions \n",
    "section": "Finding Tail Areas \\(P(X < x)\\) in R",
    "text": "Finding Tail Areas \\(P(X &lt; x)\\) in R\n\nWith mean and sd representing the mean and standard deviation of a normal distribution\n\n\npnorm(q, mean, sd) to compute \\(P(X \\le q)\\)\n\n\npnorm(q, mean, sd, lower.tail = FALSE) to compute \\(P(X &gt; q)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npnorm(1, mean = 0, sd = 1)\n\n[1] 0.841\n\npnorm(1300, mean = 1100, sd = 200)\n\n[1] 0.841\n\n\n\nThe shaded area represents the proportion 84.1% of SAT test takers who had z-score below 1.\n\n\n\nDon‚Äôt forget R Shiny app is at Normal Calculator\n\nThe default values of mean and sd are 0 and 1 respectively, i.e., \\(N(0, 1)\\).\nIt is rarely used in practice, but you can find tail areas using the normal table (Table 1 in the textbook.)"
  },
  {
    "objectID": "slides/07-distribution-slides.html#sat-example-contd-1",
    "href": "slides/07-distribution-slides.html#sat-example-contd-1",
    "title": "Probability Distributions \n",
    "section": "SAT Example Cont‚Äôd",
    "text": "SAT Example Cont‚Äôd\n\nSAT score follows \\(N(1100, 200^2)\\). Shannon is a SAT taker, and nothing is known about Shannon‚Äôs SAT aptitude. What is the probability Shannon SAT scores at least 1190?\n\n\n\n\n\n Step 1: State the problem \n\n We like to compute \\(P(X \\ge 1190)\\). \n\n\n Step 2: Draw a picture\n\n\n\n\n\n\n\n\n\n\n\n Step 3: Find the area\n\nWe want the upper tail area, so lower.tail = FALSE!\n\npnorm(q = 1190, mean = 1100, sd = 200, \n      lower.tail = FALSE)\n\n[1] 0.326"
  },
  {
    "objectID": "slides/07-distribution-slides.html#normal-percentiles-in-r",
    "href": "slides/07-distribution-slides.html#normal-percentiles-in-r",
    "title": "Probability Distributions \n",
    "section": "Normal Percentiles in R",
    "text": "Normal Percentiles in R\n\nTo get the \\(100p\\)-th percentile (or the \\(p\\)-quantile \\(q\\)), given probability \\(p\\), we use\n\n\nqnorm(p, mean, sd) to get a value of \\(X\\), \\(q\\), such that \\(P(X \\le q) = p\\)\n\n\nqnorm(p, mean, sd, lower.tail = FALSE) to get \\(q\\) such that \\(P(X \\ge q) = p\\)\n\n\n\n\n\nWe may be also interested in the value of \\(X\\) given a probability or percentage.\n\nIn other words, we‚Äôd like to find the \\(100p\\)-th percentile given the probability \\(p\\)."
  },
  {
    "objectID": "slides/07-distribution-slides.html#sat-example",
    "href": "slides/07-distribution-slides.html#sat-example",
    "title": "Probability Distributions \n",
    "section": "SAT Example",
    "text": "SAT Example\nWhat is the 95th percentile for SAT scores?\n\nFind a value \\(q\\) of the normal random variable, not an area (probability), which is 0.95.\n\n\n\n\n\n Step 1: State the problem \n\n Find a variable‚Äôs value \\(q\\) s.t \\(P(X &lt; q) = 0.95\\). \n\n\n Step 2: Draw a picture\n\n\n\n\n\n\n\n\n\n\n\n Step 3: Find the quantile\n\nWe want the quantile, so use qnorm()!\n\nqnorm(p = 0.95, mean = 1100, sd = 200)\n\n[1] 1429"
  },
  {
    "objectID": "slides/07-distribution.html#random-variables-1",
    "href": "slides/07-distribution.html#random-variables-1",
    "title": "Probability Distributions \n",
    "section": "Random Variables",
    "text": "Random Variables\n\nRecap: A variable in a data set is a characteristic that varies from one to another.\n\nA variable can be either categorical or numerical.\nNumerical variables can be either discrete or continuous.\n\n\n\n. . .\n\nA random variable, usually written as \\(X\\) 1, is a variable whose possible values are numerical outcomes determined by chance or randomness of a procedure or experiment.\n\n Toss a coin 2 times. \\(X\\) = # of heads. \n \\(X\\) = # of accidents in W. Wisconsin Ave. per day.\n\n\n\nA random variable has a probability distribution associated with it, accounting for its randomness.\n\n\n\nSimilar to relative frequency table (distribution)\nA probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment in terms of values of r.v. X. Yes. The possible values of \\(X\\) are numerical \\(X = 0, 1, 2\\) that are determined by randomness of the tossing-coin experiment."
  },
  {
    "objectID": "slides/07-distribution.html#discrete-and-continuous-random-variables",
    "href": "slides/07-distribution.html#discrete-and-continuous-random-variables",
    "title": "Probability Distributions \n",
    "section": "Discrete and Continuous Random Variables",
    "text": "Discrete and Continuous Random Variables\n\nA discrete random variable takes on a finite or countable number of values.\nA continuous random variable has infinitely many values, and the collection of values is uncountable.\n\n. . .\n\n\n The number of relationships you‚Äôve ever had is discrete variable because we can count the number and it is finite.\n\nIf we can further determine the probability that the number is 0, 1, 2, or any possible number, it is a discrete random variable.\n\n\n\n Height is continuous because it can be any number within a range. \n\nIf we have a way to quantify the probability that the height is from any value \\(a\\) to any value \\(b\\), it is a continuous random variable."
  },
  {
    "objectID": "slides/07-distribution.html#a-statistician-should-know",
    "href": "slides/07-distribution.html#a-statistician-should-know",
    "title": "Probability Distributions \n",
    "section": "A Statistician Should Know",
    "text": "A Statistician Should Know\n\n\n\n\n\n\n\n\n\n\n\n\nThere are lots of well-known probability distributions out there.\nThe distributions shown here are just a small part of probability distributions\nIn fact there are infinitely many probability distributions.\nA function that satisfies probability distribution conditions is a probability distribution.\nYou can construct our own probability distribution."
  },
  {
    "objectID": "slides/07-distribution.html#discrete-probability-distribution",
    "href": "slides/07-distribution.html#discrete-probability-distribution",
    "title": "Probability Distributions \n",
    "section": "Discrete Probability Distribution",
    "text": "Discrete Probability Distribution\n\nThe probability (mass) function (pf, pmf) of a discrete random variable (r.v.) \\(X\\) is a function \\(P(X = x)\\) (or \\(p(x)\\)) that assigns a probability for every possible number \\(x\\).\nThe probability distribution for a discrete r.v. \\(X\\) displays its probability function.\nThe display can be a table, graph, or mathematical formula of \\(P(X = x)\\).\n\n. . .\n Example: ü™ôü™ô Toss a fair coin twice independently and \\(X\\) is the number of heads. \n\nThe probability distribution of \\(X\\) as a table is\n\n\n\n\n\nx\n0\n1\n2\n\n\nP(X = x)\n0.25\n0.5\n0.25\n\n\n\n\n. . .\n\nüëâ \\(\\{X = x\\}\\) is an event corresponding to an event of some experiment.\n\n. . .\n\nWhat is the event that \\(\\{X = 0\\}\\) corresponds to?\n\n. . .\n\nHow do we get \\(P(X = 0)\\), \\(P(X=1)\\) and \\(P(X=2)\\) ?"
  },
  {
    "objectID": "slides/07-distribution.html#discrete-probability-distribution-as-a-graph",
    "href": "slides/07-distribution.html#discrete-probability-distribution-as-a-graph",
    "title": "Probability Distributions \n",
    "section": "Discrete Probability Distribution as a Graph",
    "text": "Discrete Probability Distribution as a Graph\n\n\n\n\n\n\n\n\n\n\n\\(0 \\le P(X = x) \\le 1\\) for every value \\(x\\) of \\(X\\).\n\n \\(x = 0, 1, 2\\) \n\n\n\n. . .\n\n\n\\(\\sum_{x}P(X=x) = 1\\), where \\(x\\) assumes all possible values.\n\n \\(P(X=0) + P(X = 1) + P(X = 2) = 1\\) \n\n\n\n. . .\n\nThe probabilities for a discrete r.v. are additive because \\(\\{X = a\\}\\) and \\(\\{X = b\\}\\) are disjoint for any possible values \\(a \\ne b\\).\n\n \\(P(X = 1 \\text{ or } 2) = P(\\{X = 1\\} \\cup \\{X = 2\\}) = P(X = 1) + P(X = 2)\\)."
  },
  {
    "objectID": "slides/07-distribution.html#mean-of-a-discrete-random-variable",
    "href": "slides/07-distribution.html#mean-of-a-discrete-random-variable",
    "title": "Probability Distributions \n",
    "section": "Mean of a Discrete Random Variable",
    "text": "Mean of a Discrete Random Variable\n\nSuppose \\(X\\) takes values \\(x_1, \\dots, x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\).\nThe mean or expected value of \\(X\\) is the sum of each outcome multiplied by its corresponding probability: \\[E(X) := x_1 \\times P(X = x_1) + \\dots + x_k \\times P(X = x_k) = \\sum_{i=1}^kx_iP(X=x_i)\\]\nThe Greek letter \\(\\mu\\) may be used in place of the notation \\(E(X)\\).\n\n\n\nGive an example\n\n\n. . .\n\nüëâ The mean of a discrete random variable \\(X\\) is the weighted average of possible values \\(x\\) weighted by their corresponding probability.\n\n. . .\n\nWhat is the mean of \\(X\\) (the number of heads) in the previous example?\n\n\n\nOn average, we will see one heads comes up when we toss a fair coin twice."
  },
  {
    "objectID": "slides/07-distribution.html#variance-of-a-discrete-random-variable",
    "href": "slides/07-distribution.html#variance-of-a-discrete-random-variable",
    "title": "Probability Distributions \n",
    "section": "Variance of a Discrete Random Variable",
    "text": "Variance of a Discrete Random Variable\n\nSuppose \\(X\\) takes values \\(x_1, \\dots , x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\) and expected value \\(\\mu = E(X)\\).\nThe variance of \\(X\\), denoted by \\(\\mathrm{Var}(X)\\) or \\(\\sigma^2\\), is \\[\\small \\mathrm{Var}(X) := (x_1 - \\mu)^2 \\times P(X = x_1) + \\dots + (x_k - \\mu)^2 \\times P(X = x_k) = \\sum_{i=1}^k(x_i - \\mu)^2P(X=x_i)\\]\nThe standard deviation of \\(X\\), \\(\\sigma\\), is the square root of the variance.\n\n. . .\n\nüëâ The variance of a discrete random variable \\(X\\) is the weighted sum of squared deviation from the mean weighted by probability values.\n\n. . .\n\nWhat is the variance of \\(X\\) (the number of heads) in the previous example?"
  },
  {
    "objectID": "slides/07-distribution.html#binomial-experiment-and-random-variable",
    "href": "slides/07-distribution.html#binomial-experiment-and-random-variable",
    "title": "Probability Distributions \n",
    "section": "Binomial Experiment and Random Variable",
    "text": "Binomial Experiment and Random Variable\n\nA binomial experiment is the one having the following properties:\n\nüëâ The experiment consists of a fixed number of identical trials \\(n\\).\nüëâ Each trial results in one of exactly two outcomes (success (S) and failure (F)).\nüëâ Trials are independent, meaning that the outcome of any trial does not affect the outcome of any other trial.\nüëâ The probability of success is constant for all trials.\n\n\nIf \\(X\\) is defined as  the number of successes observed in \\(n\\) trials , \\(X\\) is a binomial random variable.\n\n. . .\n\n\nThe word success just means one of the two outcomes, and does not necessarily mean something good. \n\nüò≤ Can define Drug abuse as success and No drug abuse as failure.\n\n\n\nif smoking and non-smoking are the only two outcomes of some binomial experiment."
  },
  {
    "objectID": "slides/07-distribution.html#binomial-distribution-2",
    "href": "slides/07-distribution.html#binomial-distribution-2",
    "title": "Probability Distributions \n",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\nThe probability function \\(P(X = x)\\) of a binomial r.v. \\(X\\) can be fully determined by\n\nthe number of trials \\(n\\)\n\nprobability of success \\(\\pi\\)\n\n\n\nDifferent \\((n, \\pi)\\) pairs generate different binomial probability distributions.\n\\(X\\) is said to follow a binomial distribution with parameters \\(n\\) and \\(\\pi\\), written as \\(\\color{blue}{X \\sim binomial(n, \\pi)}\\).\nThe binomial probability function is \\[ \\color{blue}{P(X = x \\mid n, \\pi) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x}, \\quad x = 0, 1, 2, \\dots, n}\\] with mean \\(\\mu = E(X) = n\\pi\\) and variance \\(\\sigma^2 = \\mathrm{Var}(X) = n\\pi(1-\\pi)\\).\n\n\n\nOnce \\((n, \\pi)\\) is fixed, we know exactly what the distribution looks like, and we can form a table, graph, and provide a mathematical formula of the binomial distribution.\n\n\n. . .\n\nTossing a fair coin two times independently. Let \\(X =\\) # of heads. Is \\(X\\) a binomial r.v.?\n\n\n\nExample: Toss a fair coin two times independently.\n\n\n\\(X =\\) # of heads\n\\(X \\sim binomial(n=2, \\pi=1/2)\\)"
  },
  {
    "objectID": "slides/07-distribution.html#binomial-distribution-example",
    "href": "slides/07-distribution.html#binomial-distribution-example",
    "title": "Probability Distributions \n",
    "section": "Binomial Distribution Example",
    "text": "Binomial Distribution Example\n\n\nAssume that 20% of all drivers have a blood alcohol level above the legal limit. For a random sample of 15 vehicles, compute the probability that:\n\nExactly 6 of the 15 drivers will exceed the legal limit.\nOf the 15 drivers, 6 or more will exceed the legal limit.\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\nSuppose it is a binomial experiment with \\(n = 15\\) and \\(\\pi = 0.2\\).\nLet \\(X\\) be the number of drivers exceeding limit.\n\\(X \\sim binomial(15, 0.2)\\).\n\n\\[ \\color{blue}{P(X = x \\mid n=15, \\pi=0.2) = \\frac{15!}{x!(15-x)!}(0.2)^x(1-0.2)^{15-x}, \\quad x = 0, 1, 2, \\dots, 15}\\]\n\n\nOn a particular roadway, assume that 20% of all drivers have a blood alcohol level above the legal limit.\n\nAll 15 drivers will have a blood alcohol level within the legal limit."
  },
  {
    "objectID": "slides/07-distribution.html#binomial-distribution-example-x-sim-binomial15-0.2",
    "href": "slides/07-distribution.html#binomial-distribution-example-x-sim-binomial15-0.2",
    "title": "Probability Distributions \n",
    "section": "Binomial Distribution Example \\(X \\sim binomial(15, 0.2)\\)\n",
    "text": "Binomial Distribution Example \\(X \\sim binomial(15, 0.2)\\)"
  },
  {
    "objectID": "slides/07-distribution.html#binomial-distribution-example-1",
    "href": "slides/07-distribution.html#binomial-distribution-example-1",
    "title": "Probability Distributions \n",
    "section": "Binomial Distribution Example",
    "text": "Binomial Distribution Example\n\n\nAssume that 20% of all drivers have a blood alcohol level above the legal limit. For a random sample of 15 vehicles, compute the probability that:\n\nExactly 6 of the 15 drivers will exceed the legal limit.\nOf the 15 drivers, 6 or more will exceed the legal limit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn a particular roadway, assume that 20% of all drivers have a blood alcohol level above the legal limit.\nSuppose it is a binomial experiment with \\(n = 15\\) and \\(\\pi = 0.2\\).\nLet \\(X\\) be the number of drivers exceeding limit. Then \\(X \\sim binomial(15, 0.2)\\).\n\n\n. . .\n\n\\(\\small P(X = 6) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x} = \\frac{15!}{6!(15-6)!}(0.2)^6(1-0.2)^{15-6} = 0.043\\)\n\n. . .\n\n\\(\\small P(X \\ge 6) = p(6) + \\dots + p(15) = 1 - P(X \\le 5) = 1 - (p(0) + p(1) + \\dots + p(5)) = 0.0611\\)\n\n\n\nAll 15 drivers will have a blood alcohol level within the legal limit.\n\\(\\small P(X = 0) = \\frac{15!}{0!(15-0)!}(0.2)^0(1-0.2)^{15-0} = 0.0352\\)\n\n\n\n\\(P(X \\ge 6) = 1 - P(X \\le 5)\\) very useful trick\nCalculation is super tedious, even you use a calculator!\n\n\n. . .\n\nNever do this by hand. We compute them using R!"
  },
  {
    "objectID": "slides/07-distribution.html#binomial-example-computation-in-r",
    "href": "slides/07-distribution.html#binomial-example-computation-in-r",
    "title": "Probability Distributions \n",
    "section": "Binomial Example Computation in R",
    "text": "Binomial Example Computation in R\n\n\nWith size the number of trials and prob the probability of success,\n\n\ndbinom(x, size, prob) to compute \\(P(X = x)\\)\n\n\npbinom(q, size, prob) to compute \\(P(X \\le q)\\)\n\n\npbinom(q, size, prob, lower.tail = FALSE) to compute \\(P(X &gt; q)\\)\n\n\n\n\n\n\n\n## 1. P(X = 6)\ndbinom(x = 6, size = 15, prob = 0.2) \n\n[1] 0.043\n\n## 2. P(X &gt;= 6) = 1 - P(X &lt;= 5)\n1 - pbinom(q = 5, size = 15, prob = 0.2) \n\n[1] 0.0611\n\n\n\n\n## 2. P(X &gt;= 6) = P(X &gt; 5)\npbinom(q = 5, size = 15, prob = 0.2, \n       lower.tail = FALSE)  \n\n[1] 0.0611"
  },
  {
    "objectID": "slides/07-distribution.html#binomial15-0.2",
    "href": "slides/07-distribution.html#binomial15-0.2",
    "title": "Probability Distributions \n",
    "section": "Binomial(15, 0.2)",
    "text": "Binomial(15, 0.2)\n\nplot(x = 0:15, y = dbinom(0:15, size = 15, prob = 0.2), type = 'h', xlab = \"x\", \n     ylab = \"P(X = x)\", lwd = 5, main = \"Binomial(15, 0.2)\")\n\n\n\n\n\n\n\n\n\nSince \\(n = 15\\) and \\(\\pi = 0.2\\), mean is 3.\nMost of the probabilities are around \\(x = 3\\).\nIt‚Äôs very uncommon to see that more than 10 drivers have alcohol level above the legal limit."
  },
  {
    "objectID": "slides/07-distribution.html#poisson-random-variables",
    "href": "slides/07-distribution.html#poisson-random-variables",
    "title": "Probability Distributions \n",
    "section": "Poisson Random Variables",
    "text": "Poisson Random Variables\n\nIf we‚Äôd like to count the number of occurrences of some event over a unit of time period or space (region) and calculate its associated probability, we could consider the Poisson distribution.\n\nNumber of COVID patients arriving at ICU in one hour\nNumber of Marquette students logging onto D2L in one day\nNumber of dandelions per square meter in Marquette campus\n\n\n\n. . .\n\nLet \\(X\\) be a Poisson r.v. Then \\(\\color{blue}{X \\sim Poisson(\\lambda)}\\), where \\(\\lambda\\) is the parameter representing the mean number of occurrences of the event in the interval. \\[\\color{blue}{P(X = x \\mid \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad x = 0, 1, 2, \\dots}\\] with both mean and variance being equal to \\(\\lambda\\).\n\n\n\nLet \\(X\\) be a Poisson r.v. representing the number of occurrences of some event over a unit of interval (time, space, volume)."
  },
  {
    "objectID": "slides/07-distribution.html#assumptions-and-properties-of-poisson-variables",
    "href": "slides/07-distribution.html#assumptions-and-properties-of-poisson-variables",
    "title": "Probability Distributions \n",
    "section": "Assumptions and Properties of Poisson Variables",
    "text": "Assumptions and Properties of Poisson Variables\n\nüëâ Events occur one at a time; two or more events do not occur at the same time or in the same space or spot.\nüëâ The occurrence of an event in a given period of time or region of space is independent of the occurrence of the event in a nonoverlapping time period or region of space.\nüëâ \\(\\lambda\\) is constant of any period or region.\n\n\n\nYou cannot say two patients arrived at ICU at the same time.\nYou can always know which patient arrives at ICU earlier.\nYou can always know which students log in D2L earlier\n\n\n. . .\n\nCan you find the difference between binomial and Poisson distributions?\n\n. . .\n\nThe Poisson distribution\n\nis determined by one single parameter \\(\\lambda\\)\n\nhas possible values \\(x = 0, 1, 2, \\dots\\) with no upper limit (countable), while a binomial variable has possible values \\(0, 1, 2, \\dots, n\\) (finite)"
  },
  {
    "objectID": "slides/07-distribution.html#poisson-distribution-example",
    "href": "slides/07-distribution.html#poisson-distribution-example",
    "title": "Probability Distributions \n",
    "section": "Poisson Distribution Example",
    "text": "Poisson Distribution Example\n\n\nLast year there were 4200 births at the University of Wisconsin Hospital. Assume \\(X\\) be the number of births in a given day at the center, and \\(X \\sim Poisson(\\lambda)\\). Find\n\n\n\\(\\lambda\\), the mean number of births per day.\nthe probability that on a randomly selected day, there are exactly 10 births.\n\n\\(P(X &gt; 10)\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\n\\(\\small \\lambda = \\frac{\\text{Number of birth in a year}}{\\text{Number of days}} = \\frac{4200}{365} = 11.5\\)\n\n. . .\n\n\\(\\small P(X = 10 \\mid \\lambda = 11.5) = \\frac{\\lambda^x e^{-\\lambda}}{x!} = \\frac{11.5^{10} e^{-11.5}}{10!} = 0.113\\)\n\n. . .\n\n\n\\(\\small P(X &gt; 10) = p(11) + p(12) + \\cdots + p(20) + \\cdots\\) (No end!) \\(\\small P(X &gt; 10) = 1 - P(X \\le 10) = 1 - (p(0) + p(1) + p(2) + \\dots + p(10))\\).\n\n\n.tip[ I know you are waiting for R implementation!"
  },
  {
    "objectID": "slides/07-distribution.html#poisson-example-compuatation-in-r",
    "href": "slides/07-distribution.html#poisson-example-compuatation-in-r",
    "title": "Probability Distributions \n",
    "section": "Poisson Example Compuatation in R",
    "text": "Poisson Example Compuatation in R\n\n\n\nWith lambda the mean of Poisson distribution,\n\n\ndpois(x, lambda) to compute \\(P(X = x)\\)\n\n\nppois(q, lambda) to compute \\(P(X \\le q)\\)\n\n\nppois(q, lambda, lower.tail = FALSE) to compute \\(P(X &gt; q)\\)\n\n\n\n\n\n\n\n(lam &lt;- 4200 / 365)\n\n[1] 11.5\n\n## P(X = 10)\ndpois(x = 10, lambda = lam)  \n\n[1] 0.113\n\n\n\n\n## P(X &gt; 10) = 1 - P(X &lt;= 10)\n1 - ppois(q = 10, lambda = lam)  \n\n[1] 0.599\n\n## P(X &gt; 10)\nppois(q = 10, lambda = lam, \n      lower.tail = FALSE) \n\n[1] 0.599"
  },
  {
    "objectID": "slides/07-distribution.html#poisson11.5",
    "href": "slides/07-distribution.html#poisson11.5",
    "title": "Probability Distributions \n",
    "section": "Poisson(11.5)",
    "text": "Poisson(11.5)\n\n\n\\(X\\) has no upper limit. The graph is truncated at \\(x = 24\\).\n\n\nCodepar(mar = c(3.8, 3.8, 1, 0), mgp = c(2.5, 0.5, 0))\nplot(0:24, dpois(0:24, lambda = lam), type = 'h', lwd = 5, las = 1,\n     ylab = \"P(X = x)\", xlab = \"x\", main = \"Poisson(11.5)\")"
  },
  {
    "objectID": "slides/07-distribution.html#continuous-probability-distributions-1",
    "href": "slides/07-distribution.html#continuous-probability-distributions-1",
    "title": "Probability Distributions \n",
    "section": "Continuous Probability Distributions",
    "text": "Continuous Probability Distributions\n\nA continuous r.v. can take on any values from an interval of the real line.\nInstead of probability functions, a continuous r.v. \\(X\\) has the probability density function (pdf) \\(f(x)\\) such that for any real value \\(a &lt; b\\), \\[P(a &lt; X &lt; b) = \\int_{a}^b f(x) dx\\]\nThe cumulative distribution function (cdf) of \\(X\\) is defined as \\[F(x) := P(X \\le x) = \\int_{-\\infty}^x f(t)dt\\]\n\n. . .\n\nEvery pdf must satisfy  (1) \\(f(x) \\ge 0\\) for all \\(x\\); (2) \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\) \n\n\n. . .\nüòé Luckily we don‚Äôt deal with integrals in this course."
  },
  {
    "objectID": "slides/07-distribution.html#density-curve",
    "href": "slides/07-distribution.html#density-curve",
    "title": "Probability Distributions \n",
    "section": "Density Curve",
    "text": "Density Curve\n\nA pdf generates a graph called the density curve that shows the likelihood of a random variable at all possible values.\n\\(P(a &lt; X &lt; b) = \\int_{a}^b f(x) dx\\): The area under the density curve between \\(a\\) and \\(b\\).\n\\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\): The total area under any density curve is equal to 1.\n\n\n\n\n\n\n\n\n\n\n\n** \\(P(a &lt; X &lt; b)\\) is the area under the density curve between \\(a\\) and \\(b\\).**\nThe total area under any density curve is equal to 1."
  },
  {
    "objectID": "slides/07-distribution.html#commonly-used-continuous-distributions",
    "href": "slides/07-distribution.html#commonly-used-continuous-distributions",
    "title": "Probability Distributions \n",
    "section": "Commonly Used Continuous Distributions",
    "text": "Commonly Used Continuous Distributions\n\nDistribution Applet\nIn this course, we will touch normal (Gaussian), Student‚Äôs t, chi-squared, F\nSome other common distributions include uniform, exponential, gamma, beta, inverse gamma, Cauchy, etc. (MATH 4700)"
  },
  {
    "objectID": "slides/07-distribution.html#normal-gaussian-distribution",
    "href": "slides/07-distribution.html#normal-gaussian-distribution",
    "title": "Probability Distributions \n",
    "section": "Normal (Gaussian) Distribution",
    "text": "Normal (Gaussian) Distribution\n\nThe normal distribution, \\(N(\\mu, \\sigma^2\\)), has the pdf given by \\[\\small f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}, \\quad -\\infty &lt; x &lt; \\infty\\]\n\nTwo parameters mean \\(\\mu\\) and variance \\(\\sigma^2\\) (standard deviation \\(\\sigma\\))\nAlways bell shaped, and symmetric about the mean \\(\\mu\\)\n\nWhen \\(\\mu = 0\\) and \\(\\sigma = 1\\), \\(N(0, 1)\\) is called standard normal."
  },
  {
    "objectID": "slides/07-distribution.html#normal-density-curves",
    "href": "slides/07-distribution.html#normal-density-curves",
    "title": "Probability Distributions \n",
    "section": "Normal Density Curves",
    "text": "Normal Density Curves"
  },
  {
    "objectID": "slides/07-distribution.html#standardization-and-z-scores",
    "href": "slides/07-distribution.html#standardization-and-z-scores",
    "title": "Probability Distributions \n",
    "section": "Standardization and Z-Scores",
    "text": "Standardization and Z-Scores\n\nStandardization: Convert \\(N(\\mu, \\sigma^2)\\) to \\(N(0, 1)\\).\nWhy standardization: Put data onto a standardized scale, making comparisons easier!\n\n\n\n\nMeasure\nSAT\nACT\n\n\n\nMean\n1100\n21\n\n\nSD\n200\n6\n\n\n\n\nThe distribution of SAT and ACT scores are both nearly normal.\nSuppose Anna scored 1300 on her SAT and Tommy scored 24 on his ACT. Who performed better?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor normal distribution, we usually do the so-called Standardization.\nThe idea is to Make a normal distribution standard normal, i.e., convert \\(N(\\mu, \\sigma^2)\\) to \\(N(0, 1)\\).\nBut why do we want to do that?\nWell, by doing so, we can put any data onto a standardized scale, making comparisons easier and reasonable.\nThe distribution of SAT and ACT scores are both nearly normal.\nSuppose Anna scored 1300 on her SAT and Tommy scored 24 on his ACT. Who performed better?\nIt‚Äôs hard to compare the two scores, right? because they are on the different scales.\nAnd standardization helps us compare Anna and Tommy‚Äôs performance because we are gonna put their scores on the same scale. Let‚Äôs see how."
  },
  {
    "objectID": "slides/07-distribution.html#standardization-and-z-scores-1",
    "href": "slides/07-distribution.html#standardization-and-z-scores-1",
    "title": "Probability Distributions \n",
    "section": "Standardization and Z-Scores",
    "text": "Standardization and Z-Scores\n\nIf \\(x\\) is an observation from a distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the standardized value of \\(x\\) is so-called \\(z\\)-score: \\[z = \\frac{x - \\mu}{\\sigma}\\]\n\nA \\(z\\)-score tells us how many standard deviations \\(x\\) falls away from the mean, and in which direction.\n\nObservations larger (smaller) than the mean have positive (negative) \\(z\\)-scores.\nA \\(z\\)-score -1.2 means that \\(x\\) is 1.2 standard deviations to the left of (below) the mean.\nA \\(z\\)-score 1.8 means that \\(x\\) is 1.8 standard deviations to the right of (above) the mean.\n\n\n If \\(X \\sim N(\\mu, \\sigma^2)\\), \\(Z = \\frac{X - \\mu}{\\sigma}\\) follows the standard normal distribution, i.e., \\(Z \\sim N(0, 1)\\). \n\n\n\nIf z-score is positive, it means that the original x is greater the mean.\nSince we divided by sigma, the standardized value is measured using SD as the unit.\nA \\(z\\)-score tells us how many standard deviations the original observation \\(x\\) falls away from the mean, and in which direction.\n\nObservations larger (smaller) than the mean have positive (negative) \\(z\\)-scores.\nA \\(z\\)-score -1.2 means that \\(x\\) is 1.2 standard deviations to the left of (below) the mean.\nA \\(z\\)-score 1.8 means that the original observation \\(x\\) is 1.8 standard deviations to the right of (above) the mean.\n\n\n If \\(X \\sim N(\\mu, \\sigma^2)\\), \\(Z = \\frac{X - \\mu}{\\sigma}\\) follows the standard normal distribution, i.e., \\(Z \\sim N(0, 1)\\)."
  },
  {
    "objectID": "slides/07-distribution.html#standardization-illustration",
    "href": "slides/07-distribution.html#standardization-illustration",
    "title": "Probability Distributions \n",
    "section": "Standardization Illustration",
    "text": "Standardization Illustration\n\n\n\\(X - \\mu\\) shifts the mean from \\(\\mu\\) to 0\n\n\n\n\n\n\n\n\n\n. . .\n\n\n\\(\\frac{X - \\mu}{\\sigma}\\) scales the variation from 4 to 1"
  },
  {
    "objectID": "slides/07-distribution.html#standardization-illustration-1",
    "href": "slides/07-distribution.html#standardization-illustration-1",
    "title": "Probability Distributions \n",
    "section": "Standardization Illustration",
    "text": "Standardization Illustration\n\nA value of \\(x\\) that is 2 standard deviation below \\(\\mu\\) corresponds to \\(z = -2\\).\n\\(z = \\frac{x  -\\mu}{\\sigma} \\iff x = \\mu + z\\sigma\\). If \\(z = -2\\), \\(x = \\mu - 2\\sigma\\)."
  },
  {
    "objectID": "slides/07-distribution.html#sat-and-act-example",
    "href": "slides/07-distribution.html#sat-and-act-example",
    "title": "Probability Distributions \n",
    "section": "SAT and ACT Example",
    "text": "SAT and ACT Example\n\n\n\\(z_{A} = \\frac{x_{A} - \\mu_{SAT}}{\\sigma_{SAT}} = \\frac{1300-1100}{200} = 1\\); \\(z_{T} = \\frac{x_{T} - \\mu_{ACT}}{\\sigma_{ACT}} = \\frac{24-21}{6} = 0.5\\).\n\n\n\n\n\n\n\n\n\n\n\nAfter standardizing SAT and ACT distributions, we can compare SAT and ACT scores together.\nThe z-score for Anna is 1, so her SAT score is 1 SD above the mean of SAT.\nThe z-score for Tommy is 0.5, so his ACT score is 0.5 SD above the mean of ACT.\nOriginally, SAT and ACT are two different distributions with different scales.\nBut now we convert both distributions to standard normal distributions, so that they can be compared.\nBecause Anna‚Äôs score is 1 sd higher than the mean, and Tommy‚Äôs socre is 0.5 sd higher than the mean, we conclude that Anna performed better on the test.\n\nAn observation x1 is said to be more unusual than another observation x2 if the absolute value of its Zscore is larger than the absolute value of the other observation‚Äôs Z-score."
  },
  {
    "objectID": "slides/07-distribution.html#finding-tail-areas-px-x",
    "href": "slides/07-distribution.html#finding-tail-areas-px-x",
    "title": "Probability Distributions \n",
    "section": "Finding Tail Areas \\(P(X < x)\\)\n",
    "text": "Finding Tail Areas \\(P(X &lt; x)\\)\n\n\nWhat fraction of students have an SAT score below Anna‚Äôs score of 1300?\n\n\nThis is the same as the percentile Anna is at, which is the percentage of cases that have lower scores than Anna.\nNeed \\(P(X &lt; 1300 \\mid \\mu = 1100, \\sigma = 200)\\) or \\(P(Z &lt; 1 \\mid \\mu = 0, \\sigma = 1)\\).\n\n\n\n\n\n\n\n\n\n\n\nIf there are 100 test takers, how many students score are lower than Anna‚Äôs score? It‚Äôs very useful in statistics to be able to identify tail areas of distributions. For instance, what fraction of people have an SAT score below Anna‚Äôs score of 1300? This is the same as the percentile Anna is at, which is the percentage of cases that have lower scores than Anna. We can visualize such a tail area like the curve and shading shown in Figure 4.6.\nThe area to the left of Z represents the fraction of people who scored lower than Anna."
  },
  {
    "objectID": "slides/07-distribution.html#finding-tail-areas-px-x-in-r",
    "href": "slides/07-distribution.html#finding-tail-areas-px-x-in-r",
    "title": "Probability Distributions \n",
    "section": "Finding Tail Areas \\(P(X < x)\\) in R",
    "text": "Finding Tail Areas \\(P(X &lt; x)\\) in R\n\nWith mean and sd representing the mean and standard deviation of a normal distribution\n\n\npnorm(q, mean, sd) to compute \\(P(X \\le q)\\)\n\n\npnorm(q, mean, sd, lower.tail = FALSE) to compute \\(P(X &gt; q)\\)\n\n\n\n\n. . .\n\n\n\n\n\n\n\n\n\n\n\n\npnorm(1, mean = 0, sd = 1)\n\n[1] 0.841\n\npnorm(1300, mean = 1100, sd = 200)\n\n[1] 0.841\n\n\n\nThe shaded area represents the proportion 84.1% of SAT test takers who had z-score below 1.\n\n\n\n\n\nDon‚Äôt forget R Shiny app is at Normal Calculator\n\nThe default values of mean and sd are 0 and 1 respectively, i.e., \\(N(0, 1)\\).\nIt is rarely used in practice, but you can find tail areas using the normal table (Table 1 in the textbook.)"
  },
  {
    "objectID": "slides/07-distribution.html#sat-example-contd",
    "href": "slides/07-distribution.html#sat-example-contd",
    "title": "Probability Distributions \n",
    "section": "SAT Example Cont‚Äôd",
    "text": "SAT Example Cont‚Äôd\n\nSAT score follows \\(N(1100, 200^2)\\). Shannon is a SAT taker, and nothing is known about Shannon‚Äôs SAT aptitude. What is the probability Shannon SAT scores at least 1190?"
  },
  {
    "objectID": "slides/07-distribution.html#sat-example-contd-1",
    "href": "slides/07-distribution.html#sat-example-contd-1",
    "title": "Probability Distributions \n",
    "section": "SAT Example Cont‚Äôd",
    "text": "SAT Example Cont‚Äôd\n\nSAT score follows \\(N(1100, 200^2)\\). Shannon is a SAT taker, and nothing is known about Shannon‚Äôs SAT aptitude. What is the probability Shannon SAT scores at least 1190?\n\n\n\n\n\n Step 1: State the problem \n\n We like to compute \\(P(X \\ge 1190)\\). \n\n\n Step 2: Draw a picture\n\n\n\n\n\n\n\n\n\n\n\n Step 3: Find the area\n\nWe want the upper tail area, so lower.tail = FALSE!\n\npnorm(q = 1190, mean = 1100, sd = 200, \n      lower.tail = FALSE)\n\n[1] 0.326"
  },
  {
    "objectID": "slides/07-distribution.html#sat-example-contd-2",
    "href": "slides/07-distribution.html#sat-example-contd-2",
    "title": "Probability Distributions \n",
    "section": "SAT Example Cont‚Äôd",
    "text": "SAT Example Cont‚Äôd\n\n\n\n\n\n\n\n\n\n\n Step 3: Find \\(z\\)-score :\n\n \\(z = \\frac{1190 - 1100}{200} = 0.45\\) and we like to compute \\(P(X &gt; 1190) = P\\left( \\frac{X - \\mu}{\\sigma} &gt; \\frac{1190 - 1000}{200} \\right) = P(Z &gt; 0.45) = 1 - P(Z \\le 0.45)\\) \n\n\n Step 4: Find the area using pnorm() \n\n\n1 - pnorm(0.45)\n\n[1] 0.326"
  },
  {
    "objectID": "slides/07-distribution.html#normal-percentiles-in-r",
    "href": "slides/07-distribution.html#normal-percentiles-in-r",
    "title": "Probability Distributions \n",
    "section": "Normal Percentiles in R",
    "text": "Normal Percentiles in R\n\nTo get the \\(100p\\)-th percentile (or the \\(p\\)-quantile \\(q\\)), given probability \\(p\\), we use\n\n\nqnorm(p, mean, sd) to get a value of \\(X\\), \\(q\\), such that \\(P(X \\le q) = p\\)\n\n\nqnorm(p, mean, sd, lower.tail = FALSE) to get \\(q\\) such that \\(P(X \\ge q) = p\\)\n\n\n\n\n\n\nWe may be also interested in the value of \\(X\\) given a probability or percentage.\n\nIn other words, we‚Äôd like to find the \\(100p\\)-th percentile given the probability \\(p\\)."
  },
  {
    "objectID": "slides/07-distribution.html#sat-example",
    "href": "slides/07-distribution.html#sat-example",
    "title": "Probability Distributions \n",
    "section": "SAT Example",
    "text": "SAT Example\nWhat is the 95th percentile for SAT scores?\n\nFind a value \\(q\\) of the normal random variable, not an area (probability), which is 0.95.\n\n\n\n\n\n Step 1: State the problem \n\n Find a variable‚Äôs value \\(q\\) s.t \\(P(X &lt; q) = 0.95\\). \n\n\n Step 2: Draw a picture\n\n\n\n\n\n\n\n\n\n\n\n Step 3: Find the quantile\n\nWe want the quantile, so use qnorm()!\n\nqnorm(p = 0.95, mean = 1100, sd = 200)\n\n[1] 1429"
  },
  {
    "objectID": "slides/07-distribution.html#sat-example-contd-3",
    "href": "slides/07-distribution.html#sat-example-contd-3",
    "title": "Probability Distributions \n",
    "section": "SAT Example Cont‚Äôd",
    "text": "SAT Example Cont‚Äôd\n\n\n Step 3: Find \\(z\\)-score s.t. \\(P(Z &lt; z) = 0.95\\) using qnorm():\n\n\n(z_95 &lt;- qnorm(0.95))\n\n[1] 1.64\n\n\n\n\n Step 4: Find the \\(x\\) of the original scale \n\n \\(z_{0.95} = \\frac{x-\\mu}{\\sigma}\\). So \\(x = \\mu + z_{0.95}\\sigma\\). \n\n\n\n\n(x_95 &lt;- 1100 + z_95 * 200)\n\n[1] 1429\n\n\n\nThe 95th percentile for SAT scores is 1429.\n\n. . .\n\nqnorm(p = 0.95, mean = 1100, sd = 200)\n\n[1] 1429"
  },
  {
    "objectID": "slides/07-distribution.html#finding-probabilties",
    "href": "slides/07-distribution.html#finding-probabilties",
    "title": "Probability Distributions \n",
    "section": "Finding Probabilties",
    "text": "Finding Probabilties\nüëâ Draw and label the normal curve and shade the area of interest.\n\nüëâ Less than\n\n\\(\\small P(X &lt; x) = P(Z &lt; z)\\)\npnorm(z)\npnorm(x, mean = mu, sd = sigma)\n\n\nüëâ Greater than\n\n\\(\\small P(X &gt; x) = P(Z &gt; z) = 1 - P(Z \\le z)\\)\n1 - pnorm(z)\n\n\n\n\n\nüëâ Standardization is not a must.\nüëâ We have to specify the mean and SD of the original distribution of \\(X\\), like pnorm(x, mean = mu, sd = sigma)."
  },
  {
    "objectID": "slides/07-distribution.html#finding-probabilties-1",
    "href": "slides/07-distribution.html#finding-probabilties-1",
    "title": "Probability Distributions \n",
    "section": "Finding Probabilties",
    "text": "Finding Probabilties\n\nüëâ Between two numbers\n\n\\(\\small P(a &lt; X &lt; b) = P(z_a &lt; Z &lt; z_b) = P(Z &lt; z_b) - P(Z &lt; z_a)\\)\npnorm(z_b) - pnorm(z_a)\n\n\nüëâ Outside of two numbers \\((a &lt; b)\\) \\[\\small \\begin{align} P(X &lt; a \\text{ or } X &gt; b) &= P(Z &lt; z_a \\text{ or } Z &gt; z_b) \\\\ &= P(Z &lt; z_a) + P(Z &gt; z_b) \\\\ &= P(Z &lt; z_a) + 1 - P(Z &lt; z_b) \\end{align}\\]\n\npnorm(z_a) + 1 - pnorm(z_b)\npnorm(z_a) + pnorm(z_b, lower.tail = FALSE)\n\n\n\n\n\nüëâ Any probability can be computed using the ‚Äúless than‚Äù form (lower or left tail).\nüëâ If the calculation involves the ‚Äúgreater than‚Äù form, add lower.tail = FALSE in pnorm()."
  },
  {
    "objectID": "slides/07-distribution.html#checking-normality-normal-quantile-plot",
    "href": "slides/07-distribution.html#checking-normality-normal-quantile-plot",
    "title": "Probability Distributions \n",
    "section": "Checking Normality: Normal Quantile Plot",
    "text": "Checking Normality: Normal Quantile Plot\n\nMany statistical methods assume variables are normally distributed.\nTesting the appropriateness of the normal assumption is a key step.\n\nA normal quantile plot (normal probability plot) or a Quantile-Quantile plot (QQ plot) helps us check normality assumption.\n\n\n\\(X\\)-axis: Quantiles of the ordered data if the data were normally distributed.\n\n\\(Y\\)-axis: Ordered data values\n\n\nIf the data are like normally distributed, the points on the QQ plot will lie close to a straight line.\n\n\n\nMany statistical methods assume variables are normally distributed.\nTesting the appropriateness of the normal assumption is a key step in many data analyses.\nA normal quantile plot (normal probability plot) or a Quantile-Quantile plot (QQ plot) consists of a plot of the ordered data on the Y-axis and the \\(z\\)-scores associated with order of the observations on the X-axis.\nIf the distribution of the data is close to a normal distribution, the plotted points on the QQ plot will lie close to a straight line."
  },
  {
    "objectID": "slides/07-distribution.html#qq-plot-in-r",
    "href": "slides/07-distribution.html#qq-plot-in-r",
    "title": "Probability Distributions \n",
    "section": "QQ plot in R",
    "text": "QQ plot in R\n\nqqnorm(normal_sample, main = \"Normal data\", col = 4)\nqqline(normal_sample)\nqqnorm(right_skewed_sample, main = \"Right-skewed data\", col = 6)\nqqline(right_skewed_sample)"
  },
  {
    "objectID": "slides/07-distribution.html#footnotes",
    "href": "slides/07-distribution.html#footnotes",
    "title": "Probability Distributions \n",
    "section": "Footnotes",
    "text": "Footnotes\n\nUsually in statistics, a capital \\(X\\) represents a random variable and a small \\(x\\) represents a realized value of \\(X\\).‚Ü©Ô∏é"
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#parameter",
    "href": "slides/08-sampling-dist-clt-slides.html#parameter",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Parameter",
    "text": "Parameter\n\n\nParameters in a probability distribution are the values describing the entire distribution.\n\n Binomial: two parameters \\(n\\) and \\(\\pi\\) \n Poisson: one parameter \\(\\lambda\\) \n Normal: two parameters \\(\\mu\\) and \\(\\sigma\\) \n\n\n\n\n\nIn statistics, we usually assume our target population follows some distribution, but its parameters are unknown to us.\n\n\n\n\n\n Human weight follows \\(N(\\mu, \\sigma^2)\\) \n\n\n\n\n\n\n\n\n\n # of snowstorms in one year follows \\(Poisson(\\lambda)\\)"
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#treat-each-data-point-as-a-random-variable",
    "href": "slides/08-sampling-dist-clt-slides.html#treat-each-data-point-as-a-random-variable",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Treat Each Data Point as a Random Variable",
    "text": "Treat Each Data Point as a Random Variable\n\n\\(n\\) random variables \\(X_1, X_2, \\dots, X_n\\).\n\\(X_1, X_2, \\dots, X_n\\) come from the same distribution.\n\n\nView \\(X_i\\) as a data point to be drawn from a population with some distribution, say \\(N(\\mu, \\sigma^2)\\).\n\n\n\nPreviously when we talk about probability distributions, we usually assume there is a r.v. that follows some distribution, right? For example, \\(X\\) from binomial, poisson, normal\nNow, let‚Äôs assume we have a bunch of r.v.\n\n\\(n\\) random variables: \\(X_1, X_2, \\dots, X_n\\).\nAssume \\(X_1, X_2, \\dots, X_n\\) follow the same distribution, e.g.,  \\(X_i \\sim N(\\mu, \\sigma^2), i = 1, \\dots, n\\) ."
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#treat-each-data-point-as-a-random-variable-1",
    "href": "slides/08-sampling-dist-clt-slides.html#treat-each-data-point-as-a-random-variable-1",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Treat Each Data Point as a Random Variable",
    "text": "Treat Each Data Point as a Random Variable\n\nAssume that \\(X_1, X_2, \\dots, X_n\\) are independent, i.e., the distribution/value of \\(X_i\\) is not affected by any other \\(X_j\\).\nWith the same distribution, \\(X_1, X_2, \\dots, X_n\\) are independent and identically distributed (i.i.d.):  \\(X_1, X_2, \\dots, X_n \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\) \n\n\\((X_1, X_2, \\dots, X_n)\\) is a random sample of size \\(n\\) from the population.\n\n \\(X_1, X_2, \\dots, X_{50}\\) are randomly selected SAT scores from the SAT score population that follows \\(N(1100, 200^2)\\) \n\n\n\n\n\n\n\nBefore we actually collect the data, the data \\(X_1, X_2, \\dots, X_n\\) are random variables from the population distribution \\(N(\\mu, \\sigma^2)\\).\nOnce we collect the data, we know the realized value of these random variables, \\(x_1, x_2, \\dots, x_n\\).\n\n\n\nAssume \\(X_1, X_2, \\dots, X_n\\) follow the same distribution.\nAssume that \\(X_1, X_2, \\dots, X_n\\) are independent, i.e., the distribution of \\(X_i\\) is not affected by any other \\(X_j\\).\nNow this is where we connect the idea of sample data and population with the random variable and the distribution.\nView \\(X_i\\) as a data point to be drawn from a population with some distribution, say \\(N(\\mu, \\sigma^2)\\).\nWe haven‚Äôt really sample those points yet. Before we actually collect the data, they are random variables, and we don‚Äôt know their value.\nOnce we collect the data, we know the realized value of these random variables.\n\n\\((X_1, X_2, \\dots, X_n)\\) is our random sample data that are sampled or drawn from the population that are assumed normally distributed with mean mu and SD sigma.\n\nWhen we say \\(X\\) follows some distribution, it means that"
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#sampling-distribution-1",
    "href": "slides/08-sampling-dist-clt-slides.html#sampling-distribution-1",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\n\nAny value computed from a sample \\((X_1, X_2, \\dots, X_n)\\) is called a (sample) statistic.\n\n Sample mean \\(\\frac{1}{n}\\sum_{i=1}^n X_i\\) is a statistic. \n\n\n\n\nCan you provide another statistic?\n\n\n\n Sample variance \\(\\frac{\\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^2}{n-1}\\) is a statistic. \n\n\n\n\nSince \\(X_1, X_2, \\dots, X_n\\) are random variables, any transformation or function of \\((X_1, X_2, \\dots, X_n)\\), or statistic, is also a random variable.\nThe probability distribution of a statistic is called the sampling distribution of that statistic.\n\n\n\n\nDoes the sample mean \\(\\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\) have a sampling distribution?"
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#sampling-distribution-2",
    "href": "slides/08-sampling-dist-clt-slides.html#sampling-distribution-2",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\n\nIt is the probability distribution of that statistic if we were to repeatedly draw samples of the same size from the population.\n\n\n\n\n\n\nBiostatistics for the Biological and Health Sciences p.241"
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#sampling-distribution-3",
    "href": "slides/08-sampling-dist-clt-slides.html#sampling-distribution-3",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\n\nSampling Distribution Applet\n\n\nWhat are the differences between the sampling distribution of \\(\\overline{X}\\) and the population distribution each individual r.v. \\(X_i\\) is drawn from?\n\n\n\nSample means \\((\\overline{X})\\) are  less variable  than individual observations \\(X_i\\).\nSample means \\((\\overline{X})\\) are  more normal  than individual observations \\(X_i\\).\n\n\nSampling Distribution Applet"
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#example-sampling-distribution-of-the-sample-mean",
    "href": "slides/08-sampling-dist-clt-slides.html#example-sampling-distribution-of-the-sample-mean",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Example: Sampling Distribution of the Sample Mean",
    "text": "Example: Sampling Distribution of the Sample Mean\n\nRoll a fair die 3 times üé≤üé≤ üé≤ independently to obtain 3 values from the population \\(\\{1, 2, 3, 4, 5, 6\\}\\).\nRepeat the process 10,000 times and plot the histogram of the sampling mean."
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#sampling-distribution-of-sample-mean",
    "href": "slides/08-sampling-dist-clt-slides.html#sampling-distribution-of-sample-mean",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Sampling Distribution of Sample Mean",
    "text": "Sampling Distribution of Sample Mean\n\n\nSuppose \\((X_1, \\dots, X_n)\\) is the random sample from a population distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nThe mean of the sampling distribution of the sample mean, \\(\\overline{X} = \\frac{\\sum_{i=1}^nX_i}{n}\\), is  \\(\\mu_{\\overline{X}} = \\mu\\) .\nThe standard deviation of the sampling distribution of the sample mean \\(\\overline{X}\\) is  \\(\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\) . \nIf the population distribution is  \\(N(\\mu, \\sigma^2)\\) , the sampling distribution of \\(\\overline{X}\\) is exactly  \\(N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\) .\n\n\\(\\sigma\\). - Suppose \\((X_1, \\dots, X_n)\\) is the random sample from ach \\(X_i\\) is sampled from the same population (same \\(\\mu\\) and same \\(\\sigma\\))."
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#sampling-distribution-of-sample-mean-illustration",
    "href": "slides/08-sampling-dist-clt-slides.html#sampling-distribution-of-sample-mean-illustration",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Sampling Distribution of Sample Mean Illustration",
    "text": "Sampling Distribution of Sample Mean Illustration"
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#standardization-of-sample-mean",
    "href": "slides/08-sampling-dist-clt-slides.html#standardization-of-sample-mean",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Standardization of Sample Mean",
    "text": "Standardization of Sample Mean\n\nFor a single random variable \\(X \\sim N(\\mu, \\sigma^2)\\), \\(Z = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\\).\nFor the sample mean of \\(n\\) variables, \\(\\overline{X} \\sim N(\\mu_{\\overline{X}}, \\sigma^2_{\\overline{X}}) = N(\\mu, \\frac{\\sigma^2}{n})\\), and hence\n\n\\[Z = \\frac{\\overline{X} - \\mu_{\\overline{X}}}{\\sigma_{\\overline{X}}} = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)\\]"
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#example---psychomotor-retardation",
    "href": "slides/08-sampling-dist-clt-slides.html#example---psychomotor-retardation",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Example - Psychomotor retardation",
    "text": "Example - Psychomotor retardation\n\n\n\nPsychomotor retardation scores for a group of patients have a normal distribution with a mean of 930 and a standard deviation of 130.\nWhat is the probability that the mean retardation score of a random sample of 20 patients was between 900 and 960?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X_1, \\dots, X_{20}  \\stackrel{iid}{\\sim} N(930, 130^2)\\), then \\(\\overline{X} = \\frac{\\sum_{i=1}^{20}X_i}{20} \\sim N\\left(930, \\frac{130^2}{20} \\right)\\).\n\n\n\n\\[\\small \\begin{align}\nP(900 &lt; \\overline{X} &lt; 960) &= P\\left( \\frac{900-930}{130/\\sqrt{20}} &lt; \\frac{\\overline{X}-930}{130/\\sqrt{20}} &lt; \\frac{960-930}{130/\\sqrt{20}}\\right)=P(-1.03 &lt; Z &lt; 1.03)\\\\\n&=P(Z &lt; 1.03) - P(Z &lt; -1.03)\n  \\end{align}\\]\n\n\n\npnorm(1.03) - pnorm(-1.03)\n\n[1] 0.697\n\n\n\n\n\npnorm(960, mean = 930, sd = 130/sqrt(20)) - pnorm(900, mean = 930, sd = 130/sqrt(20))\n\n[1] 0.698"
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#central-limit-theorem-1",
    "href": "slides/08-sampling-dist-clt-slides.html#central-limit-theorem-1",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nIf \\(X_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\) , then \\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\).\n What if the population distribution is NOT normal? \n\n\n\n\n\n\n\nThe central limit theorem (CLT) gives us the answer!"
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#why-use-normal-central-limit-theorem",
    "href": "slides/08-sampling-dist-clt-slides.html#why-use-normal-central-limit-theorem",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Why Use Normal? Central Limit Theorem\n",
    "text": "Why Use Normal? Central Limit Theorem\n\nCentral Limit Theorem (CLT):\n\nSuppose \\(\\overline{X}\\) is from a random sample of size \\(n\\) and from a population distribution having mean \\(\\mu\\) and standard deviation \\(\\sigma &lt; \\infty\\).  As \\(n\\) increases, the sampling distribution of \\(\\overline{X}\\) looks more and more like \\(N(\\mu, \\sigma^2/n)\\), regardless of the distribution from which we are sampling!\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Central_limit_theorem#/media/File:IllustrationCentralTheorem.png"
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#clt-illustration-a-right-skewed-distribution",
    "href": "slides/08-sampling-dist-clt-slides.html#clt-illustration-a-right-skewed-distribution",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "CLT Illustration: A Right-Skewed Distribution",
    "text": "CLT Illustration: A Right-Skewed Distribution"
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#clt-illustration-a-u-shaped-distribution",
    "href": "slides/08-sampling-dist-clt-slides.html#clt-illustration-a-u-shaped-distribution",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "CLT Illustration: A U-shaped Distribution",
    "text": "CLT Illustration: A U-shaped Distribution"
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#why-clt-is-important",
    "href": "slides/08-sampling-dist-clt-slides.html#why-clt-is-important",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Why CLT is Important?",
    "text": "Why CLT is Important?\n\n\nMany well-developed statistical methods are based on normal distribution assumption.\nWith CLT, we can use those methods even if we are sampling from a non-normal distribution, or we have no idea of the population distribution, provided that the sample size is large."
  },
  {
    "objectID": "slides/08-sampling-dist-clt-slides.html#clt-example",
    "href": "slides/08-sampling-dist-clt-slides.html#clt-example",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "CLT Example",
    "text": "CLT Example\n\n\n\nSuppose that selling prices of houses in Milwaukee are known to have a mean of $382,000 and a standard deviation of $150,000.\nIn 100 randomly selected sales, what is the probability the average selling price is more than $400,000?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince the sample size is fairly large \\((n = 100)\\), by CLT, the sampling distribution of the average selling price is approximately normal with mean 382,000 and SD \\(150,000 / \\sqrt{100}\\).\n\\(P(\\overline{X} &gt; 400000) = P\\left(\\frac{\\overline{X} - 382000}{150000/\\sqrt{100}}  &gt; \\frac{400000 - 382000}{150000/\\sqrt{100}}\\right) \\approx P(Z &gt; 1.2)\\) where \\(Z \\sim N(0, 1)\\).\n\n\npnorm(1.2, lower.tail = FALSE)\n\n[1] 0.115\n\npnorm(400000, mean = 382000, sd = 150000/sqrt(100), lower.tail = FALSE)\n\n[1] 0.115"
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#parameter",
    "href": "slides/08-sampling-dist-clt.html#parameter",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Parameter",
    "text": "Parameter\n\n\nParameters in a probability distribution are the values describing the entire distribution.\n\n Binomial: two parameters \\(n\\) and \\(\\pi\\) \n Poisson: one parameter \\(\\lambda\\) \n Normal: two parameters \\(\\mu\\) and \\(\\sigma\\) \n\n\n\n. . .\n\nIn statistics, we usually assume our target population follows some distribution, but its parameters are unknown to us.\n\n. . .\n\n\n Human weight follows \\(N(\\mu, \\sigma^2)\\) \n\n\n\n\n\n\n\n\n\n # of snowstorms in one year follows \\(Poisson(\\lambda)\\)"
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#treat-each-data-point-as-a-random-variable",
    "href": "slides/08-sampling-dist-clt.html#treat-each-data-point-as-a-random-variable",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Treat Each Data Point as a Random Variable",
    "text": "Treat Each Data Point as a Random Variable\n\n\\(n\\) random variables \\(X_1, X_2, \\dots, X_n\\).\n\\(X_1, X_2, \\dots, X_n\\) come from the same distribution.\n\n\nView \\(X_i\\) as a data point to be drawn from a population with some distribution, say \\(N(\\mu, \\sigma^2)\\).\n\n\n\n\n\n\n\n\n\n\n\nPreviously when we talk about probability distributions, we usually assume there is a r.v. that follows some distribution, right? For example, \\(X\\) from binomial, poisson, normal\nNow, let‚Äôs assume we have a bunch of r.v.\n\n\\(n\\) random variables: \\(X_1, X_2, \\dots, X_n\\).\nAssume \\(X_1, X_2, \\dots, X_n\\) follow the same distribution, e.g.,  \\(X_i \\sim N(\\mu, \\sigma^2), i = 1, \\dots, n\\) ."
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#treat-each-data-point-as-a-random-variable-1",
    "href": "slides/08-sampling-dist-clt.html#treat-each-data-point-as-a-random-variable-1",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Treat Each Data Point as a Random Variable",
    "text": "Treat Each Data Point as a Random Variable\n\nAssume that \\(X_1, X_2, \\dots, X_n\\) are independent, i.e., the distribution/value of \\(X_i\\) is not affected by any other \\(X_j\\).\nWith the same distribution, \\(X_1, X_2, \\dots, X_n\\) are independent and identically distributed (i.i.d.):  \\(X_1, X_2, \\dots, X_n \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\) \n\n\\((X_1, X_2, \\dots, X_n)\\) is a random sample of size \\(n\\) from the population.\n\n \\(X_1, X_2, \\dots, X_{50}\\) are randomly selected SAT scores from the SAT score population that follows \\(N(1100, 200^2)\\) \n\n\n\n. . .\n\n\n\nBefore we actually collect the data, the data \\(X_1, X_2, \\dots, X_n\\) are random variables from the population distribution \\(N(\\mu, \\sigma^2)\\).\nOnce we collect the data, we know the realized value of these random variables, \\(x_1, x_2, \\dots, x_n\\).\n\n\n\n\nAssume \\(X_1, X_2, \\dots, X_n\\) follow the same distribution.\nAssume that \\(X_1, X_2, \\dots, X_n\\) are independent, i.e., the distribution of \\(X_i\\) is not affected by any other \\(X_j\\).\nNow this is where we connect the idea of sample data and population with the random variable and the distribution.\nView \\(X_i\\) as a data point to be drawn from a population with some distribution, say \\(N(\\mu, \\sigma^2)\\).\nWe haven‚Äôt really sample those points yet. Before we actually collect the data, they are random variables, and we don‚Äôt know their value.\nOnce we collect the data, we know the realized value of these random variables.\n\n\\((X_1, X_2, \\dots, X_n)\\) is our random sample data that are sampled or drawn from the population that are assumed normally distributed with mean mu and SD sigma.\n\nWhen we say \\(X\\) follows some distribution, it means that"
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#sampling-distribution-1",
    "href": "slides/08-sampling-dist-clt.html#sampling-distribution-1",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\n\nAny value computed from a sample \\((X_1, X_2, \\dots, X_n)\\) is called a (sample) statistic.\n\n Sample mean \\(\\frac{1}{n}\\sum_{i=1}^n X_i\\) is a statistic. \n\n\n\n\nCan you provide another statistic?\n\n. . .\n\n Sample variance \\(\\frac{\\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^2}{n-1}\\) is a statistic. \n\n. . .\n\nSince \\(X_1, X_2, \\dots, X_n\\) are random variables, any transformation or function of \\((X_1, X_2, \\dots, X_n)\\), or statistic, is also a random variable.\nThe probability distribution of a statistic is called the sampling distribution of that statistic.\n\n. . .\n\nDoes the sample mean \\(\\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\) have a sampling distribution?"
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#sampling-distribution-2",
    "href": "slides/08-sampling-dist-clt.html#sampling-distribution-2",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\n\nIt is the probability distribution of that statistic if we were to repeatedly draw samples of the same size from the population.\n\n\n\n\n\n\nBiostatistics for the Biological and Health Sciences p.241"
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#sampling-distribution-3",
    "href": "slides/08-sampling-dist-clt.html#sampling-distribution-3",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\n\nSampling Distribution Applet\n\n\nWhat are the differences between the sampling distribution of \\(\\overline{X}\\) and the population distribution each individual r.v. \\(X_i\\) is drawn from?\n\n. . .\n\nSample means \\((\\overline{X})\\) are  less variable  than individual observations \\(X_i\\).\nSample means \\((\\overline{X})\\) are  more normal  than individual observations \\(X_i\\).\n\n\n\nSampling Distribution Applet"
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#example-sampling-distribution-of-the-sample-mean",
    "href": "slides/08-sampling-dist-clt.html#example-sampling-distribution-of-the-sample-mean",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Example: Sampling Distribution of the Sample Mean",
    "text": "Example: Sampling Distribution of the Sample Mean\n\nRoll a fair die 3 times üé≤üé≤ üé≤ independently to obtain 3 values from the population \\(\\{1, 2, 3, 4, 5, 6\\}\\).\nRepeat the process 10,000 times and plot the histogram of the sampling mean."
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#sampling-distribution-of-sample-mean",
    "href": "slides/08-sampling-dist-clt.html#sampling-distribution-of-sample-mean",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Sampling Distribution of Sample Mean",
    "text": "Sampling Distribution of Sample Mean\n\n\nSuppose \\((X_1, \\dots, X_n)\\) is the random sample from a population distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nThe mean of the sampling distribution of the sample mean, \\(\\overline{X} = \\frac{\\sum_{i=1}^nX_i}{n}\\), is  \\(\\mu_{\\overline{X}} = \\mu\\) .\nThe standard deviation of the sampling distribution of the sample mean \\(\\overline{X}\\) is  \\(\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\) . \nIf the population distribution is  \\(N(\\mu, \\sigma^2)\\) , the sampling distribution of \\(\\overline{X}\\) is exactly  \\(N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\) .\n\n\n\\(\\sigma\\). - Suppose \\((X_1, \\dots, X_n)\\) is the random sample from ach \\(X_i\\) is sampled from the same population (same \\(\\mu\\) and same \\(\\sigma\\))."
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#sampling-distribution-of-sample-mean-illustration",
    "href": "slides/08-sampling-dist-clt.html#sampling-distribution-of-sample-mean-illustration",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Sampling Distribution of Sample Mean Illustration",
    "text": "Sampling Distribution of Sample Mean Illustration"
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#standardization-of-sample-mean",
    "href": "slides/08-sampling-dist-clt.html#standardization-of-sample-mean",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Standardization of Sample Mean",
    "text": "Standardization of Sample Mean\n\nFor a single random variable \\(X \\sim N(\\mu, \\sigma^2)\\), \\(Z = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\\).\nFor the sample mean of \\(n\\) variables, \\(\\overline{X} \\sim N(\\mu_{\\overline{X}}, \\sigma^2_{\\overline{X}}) = N(\\mu, \\frac{\\sigma^2}{n})\\), and hence\n\n\\[Z = \\frac{\\overline{X} - \\mu_{\\overline{X}}}{\\sigma_{\\overline{X}}} = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)\\]"
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#example---psychomotor-retardation",
    "href": "slides/08-sampling-dist-clt.html#example---psychomotor-retardation",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Example - Psychomotor retardation",
    "text": "Example - Psychomotor retardation\n\n\n\nPsychomotor retardation scores for a group of patients have a normal distribution with a mean of 930 and a standard deviation of 130.\nWhat is the probability that the mean retardation score of a random sample of 20 patients was between 900 and 960?\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\n\n\\(X_1, \\dots, X_{20}  \\stackrel{iid}{\\sim} N(930, 130^2)\\), then \\(\\overline{X} = \\frac{\\sum_{i=1}^{20}X_i}{20} \\sim N\\left(930, \\frac{130^2}{20} \\right)\\).\n\n. . .\n\\[\\small \\begin{align}\nP(900 &lt; \\overline{X} &lt; 960) &= P\\left( \\frac{900-930}{130/\\sqrt{20}} &lt; \\frac{\\overline{X}-930}{130/\\sqrt{20}} &lt; \\frac{960-930}{130/\\sqrt{20}}\\right)=P(-1.03 &lt; Z &lt; 1.03)\\\\\n&=P(Z &lt; 1.03) - P(Z &lt; -1.03)\n  \\end{align}\\]\n. . .\n\npnorm(1.03) - pnorm(-1.03)\n\n[1] 0.697\n\n\n. . .\n\npnorm(960, mean = 930, sd = 130/sqrt(20)) - pnorm(900, mean = 930, sd = 130/sqrt(20))\n\n[1] 0.698"
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#central-limit-theorem-1",
    "href": "slides/08-sampling-dist-clt.html#central-limit-theorem-1",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nIf \\(X_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\) , then \\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\).\n What if the population distribution is NOT normal? \n\n. . .\n\n\n\n\n\nThe central limit theorem (CLT) gives us the answer!"
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#why-use-normal-central-limit-theorem",
    "href": "slides/08-sampling-dist-clt.html#why-use-normal-central-limit-theorem",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Why Use Normal? Central Limit Theorem\n",
    "text": "Why Use Normal? Central Limit Theorem\n\nCentral Limit Theorem (CLT):\n\nSuppose \\(\\overline{X}\\) is from a random sample of size \\(n\\) and from a population distribution having mean \\(\\mu\\) and standard deviation \\(\\sigma &lt; \\infty\\).  As \\(n\\) increases, the sampling distribution of \\(\\overline{X}\\) looks more and more like \\(N(\\mu, \\sigma^2/n)\\), regardless of the distribution from which we are sampling!\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Central_limit_theorem#/media/File:IllustrationCentralTheorem.png"
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#clt-illustration-a-right-skewed-distribution",
    "href": "slides/08-sampling-dist-clt.html#clt-illustration-a-right-skewed-distribution",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "CLT Illustration: A Right-Skewed Distribution",
    "text": "CLT Illustration: A Right-Skewed Distribution"
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#clt-illustration-a-u-shaped-distribution",
    "href": "slides/08-sampling-dist-clt.html#clt-illustration-a-u-shaped-distribution",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "CLT Illustration: A U-shaped Distribution",
    "text": "CLT Illustration: A U-shaped Distribution"
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#why-clt-is-important",
    "href": "slides/08-sampling-dist-clt.html#why-clt-is-important",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "Why CLT is Important?",
    "text": "Why CLT is Important?\n\n\nMany well-developed statistical methods are based on normal distribution assumption.\nWith CLT, we can use those methods even if we are sampling from a non-normal distribution, or we have no idea of the population distribution, provided that the sample size is large."
  },
  {
    "objectID": "slides/08-sampling-dist-clt.html#clt-example",
    "href": "slides/08-sampling-dist-clt.html#clt-example",
    "title": "Sampling Distribution and Central Limit Theorem \n",
    "section": "CLT Example",
    "text": "CLT Example\n\n\n\nSuppose that selling prices of houses in Milwaukee are known to have a mean of $382,000 and a standard deviation of $150,000.\nIn 100 randomly selected sales, what is the probability the average selling price is more than $400,000?\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\n\nSince the sample size is fairly large \\((n = 100)\\), by CLT, the sampling distribution of the average selling price is approximately normal with mean 382,000 and SD \\(150,000 / \\sqrt{100}\\).\n\\(P(\\overline{X} &gt; 400000) = P\\left(\\frac{\\overline{X} - 382000}{150000/\\sqrt{100}}  &gt; \\frac{400000 - 382000}{150000/\\sqrt{100}}\\right) \\approx P(Z &gt; 1.2)\\) where \\(Z \\sim N(0, 1)\\).\n\n\npnorm(1.2, lower.tail = FALSE)\n\n[1] 0.115\n\npnorm(400000, mean = 382000, sd = 150000/sqrt(100), lower.tail = FALSE)\n\n[1] 0.115"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#one-categorical-variable-with-two-categories",
    "href": "slides/16-infer-cat-slides.html#one-categorical-variable-with-two-categories",
    "title": "Inference for Categorical Data \n",
    "section": "One Categorical Variable with Two Categories",
    "text": "One Categorical Variable with Two Categories\n\nA categorical variable Gender having 2 categories Male and Female.\n\n\n\n\n\n\n\n\n\n\n\n\nSubject\nGender\n\n\n\n1\nMale\n\n\n2\nFemale\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\nFemale\n\n\n\n\n\n\nOne-way frequency/count table\n\n\n\\(X\\)\nCount\n\n\n\nMale\n\\(y\\)\n\n\nFemale\n\\(n-y\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe sample male proportion is \\(y/n\\).\nGoal: estimate or test gender ratio or the male population proportion"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#probability-distribution-for-count-data-two-levels",
    "href": "slides/16-infer-cat-slides.html#probability-distribution-for-count-data-two-levels",
    "title": "Inference for Categorical Data \n",
    "section": "Probability Distribution for Count Data: Two Levels",
    "text": "Probability Distribution for Count Data: Two Levels\n\nThe number of Male can be viewed as a random variable because the count \\(y\\) varies from sample to sample.\n\n\nWhat probability distribution might be appropriate for the count \\(Y\\)?\n\n\n\n\n\\(binomial(n, \\pi)\\) could be a good option for the count data with 2 categories.\n\nFixed number of trials.  (Fixed \\(n\\) subjects) \nEach trial results in one of two outcomes.  (Either \\(M\\) or \\(F\\)) \nTrials are independent.  (If sample subjects randomly) \n\n\n\n\n\n\nIf the proportion of being in category \\(M\\) is \\(\\pi\\), the count \\(Y\\) has \\[P(Y = y \\mid n, \\pi) = \\frac{n!}{y!(n-y)!}\\pi^{y}(1-\\pi)^{n-y}\\]\nOur job is to do inference about \\(\\pi\\).\n\n\nor in general the count \\((Y_M, Y_F)\\) has the probability function \\[P(Y_M = y, Y_F = n-y \\mid n, \\pi) = \\frac{n!}{y!(n-y)!}\\pi^{y}(1-\\pi)^{n-y}\\]\n\nOne of our goals in categorical data analysis is to estimate or test population proportions of categories, here \\(M\\) and \\(F\\), or \\((\\pi, 1-\\pi)\\)."
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#inference-about-a-single-population-proportion",
    "href": "slides/16-infer-cat-slides.html#inference-about-a-single-population-proportion",
    "title": "Inference for Categorical Data \n",
    "section": "Inference About a Single Population Proportion",
    "text": "Inference About a Single Population Proportion\n\n\n\nExample: Exit Poll Suppose we collected data on 1,000 voters in an election with only two candidates: R and D.\n\n\n\n\n\n\nVoter\nR\nD\n\n\n\n1\nx\n\n\n\n2\n\nx\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n1000\nx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the data, we want to predict who won the election."
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#poll-example-contd",
    "href": "slides/16-infer-cat-slides.html#poll-example-contd",
    "title": "Inference for Categorical Data \n",
    "section": "Poll Example Cont‚Äôd",
    "text": "Poll Example Cont‚Äôd\n\n\n\nLet \\(Y\\) be the number of voters voted for R.\nAssume \\(Y \\sim binomial(n = 1000, \\pi)\\).\n\\(\\pi = P(\\text{a voter voted for R}) =\\) population proportion of all voters voted for R: A unknown parameter to be estimated or tested.\nPredict whether or not R won the election.\n\n\n\n\n\nVoter\nR\nD\n\n\n\n1\nx\n\n\n\n2\n\nx\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n1000\nx\n\n\n\n\n\n\nCandidate\nCount\nProportion\n\n\n\nR\n\\(y\\)\n\\(\\hat{\\pi} = y/n\\)\n\n\nD\n\\(n-y\\)\n\\(1-\\hat{\\pi} = 1- \\frac{y}{n}\\)\n\n\n\n\n\n\nWhat are \\(H_0\\) and \\(H_1\\)?\n\n\n\n \\(\\begin{align} &H_0: \\pi \\le 1/2 \\\\ &H_1: \\pi &gt; 1/2 \\text{ (more than half voted for R)} \\end{align}\\)"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#hypothesis-testing-for-pi",
    "href": "slides/16-infer-cat-slides.html#hypothesis-testing-for-pi",
    "title": "Inference for Categorical Data \n",
    "section": "Hypothesis Testing for \\(\\pi\\)\n",
    "text": "Hypothesis Testing for \\(\\pi\\)\n\n\n\nStep 0: Assumptions\n\n \\(n\\pi_0 \\ge 5\\) and \\(n(1-\\pi_0) \\ge 5\\) (the larger, the better) \n\n\n\nStep 1: Null and Alternative Hypothesis\n\n \\(\\begin{align} &H_0: \\pi = \\pi_0 \\\\ &H_1: \\pi &gt; \\pi_0 \\text{ or } \\pi &lt; \\pi_0 \\text{ or } \\pi \\ne \\pi_0 \\end{align}\\) \n\n\n\nStep 2: Set \\(\\alpha\\)\n\n\n\nStep 3: Test Statistic\n\n Under \\(H_0\\), \\(z_{test} = \\dfrac{\\hat{\\pi} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}}\\) where \\(\\hat{\\pi} = \\frac{y}{n} =\\) sample proportion \n\n\n\n\n\nThe sampling distribution of \\(\\hat{\\pi}\\) is approximately normal with mean \\(\\pi\\) and standard error \\(\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\) if \\(y_i\\) are independent and the assumptions are satisfied."
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#hypothesis-testing-for-pi-1",
    "href": "slides/16-infer-cat-slides.html#hypothesis-testing-for-pi-1",
    "title": "Inference for Categorical Data \n",
    "section": "Hypothesis Testing for \\(\\pi\\)\n",
    "text": "Hypothesis Testing for \\(\\pi\\)\n\n\nStep 4-c: Critical Value \\(z_{\\alpha}\\) (one-tailed) or \\(z_{\\alpha/2}\\) (two-tailed)\n\nStep 5-c: Draw a Conclusion Using Critical Value Method \n\n \\(H_1: \\pi &gt; \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z &gt; z_{\\alpha}\\) \n \\(H_1: \\pi &lt; \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z &lt; -z_{\\alpha}\\) \n \\(H_1: \\pi \\ne \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(|z| &gt; z_{\\alpha/2}\\) \n\n\nStep 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#poll-example-contd-hypothesis-testing",
    "href": "slides/16-infer-cat-slides.html#poll-example-contd-hypothesis-testing",
    "title": "Inference for Categorical Data \n",
    "section": "Poll Example Cont‚Äôd (Hypothesis Testing)",
    "text": "Poll Example Cont‚Äôd (Hypothesis Testing)\n\nIn an exit poll of 1,000 voters, 520 voted for R, one of the two candidates.\nStep 0: \\(n\\pi_0 = 1000(1/2) = 500 \\ge 5\\) and \\(n(1-\\pi_0) \\ge 5\\) \nStep 1:  \\(\\begin{align} &H_0: \\pi \\le 1/2 \\\\ &H_1: \\pi &gt; 1/2 \\end{align}\\) \nStep 2:  \\(\\alpha = 0.05\\) \nStep 3:  \\(z_{test} = \\frac{\\hat{\\pi} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}} =  \\frac{\\frac{520}{1000} - 0.5}{\\sqrt{\\frac{0.5(1-0.5)}{1000}}} = 1.26\\) \n\n\n\nStep 4:  \\(z_{\\alpha} = z_{0.05} = 1.645\\) \nStep 5:  Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z_{test} &gt; z_{\\alpha}\\). Since \\(z_{test} &lt; z_{\\alpha}\\), we do not reject \\(H_0\\). \nStep 6:  We do not have sufficient evidence to conclude that R won. \nWe make the same conclusion based on \\(p\\)-value.\n\n\\[ p\\text{-value} = P(Z &gt; 1.26) = 0.1 &gt; 0.05\\]\nIs there a sufficient evidence to conclude at \\(\\alpha = 0.05\\) that R won the election?"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#confidence-interval-for-pi",
    "href": "slides/16-infer-cat-slides.html#confidence-interval-for-pi",
    "title": "Inference for Categorical Data \n",
    "section": "Confidence Interval for \\(\\pi\\)\n",
    "text": "Confidence Interval for \\(\\pi\\)\n\n\nAssumptions: \\(n\\hat{\\pi} \\ge 5\\) and \\(n(1-\\hat{\\pi}) \\ge 5\\)\nA \\(100(1 - \\alpha)\\%\\) confidence interval for \\(\\pi\\) is \\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\] where \\(\\hat{\\pi} = y/n\\).\n\n\n\n\n\\(\\pi\\) is unknown and use the estimate \\(\\hat{\\pi}\\) instead: \\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}}\\]\n\n\n\nüëâ No hypothesized value \\(\\pi_0\\) in the confidence interval."
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#poll-example-contd-confidence-interval",
    "href": "slides/16-infer-cat-slides.html#poll-example-contd-confidence-interval",
    "title": "Inference for Categorical Data \n",
    "section": "Poll Example Cont‚Äôd (Confidence Interval)",
    "text": "Poll Example Cont‚Äôd (Confidence Interval)\n\nAssumption: \\(n\\hat{\\pi} = 1000(0.52) = 520 \\ge 5\\) and \\(n(1-\\hat{\\pi}) = 480 \\ge 5\\).\nEstimate the proportion of all voters voted for R using 95% confidence interval:\n\n\\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}} = 0.52 \\pm z_{0.025}\\sqrt{\\frac{0.52(1-0.52)}{1000}} = (0.49, 0.55).\\]\n\n\n\n# Use alternative = \"two.sided\" to get CI\nprop.test(x = 520, n = 1000, p = 0.5, alternative = \"greater\", correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  520 out of 1000, null probability 0.5\nX-squared = 2, df = 1, p-value = 0.1\nalternative hypothesis: true p is greater than 0.5\n95 percent confidence interval:\n 0.494 1.000\nsample estimates:\n   p \n0.52"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#inference-about-two-population-proportions",
    "href": "slides/16-infer-cat-slides.html#inference-about-two-population-proportions",
    "title": "Inference for Categorical Data \n",
    "section": "Inference About Two Population Proportions",
    "text": "Inference About Two Population Proportions\n\n\n\n\n\n\n\nGroup 1 (M)\nGroup 2 (F)\n\n\n\ntrials\n\\(n_1\\)\n\\(n_2\\)\n\n\nnumber of successes\n\\(Y_1\\)\n\\(Y_2\\)\n\n\ndistribution\n\\(binomial(n_1, \\pi_1)\\)\n\\(binomial(n_2, \\pi_2)\\)\n\n\n\n\n\n\\(\\pi_1\\): Population proportion of success of Group 1\n\n\\(\\pi_2\\): Population proportion of success of Group 2\n\n\n\n\n\nVoter\nGender\nApprove\n\n\n\n1\nM\nYes\n\n\n2\nF\nYes\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n1000\nF\nNo\n\n\n\n\n\n\nIs male‚Äôs president approval rate \\(\\pi_1\\) higher than the female‚Äôs approval rate \\(\\pi_2\\)?\n\n\n\n\n\n\n\n\n\n\n\nComparing two population proportions \\(\\pi_1\\) and \\(\\pi_2\\)."
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#hypothesis-testing-for-pi_1-and-pi_2",
    "href": "slides/16-infer-cat-slides.html#hypothesis-testing-for-pi_1-and-pi_2",
    "title": "Inference for Categorical Data \n",
    "section": "Hypothesis Testing for \\(\\pi_1\\) and \\(\\pi_2\\)\n",
    "text": "Hypothesis Testing for \\(\\pi_1\\) and \\(\\pi_2\\)\n\n\n\nStep 0: Assumptions\n\n \\(n_1\\hat{\\pi}_1 \\ge 5\\), \\(n_1(1-\\hat{\\pi}_1) \\ge 5\\) and \\(n_2\\hat{\\pi}_2 \\ge 5\\), \\(n_2(1-\\hat{\\pi}_2) \\ge 5\\) \n\n\n\nStep 1: Null and Alternative Hypothesis\n\n \\(\\begin{align}  &H_0: \\pi_1 = \\pi_2 \\\\ &H_1: \\pi_1 &gt; \\pi_2 \\text{ or } \\pi_1 &lt; \\pi_2 \\text{ or } \\pi_1 \\ne \\pi_2 \\end{align}\\) \n\n\nStep 2: Set \\(\\alpha\\)\n\nStep 3: Test Statistic\n\n \\(z_{test} = \\dfrac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\bar{\\pi}(1-\\bar{\\pi})(\\frac{1}{n_1} + \\frac{1}{n_2}})}\\), \\(\\bar{\\pi} = \\frac{y_1+y_2}{n_1+n_2}\\) is the pooled sample proportion estimating \\(\\pi\\) \n\n\n\n\\(z_{test} = \\dfrac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\frac{\\bar{\\pi}(1-\\bar{\\pi})}{n_1} + \\frac{\\bar{\\pi}(1-\\bar{\\pi})}{n_2}}} = \\dfrac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\bar{\\pi}(1-\\bar{\\pi})(\\frac{1}{n_1} + \\frac{1}{n_2}})}\\), \\(\\bar{\\pi} = \\frac{y_1+y_2}{n_1+n_2}\\) is the pooled sample proportion that is an estimate of \\(\\pi\\)"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#hypothesis-testing-for-pi_1-and-pi_2-1",
    "href": "slides/16-infer-cat-slides.html#hypothesis-testing-for-pi_1-and-pi_2-1",
    "title": "Inference for Categorical Data \n",
    "section": "Hypothesis Testing for \\(\\pi_1\\) and \\(\\pi_2\\)\n",
    "text": "Hypothesis Testing for \\(\\pi_1\\) and \\(\\pi_2\\)\n\n\nStep 4-c: Find the Critical Value \\(z_{\\alpha}\\) (one-tailed) or \\(z_{\\alpha/2}\\) (two-tailed)\n\nStep 5-c: Draw a Conclusion Using Critical Value Method\n\n\n Reject \\(H_0\\) in favor of \\(H_1\\) if \n\n \\(H_1: \\pi_1 &gt; \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z &gt; z_{\\alpha}\\) \n \\(H_1: \\pi_1 &lt; \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z &lt; -z_{\\alpha}\\) \n \\(H_1: \\pi_1 \\ne \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(|z| &gt; z_{\\alpha/2}\\) \n\n\n\n\nStep 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#example-effectiveness-of-learning",
    "href": "slides/16-infer-cat-slides.html#example-effectiveness-of-learning",
    "title": "Inference for Categorical Data \n",
    "section": "Example: Effectiveness of Learning",
    "text": "Example: Effectiveness of Learning\n\n\nA study on 300 students to compare the effectiveness of learning Statistics via online and in-person classes.\n\nRandomly assign\n\n125 students to the online program\nthe remaining 175 to the in-person program.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExam Results\nOnline Instruction\nIn-Person Instruction\n\n\n\nPass\n94\n113\n\n\nFail\n31\n62\n\n\nTotal\n125\n175\n\n\n\n\nIs there sufficient evidence to conclude that the online program is more effective than the traditional in-person program at \\(\\alpha=0.05\\)?"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#example-effectiveness-of-learning-contd-testing",
    "href": "slides/16-infer-cat-slides.html#example-effectiveness-of-learning-contd-testing",
    "title": "Inference for Categorical Data \n",
    "section": "Example: Effectiveness of Learning Cont‚Äôd (Testing)",
    "text": "Example: Effectiveness of Learning Cont‚Äôd (Testing)\n\nStep 1:  \\(H_0: \\pi_1 = \\pi_2\\) vs.¬†\\(H_1: \\pi_1 &gt; \\pi_2\\) \\(\\pi_1\\) \\((\\pi_2)\\) is the population proportion of students passing the exam under the online (in-person) program.\n\n\n\nStep 0: \\(\\hat{\\pi}_1 = 94/125 = 0.75\\) and \\(\\hat{\\pi}_2 = 113/175 = 0.65\\).\\(n_1\\hat{\\pi}_1 = 94 &gt; 5\\), \\(n_1(1-\\hat{\\pi}_1) = 31 &gt; 5\\), and \\(n_2\\hat{\\pi}_2 = 113 &gt; 5\\), \\(n_2(1-\\hat{\\pi}_2) = 62 &gt; 5\\) \n\n\n\n\n\nStep 2:  \\(\\alpha = 0.05\\) \n\n\n\n\n\nStep 3:  \\(\\bar{\\pi} = \\frac{94+113}{125+175} = 0.69\\). \\(z_{test} = \\dfrac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\bar{\\pi}(1-\\bar{\\pi})(\\frac{1}{n_1} + \\frac{1}{n_2}})} = \\frac{0.75 - 0.65}{\\sqrt{0.69(1-0.69)(\\frac{1}{125} + \\frac{1}{175})}} = 1.96\\) \n\n\n\n\n\nStep 4:  \\(z_{\\alpha} = z_{0.05} = 1.645\\) \n\n\n\n\n\nStep 5:  Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z_{test} &gt; z_{\\alpha}\\). Since \\(z_{test} &gt; z_{\\alpha}\\), we reject \\(H_0\\). \n\n\n\n\n\nStep 6:  We have sufficient evidence to conclude that the online program is more effective."
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#confidence-interval-for-pi_1---pi_2",
    "href": "slides/16-infer-cat-slides.html#confidence-interval-for-pi_1---pi_2",
    "title": "Inference for Categorical Data \n",
    "section": "Confidence Interval for \\(\\pi_1 - \\pi_2\\)\n",
    "text": "Confidence Interval for \\(\\pi_1 - \\pi_2\\)\n\n\nA \\(100(1 - \\alpha)\\%\\) confidence interval for \\(\\pi_1 - \\pi_2\\) is\n\n\\[\\hat{\\pi}_1 - \\hat{\\pi}_2 \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}_1(1-\\hat{\\pi}_1)}{n_1}+\\frac{\\hat{\\pi}_2(1-\\hat{\\pi}_2)}{n_2}}\\]\n\nRequirements: \\(n_1\\hat{\\pi}_1 \\ge 5\\), \\(n_1(1-\\hat{\\pi}_1) \\ge 5\\) and \\(n_2\\hat{\\pi}_2 \\ge 5\\), \\(n_2(1-\\hat{\\pi}_2) \\ge 5\\)"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#example-effectiveness-of-learning-contd-ci",
    "href": "slides/16-infer-cat-slides.html#example-effectiveness-of-learning-contd-ci",
    "title": "Inference for Categorical Data \n",
    "section": "Example: Effectiveness of Learning Cont‚Äôd (CI)",
    "text": "Example: Effectiveness of Learning Cont‚Äôd (CI)\n\nWant to know how much effective is the online program.\nEstimate \\(\\pi_1 - \\pi_2\\) using a \\(95\\%\\) confidence interval:\n\n\\[\\hat{\\pi}_1 - \\hat{\\pi}_2 \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}_1(1-\\hat{\\pi}_1)}{n_1}+\\frac{\\hat{\\pi}_2(1-\\hat{\\pi}_2)}{n_2}}\\]\n\n\\(z_{0.05/2} = 1.96\\)\nThe 95% confidence interval is\n\n\\[0.75 - 0.65 \\pm 1.96\\sqrt{\\frac{(0.75)(1-0.75)}{125} + \\frac{(0.65)(1-0.65)}{175}}\\\\\n= (0.002, 0.210)\\]"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#implementation-in-r",
    "href": "slides/16-infer-cat-slides.html#implementation-in-r",
    "title": "Inference for Categorical Data \n",
    "section": "Implementation in R",
    "text": "Implementation in R\n\n\n# Use alternative = \"two.sided\" to get CI\nprop.test(x = c(94, 113), n = c(125, 175), alternative = \"greater\", correct = FALSE)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  c(94, 113) out of c(125, 175)\nX-squared = 4, df = 1, p-value = 0.02\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.0193 1.0000\nsample estimates:\nprop 1 prop 2 \n 0.752  0.646 \n\n\n\nprop_ci &lt;- prop.test(x = c(94, 113), n = c(125, 175), alternative = \"two.sided\", correct = FALSE)\nprop_ci$conf.int\n\n[1] 0.00259 0.20998\nattr(,\"conf.level\")\n[1] 0.95"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#categorical-variable-with-more-than-2-categories",
    "href": "slides/16-infer-cat-slides.html#categorical-variable-with-more-than-2-categories",
    "title": "Inference for Categorical Data \n",
    "section": "Categorical Variable with More Than 2 Categories",
    "text": "Categorical Variable with More Than 2 Categories\n\n\n\nA categorical variable has \\(k\\) categories \\(A_1, \\dots, A_k\\).\n\n\n\n\n\n\n\n\n\n\n\nSubject\nVariable\n\n\n\n1\n\\(A_2\\)\n\n\n2\n\\(A_4\\)\n\n\n3\n\\(A_1\\)\n\n\n4\n\\(A_3\\)\n\n\n5\n\\(A_k\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(A_3\\)\n\n\n\n\n\n\nWith the size \\(n\\), for categories \\(A_1, \\dots , A_k\\), their observed count is \\(O_1, \\dots, O_k\\), and \\(\\sum_{i=1}^kO_i = n\\).\nOne-way count table:\n\n\n\n\n\\(A_1\\)\n\\(A_2\\)\n\\(\\cdots\\)\n\\(A_k\\)\nTotal\n\n\n\\(O_1\\)\n\\(O_2\\)\n\\(\\cdots\\)\n\\(O_k\\)\n\\(n\\)"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#example-more-than-2-categories",
    "href": "slides/16-infer-cat-slides.html#example-more-than-2-categories",
    "title": "Inference for Categorical Data \n",
    "section": "Example: More Than 2 Categories",
    "text": "Example: More Than 2 Categories\n\n\nAre the selected jurors are racially representative of the population?\n\n\nIdea: If the jury is representative of the population, the proportions in the sample should reflect the population of eligible jurors, i.e., registered voters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\nRepresentation in juries\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\\(n = 275\\)\n\n\nRegistered voters\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\\(1.00\\)"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#example-more-than-2-categories-1",
    "href": "slides/16-infer-cat-slides.html#example-more-than-2-categories-1",
    "title": "Inference for Categorical Data \n",
    "section": "Example: More Than 2 Categories",
    "text": "Example: More Than 2 Categories\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\nRepresentation in juries\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\\(n = 275\\)\n\n\nRegistered voters\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\\(1.00\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\nRepresentation in juries\n\\(O_1/n = 0.75\\)\n\\(O_2/n = 0.09\\)\n\\(O_3/n = 0.09\\)\n\\(O_4/n = 0.07\\)\n\\(1.00\\)\n\n\nRegistered voters\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\\(1.00\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile the proportions in the juries do not precisely represent the population proportions, it is unclear whether these data provide convincing evidence that the sample is not representative. If the jurors really were randomly sampled from the registered voters, we might expect small differences due to chance. However, unusually large differences may provide convincing evidence that the juries were not representative."
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#example-more-than-2-categories-2",
    "href": "slides/16-infer-cat-slides.html#example-more-than-2-categories-2",
    "title": "Inference for Categorical Data \n",
    "section": "Example: More Than 2 Categories",
    "text": "Example: More Than 2 Categories\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\nRepresentation in juries\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\\(n = 275\\)\n\n\nRegistered voters\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\\(1.00\\)\n\n\n\n\nIf the individuals are randomly selected to serve on a jury, about how many of the 275 people would we expect to be white? How about Hispanic?\n\n\n\nAbout \\(72\\%\\) of the population is white, so we would expect about \\(72\\%\\) of the jurors to be white: \\(0.72 \\times 275 = 198\\).\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\nRepresentation in juries\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\n\nHypothesized expected number\n\\(275 \\times \\pi_1^0 = 198\\)\n\\(275 \\times \\pi_2^0 = 19.25\\)\n\\(275 \\times \\pi_3^0 = 33\\)\n\\(275 \\times \\pi_4^0 = 24.75\\)"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#goodness-of-fit-test",
    "href": "slides/16-infer-cat-slides.html#goodness-of-fit-test",
    "title": "Inference for Categorical Data \n",
    "section": "Goodness-of-Fit Test",
    "text": "Goodness-of-Fit Test\n\nAre the proportions (counts) of juries close enough to the proportions (counts) of registered voters, so that we are confident saying that the jurors really were randomly sampled from the registered voters?\n\n\nA goodness-of-fit test tests the hypothesis that the observed frequency distribution fits or conforms to some claim distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\nObserved Count\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\\(n = 275\\)\n\n\nExpected Count\n\\(E_1 = 198\\)\n\\(E_2 = 19.25\\)\n\\(E_3 = 33\\)\n\\(E_4 = 24.75\\)\n\\(n = 275\\)\n\n\nProportion under \\(H_0\\)\n\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\\(1.00\\)\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nObserved Count\n205\n26\n25\n19\n\n\n\nExpected Count\n198\n19.25\n33\n24.75\n\n\nPopulation Proportion\n0.72\n0.07\n0.12\n0.09\n\n\n\n\nGiven a sample of cases that can be classified into several (more than 2) groups, determine if the sample is representative of the general population.\nEvaluate whether data resemble a particular distribution.\nThe sample proportion represented from each race among the 275 jurors was not a precise match for any ethnic group. While some sampling variation is expected, we would expect the sample proportions to be fairly similar to the population proportions if there is no bias on juries.\nWe need to test whether the differences are strong enough to provide convincing evidence that the jurors are not a random sample."
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#goodness-of-fit-test-1",
    "href": "slides/16-infer-cat-slides.html#goodness-of-fit-test-1",
    "title": "Inference for Categorical Data \n",
    "section": "Goodness-of-Fit Test",
    "text": "Goodness-of-Fit Test\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\n\nObserved Count\n205\n26\n25\n19\n275\n\n\n\nExpected Count\n198\n19.25\n33\n24.75\n275\n\n\nPopulation Proportion \\((H_0)\\)\n\n0.72\n0.07\n0.12\n0.09\n1.00\n\n\n\n\nObserved Count and Expected Count are similar if no bias on juries.\nTest whether the differences are strong enough to provide convincing evidence that the jurors are not a random sample of registered voters.\n\n \\(\\begin{align} &H_0: \\text{No racial bias in who serves on a jury, and } \\\\ &H_1: \\text{There is racial bias in juror selection} \\end{align}\\) \n\n \\(\\begin{align} &H_0: \\pi_1 = \\pi_1^0,  \\pi_2 = \\pi_2^0, \\dots, \\pi_k = \\pi_k^0\\\\ &H_1: \\pi_i \\ne \\pi_i^0 \\text{ for some } i \\end{align}\\) \n\nWhile some sampling variation is expected, we would expect the sample proportions to be similar to the population proportions if there is no bias on juries.\nWe need to test whether the differences are strong enough to provide convincing evidence that the jurors are not a random sample of registered voters.\n\\(\\quad\\text{the observed counts reflect natural sampling fluctuation.}\\)"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#goodness-of-fit-test-statistic",
    "href": "slides/16-infer-cat-slides.html#goodness-of-fit-test-statistic",
    "title": "Inference for Categorical Data \n",
    "section": "Goodness-of-Fit Test Statistic",
    "text": "Goodness-of-Fit Test Statistic\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\nObserved Count\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\\(n = 275\\)\n\n\nExpected Count\n\\(E_1 = 198\\)\n\\(E_2 = 19.25\\)\n\\(E_3 = 33\\)\n\\(E_4 = 24.75\\)\n\\(n = 275\\)\n\n\nProportion under \\(H_0\\)\n\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\\(1.00\\)\n\n\n\n\n Under \\(H_0\\), \\(\\chi^2_{test} = \\frac{(O_1 - E_1)^2}{E_1} + \\frac{(O_2 - E_2)^2}{E_2} + \\cdots + \\frac{(O_k - E_k)^2}{E_k}\\), \\(E_i = n\\pi_i^0, i = 1, \\dots, k\\) \nReject \\(H_0\\) if  \\(\\chi^2_{test} &gt; \\chi^2_{\\alpha, df}\\), \\(df = k-1\\) \nRequire each \\(E_i \\ge 5\\), \\(i = 1, \\dots, k\\).\n\n\n\n\\(\\chi^2_{test}\\) summarizes how strongly the observed counts tend to deviate from the expected counts or null counts.\n4720: In the textbook, \\(E_i \\ge 1\\) and no more than \\(20\\%\\) of the \\(E_i\\)s are less than 5."
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#goodness-of-fit-test-example",
    "href": "slides/16-infer-cat-slides.html#goodness-of-fit-test-example",
    "title": "Inference for Categorical Data \n",
    "section": "Goodness-of-Fit Test Example",
    "text": "Goodness-of-Fit Test Example\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\nObserved Count\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\\(n = 275\\)\n\n\nExpected Count\n\\(E_1 = 198\\)\n\\(E_2 = 19.25\\)\n\\(E_3 = 33\\)\n\\(E_4 = 24.75\\)\n\\(n = 275\\)\n\n\nProportion under \\(H_0\\)\n\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\\(1.00\\)\n\n\n\n\nUnder \\(H_0\\), \\(\\chi^2_{test} = \\frac{(205 - 198)^2}{198} + \\frac{(26 - 19.25)^2}{19.25} + \\frac{(25 - 33)^2}{33} + \\frac{(19 - 24.75)^2}{24.75} = 5.89\\)\n\\(\\chi^2_{0.05, 3} = 7.81\\).\nDo not reject \\(H_0\\) in favor of \\(H_1\\).\nThe data do not provide convincing evidence of racial bias in the juror selection."
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#goodness-of-fit-test-in-r",
    "href": "slides/16-infer-cat-slides.html#goodness-of-fit-test-in-r",
    "title": "Inference for Categorical Data \n",
    "section": "Goodness-of-Fit Test in R",
    "text": "Goodness-of-Fit Test in R\n\nobs &lt;- c(205, 26, 25, 19)\npi_0 &lt;- c(0.72, 0.07, 0.12, 0.09)\n\n## Use chisq.test() function\nchisq.test(x = obs, p = pi_0)\n\n\n    Chi-squared test for given probabilities\n\ndata:  obs\nX-squared = 6, df = 3, p-value = 0.1\n\n\n\n\n[1] 198.0  19.3  33.0  24.8\n\n\nX-squared \n     5.89 \n\n\n[1] 0.117"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#test-of-independence-contingency-table",
    "href": "slides/16-infer-cat-slides.html#test-of-independence-contingency-table",
    "title": "Inference for Categorical Data \n",
    "section": "Test of Independence (Contingency Table)",
    "text": "Test of Independence (Contingency Table)\n\nHave TWO categorical variables, and want to test whether or not the two variables are independent.\n\n\n\n Does the ‚ÄúOpinion on President‚Äôs Job Performance‚Äù depends on ‚ÄúGender‚Äù? \n\n\nJob performance: approve, disapprove, no opinion\nGender: male, female\n\n\n\n\nVoter\nGender\nPerformance\n\n\n\n1\nM\napprove\n\n\n2\nF\ndisapprove\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\nn\nF\ndisapprove\n\n\n\n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\nMale\n18\n22\n10\n\n\nFemale\n23\n20\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen we consider a two-way table, we often would like to know, are these variables related in any way? That is, are they dependent (versus independent)?"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#test-of-independence-expected-count",
    "href": "slides/16-infer-cat-slides.html#test-of-independence-expected-count",
    "title": "Inference for Categorical Data \n",
    "section": "Test of Independence: Expected Count",
    "text": "Test of Independence: Expected Count\n\nCompute the expected count of each cell in the two-way table under the condition that the two variables were independent with each other.\n\n\n\n\nApprove\nDisapprove\nNo Opinion\nTotal\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n50\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n55\n\n\nTotal\n41\n42\n22\n105\n\n\n\n\n\nThe expected count for the \\(i\\)th row and \\(j\\)th column:\n\n\\[\\text{Expected Count}_{\\text{row i; col j}} = \\frac{\\text{(row i total}) \\times (\\text{col j total})}{\\text{table total}}\\]\n\nAs test of goodness-of-fit, we want to compute the expected count of each cell in the two-way table under the condition that the two variables were independent with each other."
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#test-of-independence-procedure",
    "href": "slides/16-infer-cat-slides.html#test-of-independence-procedure",
    "title": "Inference for Categorical Data \n",
    "section": "Test of Independence Procedure",
    "text": "Test of Independence Procedure\n\nRequire every \\(E_{ij} \\ge 5\\) in the contingency table.\n \\(\\begin{align} &H_0: \\text{Two variables are independent }\\\\ &H_1: \\text{The two are dependent (associated) } \\end{align}\\)\n\\(\\chi^2_{test} = \\sum_{i=1}^r\\sum_{j=1}^c\\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\), where \\(r\\) is the number of rows and \\(c\\) is the number of columns in the table.\nReject \\(H_0\\) if \\(\\chi^2_{test} &gt; \\chi^2_{\\alpha, \\, df}\\), \\(df = (r-1)(c-1)\\).\n\nIn the 4720 textbook, every \\(E_{ij} \\ge 1\\) and no more than \\(20\\%\\) of the \\(E_{ij}\\)s less than 5."
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#test-of-independence-example",
    "href": "slides/16-infer-cat-slides.html#test-of-independence-example",
    "title": "Inference for Categorical Data \n",
    "section": "Test of Independence Example",
    "text": "Test of Independence Example\n\n\n\nApprove\nDisapprove\nNo Opinion\nTotal\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n50\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n55\n\n\nTotal\n41\n42\n22\n105\n\n\n\n\n \\(\\begin{align} &H_0: \\text{ Opinion does not depend on gender } \\\\ &H_1: \\text{ Opinion and gender are dependent } \\end{align}\\)\n\\(\\small \\chi^2_{test} = \\frac{(18 - 19.52)^2}{19.52} + \\frac{(22 - 20)^2}{20} + \\frac{(10 - 10.48)^2}{10.48} + \\frac{(23 - 21.48)^2}{21.48} + \\frac{(20 - 22)^2}{22} + \\frac{(12 - 11.52)^2}{11.52}= 0.65\\)\n\\(\\chi^2_{\\alpha, df} =\\chi^2_{0.05, (2-1)(3-1)} = 5.991\\).\nSince \\(\\chi_{test}^2 &lt; \\chi^2_{\\alpha, df}\\), we do not reject \\(H_0\\).\nWe do not conclude that the Opinion on President‚Äôs Job Performance depends on Gender."
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#test-of-independence-in-r",
    "href": "slides/16-infer-cat-slides.html#test-of-independence-in-r",
    "title": "Inference for Categorical Data \n",
    "section": "Test of Independence in R",
    "text": "Test of Independence in R\n\n(contingency_table &lt;- matrix(c(18, 23, 22, 20, 10, 12), nrow = 2, ncol = 3))\n\n     [,1] [,2] [,3]\n[1,]   18   22   10\n[2,]   23   20   12\n\n## Using chisq.test() function\n(indept_test &lt;- chisq.test(contingency_table))\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 0.7, df = 2, p-value = 0.7\n\nqchisq(0.05, df = (2 - 1) * (3 - 1), lower.tail = FALSE)  ## critical value\n\n[1] 5.99"
  },
  {
    "objectID": "slides/16-infer-cat-slides.html#ai-education-research",
    "href": "slides/16-infer-cat-slides.html#ai-education-research",
    "title": "Inference for Categorical Data \n",
    "section": "AI Education Research",
    "text": "AI Education Research\n\nHelp Marquette to offer AI-related courses, and better teaching using AI!\nhttps://carrollu.qualtrics.com/jfe/form/SV_07bFl5NeSI8tIWO"
  },
  {
    "objectID": "slides/16-infer-cat.html#one-categorical-variable-with-two-categories",
    "href": "slides/16-infer-cat.html#one-categorical-variable-with-two-categories",
    "title": "Inference for Categorical Data \n",
    "section": "One Categorical Variable with Two Categories",
    "text": "One Categorical Variable with Two Categories\n\nA categorical variable Gender having 2 categories Male and Female.\n\n\n\n\n\n\n\n\n\n\n\n\nSubject\nGender\n\n\n\n1\nMale\n\n\n2\nFemale\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\nFemale\n\n\n\n\n\n\nOne-way frequency/count table\n\n\n\\(X\\)\nCount\n\n\n\nMale\n\\(y\\)\n\n\nFemale\n\\(n-y\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe sample male proportion is \\(y/n\\).\nGoal: estimate or test gender ratio or the male population proportion"
  },
  {
    "objectID": "slides/16-infer-cat.html#probability-distribution-for-count-data-two-levels",
    "href": "slides/16-infer-cat.html#probability-distribution-for-count-data-two-levels",
    "title": "Inference for Categorical Data \n",
    "section": "Probability Distribution for Count Data: Two Levels",
    "text": "Probability Distribution for Count Data: Two Levels\n\nThe number of Male can be viewed as a random variable because the count \\(y\\) varies from sample to sample.\n\n\nWhat probability distribution might be appropriate for the count \\(Y\\)?\n\n. . .\n\n\n\\(binomial(n, \\pi)\\) could be a good option for the count data with 2 categories.\n\nFixed number of trials.  (Fixed \\(n\\) subjects) \nEach trial results in one of two outcomes.  (Either \\(M\\) or \\(F\\)) \nTrials are independent.  (If sample subjects randomly) \n\n\n\n. . .\n\nIf the proportion of being in category \\(M\\) is \\(\\pi\\), the count \\(Y\\) has \\[P(Y = y \\mid n, \\pi) = \\frac{n!}{y!(n-y)!}\\pi^{y}(1-\\pi)^{n-y}\\]\nOur job is to do inference about \\(\\pi\\).\n\n\n\nor in general the count \\((Y_M, Y_F)\\) has the probability function \\[P(Y_M = y, Y_F = n-y \\mid n, \\pi) = \\frac{n!}{y!(n-y)!}\\pi^{y}(1-\\pi)^{n-y}\\]\n\nOne of our goals in categorical data analysis is to estimate or test population proportions of categories, here \\(M\\) and \\(F\\), or \\((\\pi, 1-\\pi)\\)."
  },
  {
    "objectID": "slides/16-infer-cat.html#inference-about-a-single-population-proportion",
    "href": "slides/16-infer-cat.html#inference-about-a-single-population-proportion",
    "title": "Inference for Categorical Data \n",
    "section": "Inference About a Single Population Proportion",
    "text": "Inference About a Single Population Proportion\n\n\n\nExample: Exit Poll Suppose we collected data on 1,000 voters in an election with only two candidates: R and D.\n\n\n\n\n\n\nVoter\nR\nD\n\n\n\n1\nx\n\n\n\n2\n\nx\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n1000\nx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the data, we want to predict who won the election."
  },
  {
    "objectID": "slides/16-infer-cat.html#poll-example-contd",
    "href": "slides/16-infer-cat.html#poll-example-contd",
    "title": "Inference for Categorical Data \n",
    "section": "Poll Example Cont‚Äôd",
    "text": "Poll Example Cont‚Äôd\n\n\n\nLet \\(Y\\) be the number of voters voted for R.\nAssume \\(Y \\sim binomial(n = 1000, \\pi)\\).\n\\(\\pi = P(\\text{a voter voted for R}) =\\) population proportion of all voters voted for R: A unknown parameter to be estimated or tested.\nPredict whether or not R won the election.\n\n\n\n\n\nVoter\nR\nD\n\n\n\n1\nx\n\n\n\n2\n\nx\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n1000\nx\n\n\n\n\n\n\nCandidate\nCount\nProportion\n\n\n\nR\n\\(y\\)\n\\(\\hat{\\pi} = y/n\\)\n\n\nD\n\\(n-y\\)\n\\(1-\\hat{\\pi} = 1- \\frac{y}{n}\\)\n\n\n\n\n\n\n\nWhat are \\(H_0\\) and \\(H_1\\)?\n\n. . .\n\n \\(\\begin{align} &H_0: \\pi \\le 1/2 \\\\ &H_1: \\pi &gt; 1/2 \\text{ (more than half voted for R)} \\end{align}\\)"
  },
  {
    "objectID": "slides/16-infer-cat.html#hypothesis-testing-for-pi",
    "href": "slides/16-infer-cat.html#hypothesis-testing-for-pi",
    "title": "Inference for Categorical Data \n",
    "section": "Hypothesis Testing for \\(\\pi\\)\n",
    "text": "Hypothesis Testing for \\(\\pi\\)\n\n\n\nStep 0: Assumptions\n\n \\(n\\pi_0 \\ge 5\\) and \\(n(1-\\pi_0) \\ge 5\\) (the larger, the better) \n\n\n\nStep 1: Null and Alternative Hypothesis\n\n \\(\\begin{align} &H_0: \\pi = \\pi_0 \\\\ &H_1: \\pi &gt; \\pi_0 \\text{ or } \\pi &lt; \\pi_0 \\text{ or } \\pi \\ne \\pi_0 \\end{align}\\) \n\n\n\nStep 2: Set \\(\\alpha\\)\n\n\n\nStep 3: Test Statistic\n\n Under \\(H_0\\), \\(z_{test} = \\dfrac{\\hat{\\pi} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}}\\) where \\(\\hat{\\pi} = \\frac{y}{n} =\\) sample proportion \n\n\n\n. . .\n\nThe sampling distribution of \\(\\hat{\\pi}\\) is approximately normal with mean \\(\\pi\\) and standard error \\(\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\) if \\(y_i\\) are independent and the assumptions are satisfied."
  },
  {
    "objectID": "slides/16-infer-cat.html#hypothesis-testing-for-pi-1",
    "href": "slides/16-infer-cat.html#hypothesis-testing-for-pi-1",
    "title": "Inference for Categorical Data \n",
    "section": "Hypothesis Testing for \\(\\pi\\)\n",
    "text": "Hypothesis Testing for \\(\\pi\\)\n\n\nStep 4-c: Critical Value \\(z_{\\alpha}\\) (one-tailed) or \\(z_{\\alpha/2}\\) (two-tailed)\n\nStep 5-c: Draw a Conclusion Using Critical Value Method \n\n \\(H_1: \\pi &gt; \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z &gt; z_{\\alpha}\\) \n \\(H_1: \\pi &lt; \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z &lt; -z_{\\alpha}\\) \n \\(H_1: \\pi \\ne \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(|z| &gt; z_{\\alpha/2}\\) \n\n\nStep 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim"
  },
  {
    "objectID": "slides/16-infer-cat.html#poll-example-contd-hypothesis-testing",
    "href": "slides/16-infer-cat.html#poll-example-contd-hypothesis-testing",
    "title": "Inference for Categorical Data \n",
    "section": "Poll Example Cont‚Äôd (Hypothesis Testing)",
    "text": "Poll Example Cont‚Äôd (Hypothesis Testing)\n\nIn an exit poll of 1,000 voters, 520 voted for R, one of the two candidates.\nStep 0: \\(n\\pi_0 = 1000(1/2) = 500 \\ge 5\\) and \\(n(1-\\pi_0) \\ge 5\\) \nStep 1:  \\(\\begin{align} &H_0: \\pi \\le 1/2 \\\\ &H_1: \\pi &gt; 1/2 \\end{align}\\) \nStep 2:  \\(\\alpha = 0.05\\) \nStep 3:  \\(z_{test} = \\frac{\\hat{\\pi} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}} =  \\frac{\\frac{520}{1000} - 0.5}{\\sqrt{\\frac{0.5(1-0.5)}{1000}}} = 1.26\\) \n\n. . .\n\nStep 4:  \\(z_{\\alpha} = z_{0.05} = 1.645\\) \nStep 5:  Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z_{test} &gt; z_{\\alpha}\\). Since \\(z_{test} &lt; z_{\\alpha}\\), we do not reject \\(H_0\\). \nStep 6:  We do not have sufficient evidence to conclude that R won. \nWe make the same conclusion based on \\(p\\)-value.\n\n\\[ p\\text{-value} = P(Z &gt; 1.26) = 0.1 &gt; 0.05\\]\n\nIs there a sufficient evidence to conclude at \\(\\alpha = 0.05\\) that R won the election?"
  },
  {
    "objectID": "slides/16-infer-cat.html#confidence-interval-for-pi",
    "href": "slides/16-infer-cat.html#confidence-interval-for-pi",
    "title": "Inference for Categorical Data \n",
    "section": "Confidence Interval for \\(\\pi\\)\n",
    "text": "Confidence Interval for \\(\\pi\\)\n\n\nAssumptions: \\(n\\hat{\\pi} \\ge 5\\) and \\(n(1-\\hat{\\pi}) \\ge 5\\)\nA \\(100(1 - \\alpha)\\%\\) confidence interval for \\(\\pi\\) is \\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\] where \\(\\hat{\\pi} = y/n\\).\n\n. . .\n\n\n\\(\\pi\\) is unknown and use the estimate \\(\\hat{\\pi}\\) instead: \\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}}\\]\n\n\n\nüëâ No hypothesized value \\(\\pi_0\\) in the confidence interval."
  },
  {
    "objectID": "slides/16-infer-cat.html#poll-example-contd-confidence-interval",
    "href": "slides/16-infer-cat.html#poll-example-contd-confidence-interval",
    "title": "Inference for Categorical Data \n",
    "section": "Poll Example Cont‚Äôd (Confidence Interval)",
    "text": "Poll Example Cont‚Äôd (Confidence Interval)\n\nAssumption: \\(n\\hat{\\pi} = 1000(0.52) = 520 \\ge 5\\) and \\(n(1-\\hat{\\pi}) = 480 \\ge 5\\).\nEstimate the proportion of all voters voted for R using 95% confidence interval:\n\n\\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}} = 0.52 \\pm z_{0.025}\\sqrt{\\frac{0.52(1-0.52)}{1000}} = (0.49, 0.55).\\]\n. . .\n\n\n# Use alternative = \"two.sided\" to get CI\nprop.test(x = 520, n = 1000, p = 0.5, alternative = \"greater\", correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  520 out of 1000, null probability 0.5\nX-squared = 2, df = 1, p-value = 0.1\nalternative hypothesis: true p is greater than 0.5\n95 percent confidence interval:\n 0.494 1.000\nsample estimates:\n   p \n0.52"
  },
  {
    "objectID": "slides/16-infer-cat.html#inference-about-two-population-proportions",
    "href": "slides/16-infer-cat.html#inference-about-two-population-proportions",
    "title": "Inference for Categorical Data \n",
    "section": "Inference About Two Population Proportions",
    "text": "Inference About Two Population Proportions\n\n\n\n\n\n\n\nGroup 1 (M)\nGroup 2 (F)\n\n\n\ntrials\n\\(n_1\\)\n\\(n_2\\)\n\n\nnumber of successes\n\\(Y_1\\)\n\\(Y_2\\)\n\n\ndistribution\n\\(binomial(n_1, \\pi_1)\\)\n\\(binomial(n_2, \\pi_2)\\)\n\n\n\n\n\n\\(\\pi_1\\): Population proportion of success of Group 1\n\n\\(\\pi_2\\): Population proportion of success of Group 2\n\n\n\n\n\nVoter\nGender\nApprove\n\n\n\n1\nM\nYes\n\n\n2\nF\nYes\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n1000\nF\nNo\n\n\n\n\n\n\nIs male‚Äôs president approval rate \\(\\pi_1\\) higher than the female‚Äôs approval rate \\(\\pi_2\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing two population proportions \\(\\pi_1\\) and \\(\\pi_2\\)."
  },
  {
    "objectID": "slides/16-infer-cat.html#hypothesis-testing-for-pi_1-and-pi_2",
    "href": "slides/16-infer-cat.html#hypothesis-testing-for-pi_1-and-pi_2",
    "title": "Inference for Categorical Data \n",
    "section": "Hypothesis Testing for \\(\\pi_1\\) and \\(\\pi_2\\)\n",
    "text": "Hypothesis Testing for \\(\\pi_1\\) and \\(\\pi_2\\)\n\n\n\nStep 0: Assumptions\n\n \\(n_1\\hat{\\pi}_1 \\ge 5\\), \\(n_1(1-\\hat{\\pi}_1) \\ge 5\\) and \\(n_2\\hat{\\pi}_2 \\ge 5\\), \\(n_2(1-\\hat{\\pi}_2) \\ge 5\\) \n\n\n\nStep 1: Null and Alternative Hypothesis\n\n \\(\\begin{align}  &H_0: \\pi_1 = \\pi_2 \\\\ &H_1: \\pi_1 &gt; \\pi_2 \\text{ or } \\pi_1 &lt; \\pi_2 \\text{ or } \\pi_1 \\ne \\pi_2 \\end{align}\\) \n\n\nStep 2: Set \\(\\alpha\\)\n\nStep 3: Test Statistic\n\n \\(z_{test} = \\dfrac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\bar{\\pi}(1-\\bar{\\pi})(\\frac{1}{n_1} + \\frac{1}{n_2}})}\\), \\(\\bar{\\pi} = \\frac{y_1+y_2}{n_1+n_2}\\) is the pooled sample proportion estimating \\(\\pi\\) \n\n\n\n\n\\(z_{test} = \\dfrac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\frac{\\bar{\\pi}(1-\\bar{\\pi})}{n_1} + \\frac{\\bar{\\pi}(1-\\bar{\\pi})}{n_2}}} = \\dfrac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\bar{\\pi}(1-\\bar{\\pi})(\\frac{1}{n_1} + \\frac{1}{n_2}})}\\), \\(\\bar{\\pi} = \\frac{y_1+y_2}{n_1+n_2}\\) is the pooled sample proportion that is an estimate of \\(\\pi\\)"
  },
  {
    "objectID": "slides/16-infer-cat.html#hypothesis-testing-for-pi_1-and-pi_2-1",
    "href": "slides/16-infer-cat.html#hypothesis-testing-for-pi_1-and-pi_2-1",
    "title": "Inference for Categorical Data \n",
    "section": "Hypothesis Testing for \\(\\pi_1\\) and \\(\\pi_2\\)\n",
    "text": "Hypothesis Testing for \\(\\pi_1\\) and \\(\\pi_2\\)\n\n\nStep 4-c: Find the Critical Value \\(z_{\\alpha}\\) (one-tailed) or \\(z_{\\alpha/2}\\) (two-tailed)\n\nStep 5-c: Draw a Conclusion Using Critical Value Method\n\n\n Reject \\(H_0\\) in favor of \\(H_1\\) if \n\n \\(H_1: \\pi_1 &gt; \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z &gt; z_{\\alpha}\\) \n \\(H_1: \\pi_1 &lt; \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z &lt; -z_{\\alpha}\\) \n \\(H_1: \\pi_1 \\ne \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(|z| &gt; z_{\\alpha/2}\\) \n\n\n\n\nStep 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim"
  },
  {
    "objectID": "slides/16-infer-cat.html#example-effectiveness-of-learning",
    "href": "slides/16-infer-cat.html#example-effectiveness-of-learning",
    "title": "Inference for Categorical Data \n",
    "section": "Example: Effectiveness of Learning",
    "text": "Example: Effectiveness of Learning\n\n\nA study on 300 students to compare the effectiveness of learning Statistics via online and in-person classes.\n\nRandomly assign\n\n125 students to the online program\nthe remaining 175 to the in-person program.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExam Results\nOnline Instruction\nIn-Person Instruction\n\n\n\nPass\n94\n113\n\n\nFail\n31\n62\n\n\nTotal\n125\n175\n\n\n\n\nIs there sufficient evidence to conclude that the online program is more effective than the traditional in-person program at \\(\\alpha=0.05\\)?"
  },
  {
    "objectID": "slides/16-infer-cat.html#example-effectiveness-of-learning-contd-testing",
    "href": "slides/16-infer-cat.html#example-effectiveness-of-learning-contd-testing",
    "title": "Inference for Categorical Data \n",
    "section": "Example: Effectiveness of Learning Cont‚Äôd (Testing)",
    "text": "Example: Effectiveness of Learning Cont‚Äôd (Testing)\n\nStep 1:  \\(H_0: \\pi_1 = \\pi_2\\) vs.¬†\\(H_1: \\pi_1 &gt; \\pi_2\\) \\(\\pi_1\\) \\((\\pi_2)\\) is the population proportion of students passing the exam under the online (in-person) program.\n\n. . .\n\nStep 0: \\(\\hat{\\pi}_1 = 94/125 = 0.75\\) and \\(\\hat{\\pi}_2 = 113/175 = 0.65\\).\\(n_1\\hat{\\pi}_1 = 94 &gt; 5\\), \\(n_1(1-\\hat{\\pi}_1) = 31 &gt; 5\\), and \\(n_2\\hat{\\pi}_2 = 113 &gt; 5\\), \\(n_2(1-\\hat{\\pi}_2) = 62 &gt; 5\\) \n\n\n. . .\n\nStep 2:  \\(\\alpha = 0.05\\) \n\n\n. . .\n\nStep 3:  \\(\\bar{\\pi} = \\frac{94+113}{125+175} = 0.69\\). \\(z_{test} = \\dfrac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\bar{\\pi}(1-\\bar{\\pi})(\\frac{1}{n_1} + \\frac{1}{n_2}})} = \\frac{0.75 - 0.65}{\\sqrt{0.69(1-0.69)(\\frac{1}{125} + \\frac{1}{175})}} = 1.96\\) \n\n\n. . .\n\nStep 4:  \\(z_{\\alpha} = z_{0.05} = 1.645\\) \n\n\n. . .\n\nStep 5:  Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z_{test} &gt; z_{\\alpha}\\). Since \\(z_{test} &gt; z_{\\alpha}\\), we reject \\(H_0\\). \n\n\n. . .\n\nStep 6:  We have sufficient evidence to conclude that the online program is more effective."
  },
  {
    "objectID": "slides/16-infer-cat.html#confidence-interval-for-pi_1---pi_2",
    "href": "slides/16-infer-cat.html#confidence-interval-for-pi_1---pi_2",
    "title": "Inference for Categorical Data \n",
    "section": "Confidence Interval for \\(\\pi_1 - \\pi_2\\)\n",
    "text": "Confidence Interval for \\(\\pi_1 - \\pi_2\\)\n\n\nA \\(100(1 - \\alpha)\\%\\) confidence interval for \\(\\pi_1 - \\pi_2\\) is\n\n\\[\\hat{\\pi}_1 - \\hat{\\pi}_2 \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}_1(1-\\hat{\\pi}_1)}{n_1}+\\frac{\\hat{\\pi}_2(1-\\hat{\\pi}_2)}{n_2}}\\]\n\nRequirements: \\(n_1\\hat{\\pi}_1 \\ge 5\\), \\(n_1(1-\\hat{\\pi}_1) \\ge 5\\) and \\(n_2\\hat{\\pi}_2 \\ge 5\\), \\(n_2(1-\\hat{\\pi}_2) \\ge 5\\)"
  },
  {
    "objectID": "slides/16-infer-cat.html#example-effectiveness-of-learning-contd-ci",
    "href": "slides/16-infer-cat.html#example-effectiveness-of-learning-contd-ci",
    "title": "Inference for Categorical Data \n",
    "section": "Example: Effectiveness of Learning Cont‚Äôd (CI)",
    "text": "Example: Effectiveness of Learning Cont‚Äôd (CI)\n\nWant to know how much effective is the online program.\nEstimate \\(\\pi_1 - \\pi_2\\) using a \\(95\\%\\) confidence interval:\n\n\\[\\hat{\\pi}_1 - \\hat{\\pi}_2 \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}_1(1-\\hat{\\pi}_1)}{n_1}+\\frac{\\hat{\\pi}_2(1-\\hat{\\pi}_2)}{n_2}}\\]\n\n\\(z_{0.05/2} = 1.96\\)\nThe 95% confidence interval is\n\n\\[0.75 - 0.65 \\pm 1.96\\sqrt{\\frac{(0.75)(1-0.75)}{125} + \\frac{(0.65)(1-0.65)}{175}}\\\\\n= (0.002, 0.210)\\]"
  },
  {
    "objectID": "slides/16-infer-cat.html#implementation-in-r",
    "href": "slides/16-infer-cat.html#implementation-in-r",
    "title": "Inference for Categorical Data \n",
    "section": "Implementation in R",
    "text": "Implementation in R\n\n\n# Use alternative = \"two.sided\" to get CI\nprop.test(x = c(94, 113), n = c(125, 175), alternative = \"greater\", correct = FALSE)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  c(94, 113) out of c(125, 175)\nX-squared = 4, df = 1, p-value = 0.02\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.0193 1.0000\nsample estimates:\nprop 1 prop 2 \n 0.752  0.646 \n\n\n\nprop_ci &lt;- prop.test(x = c(94, 113), n = c(125, 175), alternative = \"two.sided\", correct = FALSE)\nprop_ci$conf.int\n\n[1] 0.00259 0.20998\nattr(,\"conf.level\")\n[1] 0.95"
  },
  {
    "objectID": "slides/16-infer-cat.html#categorical-variable-with-more-than-2-categories",
    "href": "slides/16-infer-cat.html#categorical-variable-with-more-than-2-categories",
    "title": "Inference for Categorical Data \n",
    "section": "Categorical Variable with More Than 2 Categories",
    "text": "Categorical Variable with More Than 2 Categories\n\n\n\nA categorical variable has \\(k\\) categories \\(A_1, \\dots, A_k\\).\n\n\n\n\n\n\n\n\n\n\n\nSubject\nVariable\n\n\n\n1\n\\(A_2\\)\n\n\n2\n\\(A_4\\)\n\n\n3\n\\(A_1\\)\n\n\n4\n\\(A_3\\)\n\n\n5\n\\(A_k\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(A_3\\)\n\n\n\n\n\n\nWith the size \\(n\\), for categories \\(A_1, \\dots , A_k\\), their observed count is \\(O_1, \\dots, O_k\\), and \\(\\sum_{i=1}^kO_i = n\\).\nOne-way count table:\n\n\n\n\n\\(A_1\\)\n\\(A_2\\)\n\\(\\cdots\\)\n\\(A_k\\)\nTotal\n\n\n\\(O_1\\)\n\\(O_2\\)\n\\(\\cdots\\)\n\\(O_k\\)\n\\(n\\)"
  },
  {
    "objectID": "slides/16-infer-cat.html#example-more-than-2-categories",
    "href": "slides/16-infer-cat.html#example-more-than-2-categories",
    "title": "Inference for Categorical Data \n",
    "section": "Example: More Than 2 Categories",
    "text": "Example: More Than 2 Categories\n\n\nAre the selected jurors are racially representative of the population?\n\n\nIdea: If the jury is representative of the population, the proportions in the sample should reflect the population of eligible jurors, i.e., registered voters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\nRepresentation in juries\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\\(n = 275\\)\n\n\nRegistered voters\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\\(1.00\\)"
  },
  {
    "objectID": "slides/16-infer-cat.html#example-more-than-2-categories-1",
    "href": "slides/16-infer-cat.html#example-more-than-2-categories-1",
    "title": "Inference for Categorical Data \n",
    "section": "Example: More Than 2 Categories",
    "text": "Example: More Than 2 Categories\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\nRepresentation in juries\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\\(n = 275\\)\n\n\nRegistered voters\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\\(1.00\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\nRepresentation in juries\n\\(O_1/n = 0.75\\)\n\\(O_2/n = 0.09\\)\n\\(O_3/n = 0.09\\)\n\\(O_4/n = 0.07\\)\n\\(1.00\\)\n\n\nRegistered voters\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\\(1.00\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile the proportions in the juries do not precisely represent the population proportions, it is unclear whether these data provide convincing evidence that the sample is not representative. If the jurors really were randomly sampled from the registered voters, we might expect small differences due to chance. However, unusually large differences may provide convincing evidence that the juries were not representative."
  },
  {
    "objectID": "slides/16-infer-cat.html#example-more-than-2-categories-2",
    "href": "slides/16-infer-cat.html#example-more-than-2-categories-2",
    "title": "Inference for Categorical Data \n",
    "section": "Example: More Than 2 Categories",
    "text": "Example: More Than 2 Categories\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\nRepresentation in juries\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\\(n = 275\\)\n\n\nRegistered voters\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\\(1.00\\)\n\n\n\n\nIf the individuals are randomly selected to serve on a jury, about how many of the 275 people would we expect to be white? How about Hispanic?\n\n. . .\n\nAbout \\(72\\%\\) of the population is white, so we would expect about \\(72\\%\\) of the jurors to be white: \\(0.72 \\times 275 = 198\\).\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\nRepresentation in juries\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\n\nHypothesized expected number\n\\(275 \\times \\pi_1^0 = 198\\)\n\\(275 \\times \\pi_2^0 = 19.25\\)\n\\(275 \\times \\pi_3^0 = 33\\)\n\\(275 \\times \\pi_4^0 = 24.75\\)"
  },
  {
    "objectID": "slides/16-infer-cat.html#goodness-of-fit-test",
    "href": "slides/16-infer-cat.html#goodness-of-fit-test",
    "title": "Inference for Categorical Data \n",
    "section": "Goodness-of-Fit Test",
    "text": "Goodness-of-Fit Test\n\nAre the proportions (counts) of juries close enough to the proportions (counts) of registered voters, so that we are confident saying that the jurors really were randomly sampled from the registered voters?\n\n\nA goodness-of-fit test tests the hypothesis that the observed frequency distribution fits or conforms to some claim distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\nObserved Count\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\\(n = 275\\)\n\n\nExpected Count\n\\(E_1 = 198\\)\n\\(E_2 = 19.25\\)\n\\(E_3 = 33\\)\n\\(E_4 = 24.75\\)\n\\(n = 275\\)\n\n\nProportion under \\(H_0\\)\n\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\\(1.00\\)\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nObserved Count\n205\n26\n25\n19\n\n\n\nExpected Count\n198\n19.25\n33\n24.75\n\n\nPopulation Proportion\n0.72\n0.07\n0.12\n0.09\n\n\n\n\nGiven a sample of cases that can be classified into several (more than 2) groups, determine if the sample is representative of the general population.\nEvaluate whether data resemble a particular distribution.\nThe sample proportion represented from each race among the 275 jurors was not a precise match for any ethnic group. While some sampling variation is expected, we would expect the sample proportions to be fairly similar to the population proportions if there is no bias on juries.\nWe need to test whether the differences are strong enough to provide convincing evidence that the jurors are not a random sample."
  },
  {
    "objectID": "slides/16-infer-cat.html#goodness-of-fit-test-1",
    "href": "slides/16-infer-cat.html#goodness-of-fit-test-1",
    "title": "Inference for Categorical Data \n",
    "section": "Goodness-of-Fit Test",
    "text": "Goodness-of-Fit Test\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\n\nObserved Count\n205\n26\n25\n19\n275\n\n\n\nExpected Count\n198\n19.25\n33\n24.75\n275\n\n\nPopulation Proportion \\((H_0)\\)\n\n0.72\n0.07\n0.12\n0.09\n1.00\n\n\n\n\nObserved Count and Expected Count are similar if no bias on juries.\nTest whether the differences are strong enough to provide convincing evidence that the jurors are not a random sample of registered voters.\n\n \\(\\begin{align} &H_0: \\text{No racial bias in who serves on a jury, and } \\\\ &H_1: \\text{There is racial bias in juror selection} \\end{align}\\) \n. . .\n \\(\\begin{align} &H_0: \\pi_1 = \\pi_1^0,  \\pi_2 = \\pi_2^0, \\dots, \\pi_k = \\pi_k^0\\\\ &H_1: \\pi_i \\ne \\pi_i^0 \\text{ for some } i \\end{align}\\) \n\n\nWhile some sampling variation is expected, we would expect the sample proportions to be similar to the population proportions if there is no bias on juries.\nWe need to test whether the differences are strong enough to provide convincing evidence that the jurors are not a random sample of registered voters.\n\\(\\quad\\text{the observed counts reflect natural sampling fluctuation.}\\)"
  },
  {
    "objectID": "slides/16-infer-cat.html#goodness-of-fit-test-statistic",
    "href": "slides/16-infer-cat.html#goodness-of-fit-test-statistic",
    "title": "Inference for Categorical Data \n",
    "section": "Goodness-of-Fit Test Statistic",
    "text": "Goodness-of-Fit Test Statistic\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\nObserved Count\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\\(n = 275\\)\n\n\nExpected Count\n\\(E_1 = 198\\)\n\\(E_2 = 19.25\\)\n\\(E_3 = 33\\)\n\\(E_4 = 24.75\\)\n\\(n = 275\\)\n\n\nProportion under \\(H_0\\)\n\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\\(1.00\\)\n\n\n\n\n Under \\(H_0\\), \\(\\chi^2_{test} = \\frac{(O_1 - E_1)^2}{E_1} + \\frac{(O_2 - E_2)^2}{E_2} + \\cdots + \\frac{(O_k - E_k)^2}{E_k}\\), \\(E_i = n\\pi_i^0, i = 1, \\dots, k\\) \nReject \\(H_0\\) if  \\(\\chi^2_{test} &gt; \\chi^2_{\\alpha, df}\\), \\(df = k-1\\) \nRequire each \\(E_i \\ge 5\\), \\(i = 1, \\dots, k\\).\n\n\n\n\n\\(\\chi^2_{test}\\) summarizes how strongly the observed counts tend to deviate from the expected counts or null counts.\n4720: In the textbook, \\(E_i \\ge 1\\) and no more than \\(20\\%\\) of the \\(E_i\\)s are less than 5."
  },
  {
    "objectID": "slides/16-infer-cat.html#goodness-of-fit-test-example",
    "href": "slides/16-infer-cat.html#goodness-of-fit-test-example",
    "title": "Inference for Categorical Data \n",
    "section": "Goodness-of-Fit Test Example",
    "text": "Goodness-of-Fit Test Example\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\nObserved Count\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\\(n = 275\\)\n\n\nExpected Count\n\\(E_1 = 198\\)\n\\(E_2 = 19.25\\)\n\\(E_3 = 33\\)\n\\(E_4 = 24.75\\)\n\\(n = 275\\)\n\n\nProportion under \\(H_0\\)\n\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\\(1.00\\)\n\n\n\n\nUnder \\(H_0\\), \\(\\chi^2_{test} = \\frac{(205 - 198)^2}{198} + \\frac{(26 - 19.25)^2}{19.25} + \\frac{(25 - 33)^2}{33} + \\frac{(19 - 24.75)^2}{24.75} = 5.89\\)\n\\(\\chi^2_{0.05, 3} = 7.81\\).\nDo not reject \\(H_0\\) in favor of \\(H_1\\).\nThe data do not provide convincing evidence of racial bias in the juror selection."
  },
  {
    "objectID": "slides/16-infer-cat.html#goodness-of-fit-test-in-r",
    "href": "slides/16-infer-cat.html#goodness-of-fit-test-in-r",
    "title": "Inference for Categorical Data \n",
    "section": "Goodness-of-Fit Test in R",
    "text": "Goodness-of-Fit Test in R\n\nobs &lt;- c(205, 26, 25, 19)\npi_0 &lt;- c(0.72, 0.07, 0.12, 0.09)\n\n## Use chisq.test() function\nchisq.test(x = obs, p = pi_0)\n\n\n    Chi-squared test for given probabilities\n\ndata:  obs\nX-squared = 6, df = 3, p-value = 0.1\n\n\n\n\n\n[1] 198.0  19.3  33.0  24.8\n\n\nX-squared \n     5.89 \n\n\n[1] 0.117"
  },
  {
    "objectID": "slides/16-infer-cat.html#test-of-independence-contingency-table",
    "href": "slides/16-infer-cat.html#test-of-independence-contingency-table",
    "title": "Inference for Categorical Data \n",
    "section": "Test of Independence (Contingency Table)",
    "text": "Test of Independence (Contingency Table)\n\nHave TWO categorical variables, and want to test whether or not the two variables are independent.\n\n. . .\n\n Does the ‚ÄúOpinion on President‚Äôs Job Performance‚Äù depends on ‚ÄúGender‚Äù? \n\n\nJob performance: approve, disapprove, no opinion\nGender: male, female\n\n\n\n\nVoter\nGender\nPerformance\n\n\n\n1\nM\napprove\n\n\n2\nF\ndisapprove\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\nn\nF\ndisapprove\n\n\n\n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\nMale\n18\n22\n10\n\n\nFemale\n23\n20\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen we consider a two-way table, we often would like to know, are these variables related in any way? That is, are they dependent (versus independent)?"
  },
  {
    "objectID": "slides/16-infer-cat.html#test-of-independence-expected-count",
    "href": "slides/16-infer-cat.html#test-of-independence-expected-count",
    "title": "Inference for Categorical Data \n",
    "section": "Test of Independence: Expected Count",
    "text": "Test of Independence: Expected Count\n\nCompute the expected count of each cell in the two-way table under the condition that the two variables were independent with each other.\n\n\n\n\nApprove\nDisapprove\nNo Opinion\nTotal\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n50\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n55\n\n\nTotal\n41\n42\n22\n105\n\n\n\n. . .\n\nThe expected count for the \\(i\\)th row and \\(j\\)th column:\n\n\\[\\text{Expected Count}_{\\text{row i; col j}} = \\frac{\\text{(row i total}) \\times (\\text{col j total})}{\\text{table total}}\\]\n\n\nAs test of goodness-of-fit, we want to compute the expected count of each cell in the two-way table under the condition that the two variables were independent with each other."
  },
  {
    "objectID": "slides/16-infer-cat.html#test-of-independence-procedure",
    "href": "slides/16-infer-cat.html#test-of-independence-procedure",
    "title": "Inference for Categorical Data \n",
    "section": "Test of Independence Procedure",
    "text": "Test of Independence Procedure\n\nRequire every \\(E_{ij} \\ge 5\\) in the contingency table.\n \\(\\begin{align} &H_0: \\text{Two variables are independent }\\\\ &H_1: \\text{The two are dependent (associated) } \\end{align}\\)\n\\(\\chi^2_{test} = \\sum_{i=1}^r\\sum_{j=1}^c\\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\), where \\(r\\) is the number of rows and \\(c\\) is the number of columns in the table.\nReject \\(H_0\\) if \\(\\chi^2_{test} &gt; \\chi^2_{\\alpha, \\, df}\\), \\(df = (r-1)(c-1)\\).\n\n\nIn the 4720 textbook, every \\(E_{ij} \\ge 1\\) and no more than \\(20\\%\\) of the \\(E_{ij}\\)s less than 5."
  },
  {
    "objectID": "slides/16-infer-cat.html#test-of-independence-example",
    "href": "slides/16-infer-cat.html#test-of-independence-example",
    "title": "Inference for Categorical Data \n",
    "section": "Test of Independence Example",
    "text": "Test of Independence Example\n\n\n\nApprove\nDisapprove\nNo Opinion\nTotal\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n50\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n55\n\n\nTotal\n41\n42\n22\n105\n\n\n\n\n \\(\\begin{align} &H_0: \\text{ Opinion does not depend on gender } \\\\ &H_1: \\text{ Opinion and gender are dependent } \\end{align}\\)\n\\(\\small \\chi^2_{test} = \\frac{(18 - 19.52)^2}{19.52} + \\frac{(22 - 20)^2}{20} + \\frac{(10 - 10.48)^2}{10.48} + \\frac{(23 - 21.48)^2}{21.48} + \\frac{(20 - 22)^2}{22} + \\frac{(12 - 11.52)^2}{11.52}= 0.65\\)\n\\(\\chi^2_{\\alpha, df} =\\chi^2_{0.05, (2-1)(3-1)} = 5.991\\).\nSince \\(\\chi_{test}^2 &lt; \\chi^2_{\\alpha, df}\\), we do not reject \\(H_0\\).\nWe do not conclude that the Opinion on President‚Äôs Job Performance depends on Gender."
  },
  {
    "objectID": "slides/16-infer-cat.html#test-of-independence-in-r",
    "href": "slides/16-infer-cat.html#test-of-independence-in-r",
    "title": "Inference for Categorical Data \n",
    "section": "Test of Independence in R",
    "text": "Test of Independence in R\n\n(contingency_table &lt;- matrix(c(18, 23, 22, 20, 10, 12), nrow = 2, ncol = 3))\n\n     [,1] [,2] [,3]\n[1,]   18   22   10\n[2,]   23   20   12\n\n## Using chisq.test() function\n(indept_test &lt;- chisq.test(contingency_table))\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 0.7, df = 2, p-value = 0.7\n\nqchisq(0.05, df = (2 - 1) * (3 - 1), lower.tail = FALSE)  ## critical value\n\n[1] 5.99"
  },
  {
    "objectID": "slides/16-infer-cat.html#ai-education-research",
    "href": "slides/16-infer-cat.html#ai-education-research",
    "title": "Inference for Categorical Data \n",
    "section": "AI Education Research",
    "text": "AI Education Research\n\nHelp Marquette to offer AI-related courses, and better teaching using AI!\nhttps://carrollu.qualtrics.com/jfe/form/SV_07bFl5NeSI8tIWO"
  },
  {
    "objectID": "slides/15-regression-slides.html#what-is-regression",
    "href": "slides/15-regression-slides.html#what-is-regression",
    "title": "Regression \n",
    "section": "What is Regression",
    "text": "What is Regression\n\nRegression models the relationship between one or more numerical/categorical response variables \\((Y)\\) and one or more numerical/categorical explanatory variables \\((X)\\).\nA regression function \\(f(X)\\) describes how a response variable \\(Y\\), on average, changes as an explanatory variable \\(X\\) changes.\n\n\n\nExamples:\n\n college GPA \\((Y)\\) vs.¬†ACT/SAT score \\((X)\\)\n sales \\((Y)\\) vs.¬†advertising expenditure \\((X)\\)\n crime rate \\((Y)\\) vs.¬†median income level \\((X)\\) \n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression models the relationship between a numerical response variable (dependent variable) and one or more (numerical/categorical) explanatory variables (independent variables, predictors, or covariates).\nA regression function describes how a response variable \\(Y\\) changes as an explanatory variable \\(X\\) changes.\nThe true relationship between \\(X\\) and \\(Y\\), the regression function, is unknown.\nThe goal of regression is to estimate the regression function and use it to predict value of \\(Y\\) given a value of \\(X\\).\nExamples:\n\n Relationship between college GPA (\\(Y\\)) and ACT/SAT scores (\\(X\\))\n Relationship between sales (\\(Y\\)) and advertising expenditure (\\(X\\))\n Relationship between crime rate (\\(Y\\)) and median income level (\\(X\\))"
  },
  {
    "objectID": "slides/15-regression-slides.html#unknown-regression-function",
    "href": "slides/15-regression-slides.html#unknown-regression-function",
    "title": "Regression \n",
    "section": "Unknown Regression Function",
    "text": "Unknown Regression Function\n\nThe true relationship between \\(X\\) and the mean of \\(Y\\), the regression function \\(f(X)\\), is unknown.\nThe collected data \\((x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\) is all we know and have.\nGoal: estimate \\(f(X)\\) from our data and use it to predict value of \\(Y\\) given any value of \\(X\\)."
  },
  {
    "objectID": "slides/15-regression-slides.html#simple-linear-regression",
    "href": "slides/15-regression-slides.html#simple-linear-regression",
    "title": "Regression \n",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nStart with simple linear regression:\n\n\nOnly one predictor \\(X\\) (known and constant) and one response variable \\(Y\\)\n\nthe regression function used for predicting \\(Y\\) is a linear function.\nuse a regression line in a X-Y plane to predict the value of \\(Y\\) for a given value of \\(X = x\\).\n\n\n\n\n\n\nMath review: A linear function \\(y = f(x) = \\beta_0 + \\beta_1 x\\) represents a straight line\n\n\\(\\beta_1\\): slope, the amount by which \\(y\\) changes when \\(x\\) increases by one unit.\n\\(\\beta_0\\): intercept, the value of \\(y\\) when \\(x = 0\\).\nThe linearity assumption: \\(\\beta_1\\) does not change as \\(x\\) changes."
  },
  {
    "objectID": "slides/15-regression-slides.html#sample-data-relationship-between-x-and-y",
    "href": "slides/15-regression-slides.html#sample-data-relationship-between-x-and-y",
    "title": "Regression \n",
    "section": "Sample Data: Relationship Between X and Y",
    "text": "Sample Data: Relationship Between X and Y\n\nReal data \\((x_i, y_i), i = 1, 2, \\dots, n\\) do not form a perfect straight line!\n\\(y_i = \\beta_0+\\beta_1x_i + \\color{red}{\\epsilon_i}\\)"
  },
  {
    "objectID": "slides/15-regression-slides.html#section-1",
    "href": "slides/15-regression-slides.html#section-1",
    "title": "Regression \n",
    "section": "",
    "text": "When we collect our data, at any given level of \\(X = x\\), \\(y\\) is assumed being drawn from a normal distribution (for inference purpose).\nIts value varies around and will not be exactly equal to its mean \\(\\mu_y\\)."
  },
  {
    "objectID": "slides/15-regression-slides.html#section-2",
    "href": "slides/15-regression-slides.html#section-2",
    "title": "Regression \n",
    "section": "",
    "text": "When we collect our data, at any given level of \\(X = x\\), \\(y\\) is assumed being drawn from a normal distribution (for inference purpose).\nIts value varies around and will not be exactly equal to its mean \\(\\mu_y\\)."
  },
  {
    "objectID": "slides/15-regression-slides.html#section-3",
    "href": "slides/15-regression-slides.html#section-3",
    "title": "Regression \n",
    "section": "",
    "text": "The mean of \\(Y\\) and \\(X\\) form a straight line."
  },
  {
    "objectID": "slides/15-regression-slides.html#simple-linear-regression-model-population",
    "href": "slides/15-regression-slides.html#simple-linear-regression-model-population",
    "title": "Regression \n",
    "section": "Simple Linear Regression Model (Population)",
    "text": "Simple Linear Regression Model (Population)\nFor the \\(i\\)-th measurement in the target population,\n\\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\]\n\n\\(Y_i\\): the \\(i\\)-th value of the response (random) variable.\n\\(X_i\\): the \\(i\\)-th known fixed value of the predictor.\n\\(\\epsilon_i\\): the \\(i\\)-th random error with assumption \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\).\n\\(\\beta_0\\) and \\(\\beta_1\\) are model coefficients.\n\\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) are fixed unknown parameters to be estimated from the sample data once we collect them.\n\n.question[ What is the distribution of \\(Y_i\\) given a value of \\(X = x\\)?"
  },
  {
    "objectID": "slides/15-regression-slides.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i",
    "href": "slides/15-regression-slides.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i",
    "title": "Regression \n",
    "section": "Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n",
    "text": "Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n\n\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\)    \n\\[\\begin{align*}\n\\mu_{Y_i \\mid X_i} = \\beta_0 + \\beta_1X_i\n\\end{align*}\\]\nThe mean response \\(\\mu_{Y\\mid X}\\) has a straight-line relationship with \\(X\\) given by a population regression line \\[\\mu_{Y\\mid X} = \\beta_0 + \\beta_1X\\]"
  },
  {
    "objectID": "slides/15-regression-slides.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i-1",
    "href": "slides/15-regression-slides.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i-1",
    "title": "Regression \n",
    "section": "Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n",
    "text": "Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n\n\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) \\[\\begin{align*}\n\\mathrm{Var}(Y_i \\mid X_i) &= \\mathrm{Var}(\\epsilon_i) = \\sigma^2\n\\end{align*}\\] The variance of \\(Y\\) does not depend on \\(X\\).\n\n\n\n\n\n\n\n\n\n\n\nThe variation of Y is the same no matter what value of x is."
  },
  {
    "objectID": "slides/15-regression-slides.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i-2",
    "href": "slides/15-regression-slides.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i-2",
    "title": "Regression \n",
    "section": "Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n",
    "text": "Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n\n\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) \\[\\begin{align*}\nY_i \\mid X_i \\stackrel{indep}{\\sim} N(\\beta_0 + \\beta_1X_i, \\sigma^2)\n\\end{align*}\\] For any fixed value of \\(X_i = x_i\\), the response \\(Y_i\\) varies with \\(N(\\mu_{Y_i\\mid x_i}, \\sigma^2)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nJob: Collect data and estimate the unknown parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\)!"
  },
  {
    "objectID": "slides/15-regression-slides.html#idea-of-fitting",
    "href": "slides/15-regression-slides.html#idea-of-fitting",
    "title": "Regression \n",
    "section": "Idea of Fitting",
    "text": "Idea of Fitting\n\nInterested in \\(\\beta_0\\) and \\(\\beta_1\\) in the following sample regression model: \\[\\begin{align*}\ny_i = \\beta_0 + \\beta_1~x_{i} + \\epsilon_i,\n\\end{align*}\\]  \n\n\n\nOK. once we collect the data, we have a sample regression model.\nHere I use small x and y to represent the collected data.\nGiven this model, we‚Äôre interested in \\(\\beta_0\\) (population parameter for the intercept) and \\(\\beta_1\\) (population parameter for the slope) because once we know \\(\\beta_0\\) and \\(\\beta_1\\), we know the exact shape of \\(f\\) and we know the relationship of \\(y\\) and \\(x\\), and given any value of \\(x\\), we can predict its corresponding value of \\(y\\) using the regression line \\(\\hat{y}_{i} = \\beta_0 + \\beta_1~x_{i}\\).\nBut again the population parameters are unknown to us.\n\\(\\hat{y}_i = E(Y|X_i=x_i)\\)\n\n\nUse sample statistics \\(b_0\\) and \\(b_1\\) computed from our sample data to estimate \\(\\beta_0\\) and \\(\\beta_1\\).\n\\(\\hat{y}_{i} = b_0 + b_1~x_{i}\\) is called fitted value of \\(y_i\\), a point estimate of the mean \\(\\mu_{y|x_i}\\) and \\(y_i\\) itself.\n\n\n\n\\(b_0\\): intercept of the sample regression line\n\n\\(b_1\\): slope of the sample regression line"
  },
  {
    "objectID": "slides/15-regression-slides.html#fitting-a-regression-line-haty-b_0-b_1x",
    "href": "slides/15-regression-slides.html#fitting-a-regression-line-haty-b_0-b_1x",
    "title": "Regression \n",
    "section": "Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\)\n",
    "text": "Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\)\n\nGiven the sample data \\(\\{ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\},\\)\n\nWhich sample regression line is the best?\nWhat are the best estimators \\(b_0\\) and \\(b_1\\) for \\(\\beta_0\\) and \\(\\beta_1\\)?\n\n\n\nNow suppose we already get our sample, and now we are trying to use the sample to get a sample regression line, and hopefully, the sample regression line and the population regression line are alike. look similarly.\nSo people usually ask\nWhich sample regression line is the best?\nWhat are the best estimators \\(b_0\\) and \\(b_1\\) for \\(\\beta_0\\) and \\(\\beta_1\\)?\nAfter all, given the data, we can generate so many different straight lines, and we need a criterion to help us determine which line is the best in some sense. Right!"
  },
  {
    "objectID": "slides/15-regression-slides.html#fitting-a-regression-line-haty-b_0-b_1x-1",
    "href": "slides/15-regression-slides.html#fitting-a-regression-line-haty-b_0-b_1x-1",
    "title": "Regression \n",
    "section": "Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\)\n",
    "text": "Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\)\n\nGiven the sample data \\(\\{ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\},\\)\n\nWhich sample regression line is the best?\nWhat are the best estimators \\(b_0\\) and \\(b_1\\) for \\(\\beta_0\\) and \\(\\beta_1\\)?"
  },
  {
    "objectID": "slides/15-regression-slides.html#what-does-best-mean-ordinary-least-squares-ols",
    "href": "slides/15-regression-slides.html#what-does-best-mean-ordinary-least-squares-ols",
    "title": "Regression \n",
    "section": "What does ‚Äúbest‚Äù mean? Ordinary Least Squares (OLS)",
    "text": "What does ‚Äúbest‚Äù mean? Ordinary Least Squares (OLS)\n\nChoose \\(b_0\\) and \\(b_1\\), or sample regression line \\(b_0 + b_1x\\) that minimizes the sum of squared residuals \\(SS_{res}\\)\n\n\n\\[SS_{res} = e_1^2 + e_2^2 + \\dots + e_n^2 = \\sum_{i = 1}^n e_i^2,\\] where the residual \\(e_i = y_i - \\hat{y}_i = y_i - (b_0 + b_1x_i)\\).\n\n\n\n\nIf \\(b_0\\) and \\(b_1\\) are the best estimators,\n\n\\[\\small{\\begin{align} SS_{res} &= (y_1 - b_0 - b_1x_1)^2 + (y_2 - b_0 - b_1x_2)^2 + \\dots + (y_n - b_0 - b_1x_n)^2\\\\ &= \\sum_{i=1}^n(y_i - b_0 - b_1x_i)^2 \\end{align}}\\] that is the smallest comparing to any other \\(SS_{res} = \\sum_{i=1}^n(y_i - a_0 - a_1x_i)^2\\) that uses another pair of values \\((a_0, a_1) \\ne (b_0, b_1)\\).\n\nNow the question is How do we get \\(b_0\\) and \\(b_1\\) that well estimate \\(\\beta_0\\) and \\(\\beta_1\\)?\nWe choose \\(b_0\\) and \\(b_1\\), or regression line \\(b_0 + b_1x\\) that minimizes the sum of squared residuals.\nIf we define residual as \\(e_i = y_i - \\hat{y}_i\\), then the sum of squared residuals is \\(\\sum_{i = 1}^n e_i^2\\).\nAnd this approach that estimates the population parameters \\(\\beta_0\\) and \\(\\beta_1\\) or the population regression line is called Ordinary Least Squares method."
  },
  {
    "objectID": "slides/15-regression-slides.html#visualizing-residuals",
    "href": "slides/15-regression-slides.html#visualizing-residuals",
    "title": "Regression \n",
    "section": "Visualizing Residuals",
    "text": "Visualizing Residuals\n\nThat‚Äôs see the idea of Ordinary Least Squares visually. Here just showed the data. - Later we will work on the data set together."
  },
  {
    "objectID": "slides/15-regression-slides.html#visualizing-residuals-cont.",
    "href": "slides/15-regression-slides.html#visualizing-residuals-cont.",
    "title": "Regression \n",
    "section": "Visualizing Residuals (cont.)",
    "text": "Visualizing Residuals (cont.)\n\n\nAll right, with the data, this figure also shows the least squares regression line, and the fitted value of \\(y\\) for each \\(x\\) in the training data, which are those red points.\nThe fitted values of y are right on the regression line.\nNow the question is, how do we find this line?\nGiven a line, we can have predicted values of y, right?\nThen what is residual on the plot? The residual will be the difference between the true observation y and the fitted value of y given any value of x.\nSo a residual in the plot will be a vertical bar at the value of x with two ends of the bar \\(y\\) and \\(\\hat{y}\\), right?\n(Show on board)\n(add \\(y_i = b_0+b_1x_i\\) and residual line)"
  },
  {
    "objectID": "slides/15-regression-slides.html#visualizing-residuals-cont.-1",
    "href": "slides/15-regression-slides.html#visualizing-residuals-cont.-1",
    "title": "Regression \n",
    "section": "Visualizing Residuals (cont.)",
    "text": "Visualizing Residuals (cont.)\n\n\nHere shows all the residuals in vertical bars.\nleast squares line is the line such that the sum of all the squared residuals is minimized.\nWhy we square the residuals?\nIt‚Äôs mathematically more convenient.\nSquaring emphasizes larger differences"
  },
  {
    "objectID": "slides/15-regression-slides.html#least-squares-estimates-lse",
    "href": "slides/15-regression-slides.html#least-squares-estimates-lse",
    "title": "Regression \n",
    "section": "Least Squares Estimates (LSE)",
    "text": "Least Squares Estimates (LSE)\n\nThe least squares approach choose \\(b_0\\) and \\(b_1\\) to minimize the \\(SS_{res}\\), i.e.,\n\n\\[(b_0, b_1) = \\arg \\min_{\\alpha_0, \\alpha_1} \\sum_{i=1}^n(y_i - \\alpha_0 - \\alpha_1x_i)^2\\]   \n\n\nMATH 1450/1455 ‚Ä¶\n\\[\\color{red}{b_0 = \\overline{y} - b_1\\overline{x}}\\]  \n\\[\\color{red}{b_1 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^n(x_i - \\overline{x})^2} = r \\frac{\\sqrt{S_{yy}}}{\\sqrt{S_{xx}}}},\\] where \\(S_{xx} = \\sum_{i=1}^n(x_i - \\overline{x})^2\\) and \\(S_{yy} = \\sum_{i=1}^n(y_i - \\overline{y})^2\\)\n\n\n\nWhat can we learn from the formula of \\(b_0\\) and \\(b_1\\)?\n\n\nThe LS regression line passes through the centroid.\n\n\\(b_1\\) is kinda like a scaled covariance of X and Y."
  },
  {
    "objectID": "slides/15-regression-slides.html#r-lab-mpg-data",
    "href": "slides/15-regression-slides.html#r-lab-mpg-data",
    "title": "Regression \n",
    "section": "\nR Lab: mpg Data",
    "text": "R Lab: mpg Data\n\nlibrary(ggplot2)  ## use data mpg in ggplot2 package\nmpg\n\n# A tibble: 234 √ó 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto‚Ä¶ f        18    29 p     comp‚Ä¶\n 2 audi         a4           1.8  1999     4 manu‚Ä¶ f        21    29 p     comp‚Ä¶\n 3 audi         a4           2    2008     4 manu‚Ä¶ f        20    31 p     comp‚Ä¶\n 4 audi         a4           2    2008     4 auto‚Ä¶ f        21    30 p     comp‚Ä¶\n 5 audi         a4           2.8  1999     6 auto‚Ä¶ f        16    26 p     comp‚Ä¶\n 6 audi         a4           2.8  1999     6 manu‚Ä¶ f        18    26 p     comp‚Ä¶\n 7 audi         a4           3.1  2008     6 auto‚Ä¶ f        18    27 p     comp‚Ä¶\n 8 audi         a4 quattro   1.8  1999     4 manu‚Ä¶ 4        18    26 p     comp‚Ä¶\n 9 audi         a4 quattro   1.8  1999     4 auto‚Ä¶ 4        16    25 p     comp‚Ä¶\n10 audi         a4 quattro   2    2008     4 manu‚Ä¶ 4        20    28 p     comp‚Ä¶\n# ‚Ñπ 224 more rows"
  },
  {
    "objectID": "slides/15-regression-slides.html#r-lab-highway-mpg-hwy-vs.-displacement-displ",
    "href": "slides/15-regression-slides.html#r-lab-highway-mpg-hwy-vs.-displacement-displ",
    "title": "Regression \n",
    "section": "\nR Lab: Highway MPG hwy vs.¬†Displacement displ\n",
    "text": "R Lab: Highway MPG hwy vs.¬†Displacement displ\n\n\nplot(x = mpg$displ, y = mpg$hwy,\n     las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")\n\n\n\nUsually, when we get data, the first thing is plotting your data, doing some exploratory data analysis, and see if there is useful information out there that may help us build an appropriate model.\nTo make a scatter plot, we can simply use the plot() function, and put displ in x axis and hwy in the y axis.\nTo grab a variable or a column of a data frame, we can use the dollar sign, the same way as a list extracting an element.\nThe rest of arguments are optional, they are just used to decorate your plot. You can generate a plot without specifying any of them.\nAnd because it seems to a linear trend downwards. We could fit a simple linear regression to the data. Right"
  },
  {
    "objectID": "slides/15-regression-slides.html#r-lab-fit-simple-linear-regression",
    "href": "slides/15-regression-slides.html#r-lab-fit-simple-linear-regression",
    "title": "Regression \n",
    "section": "\nR Lab: Fit Simple Linear Regression",
    "text": "R Lab: Fit Simple Linear Regression\n\n\n\n\nreg_fit &lt;- lm(formula = hwy ~ displ, \n              data = mpg)\nreg_fit\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nCoefficients:\n(Intercept)        displ  \n      35.70        -3.53  \n\ntypeof(reg_fit)\n\n[1] \"list\"\n\n\n\n\n## all elements in reg_fit\nnames(reg_fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n## use $ to extract an element of a list\nreg_fit$coefficients\n\n(Intercept)       displ \n      35.70       -3.53 \n\n\n\n\n\n\n\\(\\widehat{hwy}_{i} = b_0 + b_1 \\times displ_{i} =  35.7 - 3.5 \\times displ_{i}\\)\n\\(b_1\\): For one unit (litre) increase of the displacement, we expect the highway MPG to be decreased, on average, by 3.5.\n\n\nIn R, to fit a linear regression model, it cannot be easier.\nWe just need to use the command lm(). We put a formula in the function, y ~ x, and let R know which data set you are considering.\nThat‚Äôs it. And I save the fitted result in an object called reg_fit.\nYou can see lm() function returns a list.\nWe can grab the coefficient estimates this way.\nSo your sample regression line is like this."
  },
  {
    "objectID": "slides/15-regression-slides.html#r-lab-fitted-values-of-y",
    "href": "slides/15-regression-slides.html#r-lab-fitted-values-of-y",
    "title": "Regression \n",
    "section": "\nR Lab Fitted Values of \\(y\\)\n",
    "text": "R Lab Fitted Values of \\(y\\)\n\n\n## the first 5 observed response value y\nmpg$hwy[1:5]\n\n[1] 29 29 31 30 26\n\n## the first 5 fitted value y_hat\nhead(reg_fit$fitted.values, 5)\n\n   1    2    3    4    5 \n29.3 29.3 28.6 28.6 25.8 \n\n## the first 5 predictor value x\nmpg$displ[1:5]\n\n[1] 1.8 1.8 2.0 2.0 2.8\n\nlength(reg_fit$fitted.values)\n\n[1] 234"
  },
  {
    "objectID": "slides/15-regression-slides.html#r-lab-add-a-regression-line",
    "href": "slides/15-regression-slides.html#r-lab-add-a-regression-line",
    "title": "Regression \n",
    "section": "\nR Lab Add a Regression Line",
    "text": "R Lab Add a Regression Line\n\nplot(x = mpg$displ, y = mpg$hwy, las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")\nabline(reg_fit, col = \"#FFCC00\", lwd = 3)"
  },
  {
    "objectID": "slides/15-regression-slides.html#estimation-for-sigma2",
    "href": "slides/15-regression-slides.html#estimation-for-sigma2",
    "title": "Regression \n",
    "section": "Estimation for \\(\\sigma^2\\)\n",
    "text": "Estimation for \\(\\sigma^2\\)\n\n\n\n\nThink of \\(\\sigma^2\\) as variance around the line or the mean square (prediction) error.\nThe estimate of \\(\\sigma^2\\) is the mean square residual \\(MS_{res}\\):\n\n\\[\\hat{\\sigma}^2 = MS_{res} = \\frac{SS_{res}}{n-2} = \\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}\\]\n\n\n\\(MS_{res}\\) is often shown in computer output as \\(\\texttt{MS(Error)}\\) or \\(\\texttt{MS(Residual)}\\). \n\n\n\n\n\n\n\n\n\n\n\n\n\nSo we are done estimation for \\(beta\\). Let‚Äôs talk about the estimation of \\(\\sigma^2\\).\nThe estimate of \\(\\sigma^2\\), denoted as \\(\\hat{\\sigma}^2\\) or \\(s_{\\epsilon}^2\\), based on the sample data is residual sum of squares divided by \\(n-2\\), the degrees of freedom\nIt can be shown that \\(E(SS_{res}) = (n-2)\\sigma^2\\). That is, \\(\\hat{\\sigma}^2\\) is an unbiased estimator for \\(\\sigma^2\\). üëç"
  },
  {
    "objectID": "slides/15-regression-slides.html#r-lab-standard-error-of-regression",
    "href": "slides/15-regression-slides.html#r-lab-standard-error-of-regression",
    "title": "Regression \n",
    "section": "\nR Lab Standard Error of Regression",
    "text": "R Lab Standard Error of Regression\n\n(summ_reg_fit &lt;- summary(reg_fit))\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.104 -2.165 -0.224  2.059 15.010 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   35.698      0.720    49.5   &lt;2e-16 ***\ndispl         -3.531      0.195   -18.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.84 on 232 degrees of freedom\nMultiple R-squared:  0.587, Adjusted R-squared:  0.585 \nF-statistic:  329 on 1 and 232 DF,  p-value: &lt;2e-16\n\n\n\nHow do we get sigma_hat in R?\nWell you could grab residuals and df from lm fit result reg_fit, and use the formula to calculate the sigma_hat. sqrt(sum(reg_fit\\(residuals^2) / reg_fit\\)df.residual)\nIf you want R to do the calculation for you, you can get the summary of the fitted result reg_fit.\nThen the sigma hat is right here."
  },
  {
    "objectID": "slides/15-regression-slides.html#r-lab-standard-error-of-regression-1",
    "href": "slides/15-regression-slides.html#r-lab-standard-error-of-regression-1",
    "title": "Regression \n",
    "section": "\nR Lab Standard Error of Regression",
    "text": "R Lab Standard Error of Regression\n\n# lots of fitted information saved in summary(reg_fit)!\nnames(summ_reg_fit)\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n# residual standard error (sigma_hat)\nsumm_reg_fit$sigma\n\n[1] 3.84\n\n# from reg_fit\nsqrt(sum(reg_fit$residuals^2) / reg_fit$df.residual)\n\n[1] 3.84\n\n\n\nHere shows the fitted information saved in summary(reg_fit)\nYou see sigma is right here. So you just extract that value if you need."
  },
  {
    "objectID": "slides/15-regression-slides.html#confidence-interval-for-beta_1",
    "href": "slides/15-regression-slides.html#confidence-interval-for-beta_1",
    "title": "Regression \n",
    "section": "Confidence Interval for \\(\\beta_1\\)\n",
    "text": "Confidence Interval for \\(\\beta_1\\)\n\n\n\\(\\frac{b_1 - \\beta_1}{\\sqrt{\\hat{\\sigma}^2/S_{xx}}} \\sim t_{n-2}\\)\n\n\n\n\n\\((1-\\alpha)100\\%\\) CI for \\(\\beta_1\\) is \\(b_1 \\pm t_{\\alpha/2, n-2}\\sqrt{\\hat{\\sigma}^2/S_{xx}}\\)\n\n\n\n\n\nconfint(reg_fit, level = 0.95)\n\n            2.5 % 97.5 %\n(Intercept) 34.28  37.12\ndispl       -3.91  -3.15"
  },
  {
    "objectID": "slides/15-regression-slides.html#hypothesis-testing-beta_1",
    "href": "slides/15-regression-slides.html#hypothesis-testing-beta_1",
    "title": "Regression \n",
    "section": "Hypothesis Testing: \\(\\beta_1\\)\n",
    "text": "Hypothesis Testing: \\(\\beta_1\\)\n\n\n \\(H_0: \\beta_1 = \\beta_1^0 \\quad H_1: \\beta_1 \\ne \\beta_1^0\\)  \nTest statistic: Under \\(H_0\\),\n\n\\[t_{test} = \\frac{b_1 - \\color{red}{\\beta_1^0}}{\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}} \\sim t_{n-2}\\]\n\nReject \\(H_0\\) in favor of \\(H_1\\) if\n\n\\(|t_{test}| &gt; t_{\\alpha/2, \\, n-2}\\)\n\\(\\text{p-value} = 2P(t_{n-2} &gt; |t_{test}|) &lt; \\alpha\\)\n\n\n\n\nin addition to estimation, we may be interested in testing.\nTo do the testing on \\(\\beta_1\\), the testing procedure is basically the same as the procedure for population mean \\(\\mu\\) we reviewed last week.\nUsually \\(\\beta_1^0 = 0\\), but we may be interested in other values.\ngood growth \\(\\beta_1 &gt; 2\\)"
  },
  {
    "objectID": "slides/15-regression-slides.html#r-lab-testing-on-beta_0-and-beta_1",
    "href": "slides/15-regression-slides.html#r-lab-testing-on-beta_0-and-beta_1",
    "title": "Regression \n",
    "section": "\nR Lab Testing on \\(\\beta_0\\) and \\(\\beta_1\\)\n",
    "text": "R Lab Testing on \\(\\beta_0\\) and \\(\\beta_1\\)\n\n\nsumm_reg_fit$coefficients\n\n            Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept)    35.70      0.720    49.6 2.12e-125\ndispl          -3.53      0.195   -18.2  2.04e-46\n\n\n\nTesting \\(H_0: \\beta_0 = 0\\) and \\(H_0: \\beta_1 = 0\\)"
  },
  {
    "objectID": "slides/15-regression-slides.html#interpretation-of-testing-results",
    "href": "slides/15-regression-slides.html#interpretation-of-testing-results",
    "title": "Regression \n",
    "section": "Interpretation of Testing Results",
    "text": "Interpretation of Testing Results\n\n \\(H_0: \\beta_1 = 0 \\quad H_1: \\beta_1 \\ne 0\\) \nFailing to reject \\(H_0: \\beta_1 = 0\\) implies there is no linear relationship between \\(Y\\) and \\(X\\).\n\n\nSo back to the testing on \\(\\beta_1\\).\nIf we do not reject \\(H_0\\), it implies there is no linear relationship between \\(Y\\) and \\(X\\). Right? Because \\(\\beta_1\\), the slope of the regression line is pretty much 0.\n\n\n\n\n\n\n\n\n\n\nBut it actually has two cases. One \\(x\\) and \\(y\\) may have no relationship at all, or they don‚Äôt have linear relationship but have another kind of relationship, like quadratic.\nSo we have to be careful when we interpret the testing result. OK.\n\n\n\n\nIf we reject \\(H_0: \\beta_1 = 0\\), does it mean \\(X\\) and \\(Y\\) are linearly related?\n\nWe have to be more careful and precise on what we are claiming."
  },
  {
    "objectID": "slides/15-regression-slides.html#test-of-significance-of-regression",
    "href": "slides/15-regression-slides.html#test-of-significance-of-regression",
    "title": "Regression \n",
    "section": "Test of Significance of Regression",
    "text": "Test of Significance of Regression\n\n\nRejecting \\(H_0: \\beta_1 = 0\\) could mean\n\nthe straight-line model is adequate\nbetter results could be obtained with a more complicated model\n\n\n\n\n\n\nRejecting \\(H_0: \\beta_1 = 0\\) could mean\n\nthe straight-line model is adequate\nbetter results could be obtained with a more complicated model even though there is a linear effect of \\(x\\)"
  },
  {
    "objectID": "slides/15-regression-slides.html#x---y-relationship-explains-some-deviation",
    "href": "slides/15-regression-slides.html#x---y-relationship-explains-some-deviation",
    "title": "Regression \n",
    "section": "\n\\(X\\) - \\(Y\\) Relationship Explains Some Deviation",
    "text": "\\(X\\) - \\(Y\\) Relationship Explains Some Deviation\n\nSuppose we only have data \\(Y\\) and have no information about \\(X\\) and relationship between \\(X\\) and \\(Y\\). How do we predict a value of \\(Y\\)?\n\n\nFor example, suppose we only have MPG information for the sample of cars and have no info about displacement and their relationship. How do we predict a car‚Äôs MPG?\n\n\nOur best guess would be \\(\\overline{y}\\) if the data have no pattern, i.e., \\(\\hat{y}_i = \\overline{y}\\).\nTreat \\(X\\) and \\(Y\\) as uncorrelated.\nThe (total) deviation from the mean is \\((y_i - \\overline{y})\\)\n\n\n\n\nIf \\(X\\) and \\(Y\\) are linearly related, fitting a linear regression model helps us predict the value of \\(Y\\) when the value of \\(X\\) is provided.\n\\(\\hat{y}_i = b_0 + b_1x_i\\) is closer to \\(y_i\\) than \\(\\overline{y}\\).\nThe regression model explains some deviation of \\(y\\).\n\n\nWhen we have no information about the relationship between \\(X\\) and \\(Y\\), to predict a value of \\(y\\) using the same value given any value of \\(x\\).\nWhen \\(X\\) and \\(Y\\) are uncorrelated, the regression model is not helping predict \\(Y\\) because \\(X\\) provides no information about \\(Y\\).\nIt means \\(b_1 = 0\\) and \\(\\hat{y}_i = \\bar{y}\\) for all values of \\(X\\).\nThe result is the same as the one when we only have data of \\(Y\\).\nThis prediction deviation \\((y_i - \\overline{y})\\) is generally the biggest deviation we can have when we have no information about how y varies or how y is affected by others."
  },
  {
    "objectID": "slides/15-regression-slides.html#partition-of-deviation",
    "href": "slides/15-regression-slides.html#partition-of-deviation",
    "title": "Regression \n",
    "section": "Partition of Deviation",
    "text": "Partition of Deviation\n\nTotal deviation = Deviation explained by regression + Unexplained deviation\n\\((y_i - \\overline{y}) = (\\hat{y}_i - \\overline{y}) + (y_i - \\hat{y}_i)\\)\n\\((19 - 9) = (13 - 9) + (19 - 13)\\)\n\n\n\nIf \\(X\\) and \\(Y\\) are actually linearly related, fitting a linear regression helps us predict the value of \\(Y\\) when the value of \\(X\\) is provided.\nIt means that \\(\\hat{y}_i = b_0 + b_1x_i\\) is closer to \\(y_i\\) than \\(\\overline{y}\\).\nIntuitively, the regression model explains some deviation of \\(y\\) by the predictor \\(X\\).\n\\(X\\) does contain some useful and valuable information for predicting value of \\(Y\\).\nSo, we can actually Partition the total deviation into two parts.\nPartition of deviation: Total deviation = Explained deviation by regression + Unexplained deviation\n\\((y_i - \\overline{y}) = (\\hat{y}_i - \\overline{y}) + (y_i - \\hat{y}_i)\\)"
  },
  {
    "objectID": "slides/15-regression-slides.html#sum-of-squares-ss",
    "href": "slides/15-regression-slides.html#sum-of-squares-ss",
    "title": "Regression \n",
    "section": "Sum of Squares (SS)",
    "text": "Sum of Squares (SS)\n\n\\(\\sum_{i=1}^n(y_i - \\overline{y})^2 = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 + \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\)\nTotal SS \\((SS_T)\\) = Regression SS \\((SS_R)\\) + Residual SS \\((SS_{res})\\)\n\\(df_T = df_R + df_{res}\\)\n\\(\\color{blue}{(n-1) = 1 +(n-2)}\\)\n\n\nTotal variability = variability explained by regression + unexplained variability\n\\(\\sum_{i=1}^n(y_i - \\overline{y})^2 = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 + \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 + 2\\sum_{i=1}^n(\\hat{y}_i - \\overline{y})(y_i - \\hat{y}_i)\\)\n\n\n \\(df_T = n - 1\\): lose 1 df with constraint \\(\\sum_{i=1}^n(y_i - \\overline{y}) = 0\\)\n\n\n \\(df_R = 1\\): all \\(\\hat{y}_i\\) are on the regression line with 2 dfs (intercept and slope), but with constraint \\(\\sum_{i=1}^n(\\hat{y}_i - \\overline{y}) = 0\\)\n\n\n \\(df_{res} = n - 2\\): lose 2 dfs because \\(\\beta_0\\) and \\(\\beta_1\\) are estimated by \\(b_0\\) and \\(b_1\\), which are linear combo of \\(y_i\\)\n\ndegrees of freedom is the equivalent number of values in the calculation of a statistic that are free to vary.\n\n\\(SS_R = b_1S_{xy}\\) or \\(SS_{res} = SS_T - b_1S_{xy}\\)."
  },
  {
    "objectID": "slides/15-regression-slides.html#anova-for-testing-significance-of-regression",
    "href": "slides/15-regression-slides.html#anova-for-testing-significance-of-regression",
    "title": "Regression \n",
    "section": "ANOVA for Testing Significance of Regression",
    "text": "ANOVA for Testing Significance of Regression\n\n\nA larger value of \\(F_{test}\\) indicates that regression is significant.\n\nReject \\(H_0\\) if\n\n\\(F_{test} &gt; F_{\\alpha, 1, n-2}\\)\n\n\\(\\text{p-value} = P(F_{1, n-2} &gt; F_{test}) &lt; \\alpha\\).\n\n\nThe ANOVA is designed to test \\(H_0\\) that all predictors have no value in predicting \\(y\\).\nIn SLR, the \\(F\\)-test of ANOVA gives the same result as a two-sided \\(t\\)-test of \\(H_0: \\beta_1=0\\).\n\n\nANOVA is used for testing significance of regression.\nIt is testing if any predictors or regressors have explanatory power for predicting y.\nIn other words, it is testing if the whole regression model is useful or not.\nHere is the ANOVA table.\n\\(H_0: \\beta_1 = 0\\)\nA larger value of \\(F_{test}\\) indicates that regression is significant.\nReject \\(H_0\\) in favor of \\(H_1\\) if \\(F_{test} &gt; F_{\\alpha, 1, n-2}\\) or \\(\\text{p-value} = P(F_{1, n-2} &gt; F_{test}) &lt; \\alpha\\).\nThe ANOVA is designed to test \\(H_0\\) that all predictors have no value in predicting \\(y\\).\nIn SLR, there is only one predictor, and hence the \\(F\\)-test of ANOVA gives the same result as a two-sided \\(t\\)-test of \\(H_0: \\beta_1=0\\).\n\n\nIf we have \\(k \\ge 2\\) predictors, the \\(F\\)-test is testing \\(H_0: \\beta_1=\\beta_2=\\cdots=\\beta_k=0\\)."
  },
  {
    "objectID": "slides/15-regression-slides.html#r-lab-anova-table",
    "href": "slides/15-regression-slides.html#r-lab-anova-table",
    "title": "Regression \n",
    "section": "\nR Lab ANOVA Table",
    "text": "R Lab ANOVA Table\n\nanova(reg_fit)\n\nAnalysis of Variance Table\n\nResponse: hwy\n           Df Sum Sq Mean Sq F value Pr(&gt;F)    \ndispl       1   4848    4848     329 &lt;2e-16 ***\nResiduals 232   3414      15                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nFor \\(H_0: \\beta_1 = 0\\) in SLR, \\(t_{test}^2 = F_{test}\\).\n\n\n\n            Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept)    35.70      0.720    49.6 2.12e-125\ndispl          -3.53      0.195   -18.2  2.04e-46\n\n\n[1] 329\n\n\n\nFor \\(H_0: \\beta_1 = 0\\), \\(t_{test}^2 = \\left(\\frac{b_1}{\\sqrt{MS_{res}/S_{xx}}}\\right)^2 = \\frac{b_1^2S_{xx}}{MS_{res}} = \\frac{b_1S_{xy}}{MS_{res}} = \\frac{MS_R}{MS_{res}} = F_{test}\\)"
  },
  {
    "objectID": "slides/15-regression-slides.html#coefficient-of-determination",
    "href": "slides/15-regression-slides.html#coefficient-of-determination",
    "title": "Regression \n",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\n\nThe coefficient of determination \\((R^2)\\) is the proportion of the variation in \\(y\\) that is explained by the regression model:\n\n\\[R^2 = \\frac{SS_R}{SS_T} =\\frac{SS_T - SS_{res}}{SS_T} = 1 - \\frac{SS_{res}}{SS_T}\\]\n\n\\(R^2\\) as the proportionate reduction of total variation associated with the use of \\(X\\).\n(a) \\(\\hat{y}_i = y_i\\) and \\(\\small SS_{res} =  \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = 0\\). (b) \\(\\hat{y}_i = \\overline{y}\\) and \\(\\small SS_R = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2  = 0\\).\n\n\n\n\ncoefficient of determination can be used to measure the quality of our regression or the explanatory power of regressors.\nHere (a) and (b) are two extreme cases.\nIn (a), the fitted value = the true observation. So the regression model explains all the variation in \\(Y\\), and hence \\(R^2 = 1\\).\nIn (b), the fitted value = mean of y as if we don‚Äôt have information about \\(x\\) or \\(x\\) is totally useless in predicting \\(Y\\). In this case, the regression model explains no the variation in \\(Y\\), and all variation remain unexplained. So \\(R^2 = 0\\)."
  },
  {
    "objectID": "slides/15-regression-slides.html#r-lab-r2",
    "href": "slides/15-regression-slides.html#r-lab-r2",
    "title": "Regression \n",
    "section": "\nR Lab \\(R^2\\)\n",
    "text": "R Lab \\(R^2\\)\n\n\n\nsumm_reg_fit\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.104 -2.165 -0.224  2.059 15.010 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   35.698      0.720    49.5   &lt;2e-16 ***\ndispl         -3.531      0.195   -18.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.84 on 232 degrees of freedom\nMultiple R-squared:  0.587, Adjusted R-squared:  0.585 \nF-statistic:  329 on 1 and 232 DF,  p-value: &lt;2e-16\n\n\n\n\nsumm_reg_fit$r.squared\n\n[1] 0.587"
  },
  {
    "objectID": "slides/15-regression-slides.html#predicting-the-mean-response",
    "href": "slides/15-regression-slides.html#predicting-the-mean-response",
    "title": "Regression \n",
    "section": "Predicting the Mean Response",
    "text": "Predicting the Mean Response\n\n\n\nWith predictor value \\(x = x_0\\), we want to estimate the mean response \\(\\mu_{y|x_0} = \\beta_0 + \\beta_1 x_0\\).\n\n The mean highway MPG \\(\\mu_{y|x_0}\\) when displacement is \\(x = x_0 = 5.5\\). \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf \\(x_0\\) is within the range of \\(x\\), an unbiased point estimator for \\(\\mu_{y|x_0}\\) is\n\n\\[\\hat{\\mu}_{y | x_0} = b_0 + b_1 x_0\\]\n\n\n\n\n\nThe \\((1-\\alpha)100\\%\\) CI for \\(\\mu_{y|x_0}\\) is\n\n\\[\\hat{\\mu}_{y | x_0} \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}\\]\n\n\n\n\nDoes the length of the CI for \\(\\mu_{y|x_0}\\) stay the same at any location of \\(x_0\\)?\n\n\n\n\n\nWith predictor value \\(x = x_0\\), we want to estimate the mean response \\(E(y\\mid x_0) = \\mu_{y|x_0} = \\beta_0 + \\beta_1 x_0\\).\n\n The mean highway MPG \\(E(y \\mid x_0)\\) when displacement is \\(x = x_0 = 5.5\\). \n\n\nIf \\(x_0\\) is within the range of \\(x\\), an unbiased point estimator for \\(E(y\\mid x_0)\\) is \\[\\widehat{E(y\\mid x_0)} = \\hat{\\mu}_{y | x_0} = b_0 + b_1 x_0\\]\nThe \\((1-\\alpha)100\\%\\) CI for \\(E(y\\mid x_0)\\) is \\(\\boxed{\\hat{\\mu}_{y | x_0} \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}}\\)."
  },
  {
    "objectID": "slides/15-regression-slides.html#section-4",
    "href": "slides/15-regression-slides.html#section-4",
    "title": "Regression \n",
    "section": "",
    "text": "## CI for the mean response\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"confidence\", level = 0.95)\n\n   fit  lwr  upr\n1 16.3 15.4 17.2"
  },
  {
    "objectID": "slides/15-regression-slides.html#predicting-new-observations",
    "href": "slides/15-regression-slides.html#predicting-new-observations",
    "title": "Regression \n",
    "section": "Predicting New Observations",
    "text": "Predicting New Observations\n\nPredict the value of a new observation \\(y_0\\) with \\(x = x_0\\).\n\n The highway MPG of a car \\(y_0(x_0)\\) when its displacement is \\(x = x_0 = 5.5\\). \n\n\n\n\n\nAn unbiased point estimator for \\(y_0(x_0)\\) is\n\n\\[\\hat{y}_0(x_0) = b_0 + b_1 x_0\\]\n\n\n\nThe \\((1-\\alpha)100\\%\\) prediction interval (PI) for \\(y_0(x_0)\\) is\n\n\\[\\hat{y}_0 \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{{\\color{red}{1}}+ \\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}\\]\n\nWhat is the difference between CI for \\(\\mu_{y | x_0}\\) and PI for \\(y_0(x_0)\\)?\n\n\n\n\n\nThe PI is wider as it includes the uncertainty about \\(b_0\\), \\(b_1\\) as well as \\(y_0\\) due to error \\(\\epsilon\\).\n\n\nPredict the value of a new observation \\(y_0\\) corresponding to a specified value of predictor \\(x = x_0\\).\n\n The highway MPG of a car \\(y_0(x_0)\\) when its displacement is \\(x = x_0 = 5.5\\). \n\n\nAn unbiased point estimator for \\(y_0(x_0)\\) is \\[\\hat{y}_0(x_0) = b_0 + b_1 x_0\\]"
  },
  {
    "objectID": "slides/15-regression-slides.html#r-lab-prediction",
    "href": "slides/15-regression-slides.html#r-lab-prediction",
    "title": "Regression \n",
    "section": "\nR Lab Prediction",
    "text": "R Lab Prediction\n\n\n## CI for the mean response\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"confidence\", level = 0.95)\n\n   fit  lwr  upr\n1 16.3 15.4 17.2\n\n## PI for the new observation\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"predict\", level = 0.95)\n\n   fit  lwr  upr\n1 16.3 8.67 23.9\n\n\n\n\n\nIf I can only use one point or value to predict \\(y_0\\), the best guess is the predicted value on the regression line.\nAfter all, values around the lines would more likely to be drawn based on our model.\nHere you can understand why uncertainty quantification is important.\nYes, we can predict an new observation value, but the prediction quality is gonna be bad because we are predict a random variable, not a constant, and \\(y_0\\) can vary a lot around the regression line.\nAnd prediction interval gives us an idea of how good or how bad our prediction is."
  },
  {
    "objectID": "slides/15-regression-slides.html#section-5",
    "href": "slides/15-regression-slides.html#section-5",
    "title": "Regression \n",
    "section": "",
    "text": "CI is the shortest when \\(x = \\bar{x}\\). (Also PI)\nPI length looks the same along with \\(x\\) because the \\(\\sigma^2\\) dominates the uncertainty, comparing to the uncertainty about \\(b_0\\) and \\(b_1\\)."
  },
  {
    "objectID": "slides/15-regression.html#what-is-regression",
    "href": "slides/15-regression.html#what-is-regression",
    "title": "Regression \n",
    "section": "What is Regression",
    "text": "What is Regression\n\nRegression models the relationship between one or more numerical/categorical response variables \\((Y)\\) and one or more numerical/categorical explanatory variables \\((X)\\).\nA regression function \\(f(X)\\) describes how a response variable \\(Y\\), on average, changes as an explanatory variable \\(X\\) changes.\n\n\n\nExamples:\n\n college GPA \\((Y)\\) vs.¬†ACT/SAT score \\((X)\\)\n sales \\((Y)\\) vs.¬†advertising expenditure \\((X)\\)\n crime rate \\((Y)\\) vs.¬†median income level \\((X)\\) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression models the relationship between a numerical response variable (dependent variable) and one or more (numerical/categorical) explanatory variables (independent variables, predictors, or covariates).\nA regression function describes how a response variable \\(Y\\) changes as an explanatory variable \\(X\\) changes.\nThe true relationship between \\(X\\) and \\(Y\\), the regression function, is unknown.\nThe goal of regression is to estimate the regression function and use it to predict value of \\(Y\\) given a value of \\(X\\).\nExamples:\n\n Relationship between college GPA (\\(Y\\)) and ACT/SAT scores (\\(X\\))\n Relationship between sales (\\(Y\\)) and advertising expenditure (\\(X\\))\n Relationship between crime rate (\\(Y\\)) and median income level (\\(X\\))"
  },
  {
    "objectID": "slides/15-regression.html#unknown-regression-function",
    "href": "slides/15-regression.html#unknown-regression-function",
    "title": "Regression \n",
    "section": "Unknown Regression Function",
    "text": "Unknown Regression Function\n\nThe true relationship between \\(X\\) and the mean of \\(Y\\), the regression function \\(f(X)\\), is unknown.\nThe collected data \\((x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\) is all we know and have.\nGoal: estimate \\(f(X)\\) from our data and use it to predict value of \\(Y\\) given any value of \\(X\\)."
  },
  {
    "objectID": "slides/15-regression.html#simple-linear-regression",
    "href": "slides/15-regression.html#simple-linear-regression",
    "title": "Regression \n",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nStart with simple linear regression:\n\n\nOnly one predictor \\(X\\) (known and constant) and one response variable \\(Y\\)\n\nthe regression function used for predicting \\(Y\\) is a linear function.\nuse a regression line in a X-Y plane to predict the value of \\(Y\\) for a given value of \\(X = x\\).\n\n\n\n. . .\n\n\nMath review: A linear function \\(y = f(x) = \\beta_0 + \\beta_1 x\\) represents a straight line\n\n\\(\\beta_1\\): slope, the amount by which \\(y\\) changes when \\(x\\) increases by one unit.\n\\(\\beta_0\\): intercept, the value of \\(y\\) when \\(x = 0\\).\nThe linearity assumption: \\(\\beta_1\\) does not change as \\(x\\) changes."
  },
  {
    "objectID": "slides/15-regression.html#sample-data-relationship-between-x-and-y",
    "href": "slides/15-regression.html#sample-data-relationship-between-x-and-y",
    "title": "Regression \n",
    "section": "Sample Data: Relationship Between X and Y",
    "text": "Sample Data: Relationship Between X and Y\n\nReal data \\((x_i, y_i), i = 1, 2, \\dots, n\\) do not form a perfect straight line!\n\\(y_i = \\beta_0+\\beta_1x_i + \\color{red}{\\epsilon_i}\\)"
  },
  {
    "objectID": "slides/15-regression.html#section-1",
    "href": "slides/15-regression.html#section-1",
    "title": "Regression \n",
    "section": "",
    "text": "When we collect our data, at any given level of \\(X = x\\), \\(y\\) is assumed being drawn from a normal distribution (for inference purpose).\nIts value varies around and will not be exactly equal to its mean \\(\\mu_y\\)."
  },
  {
    "objectID": "slides/15-regression.html#section-2",
    "href": "slides/15-regression.html#section-2",
    "title": "Regression \n",
    "section": "",
    "text": "When we collect our data, at any given level of \\(X = x\\), \\(y\\) is assumed being drawn from a normal distribution (for inference purpose).\nIts value varies around and will not be exactly equal to its mean \\(\\mu_y\\)."
  },
  {
    "objectID": "slides/15-regression.html#section-3",
    "href": "slides/15-regression.html#section-3",
    "title": "Regression \n",
    "section": "",
    "text": "The mean of \\(Y\\) and \\(X\\) form a straight line."
  },
  {
    "objectID": "slides/15-regression.html#simple-linear-regression-model-population",
    "href": "slides/15-regression.html#simple-linear-regression-model-population",
    "title": "Regression \n",
    "section": "Simple Linear Regression Model (Population)",
    "text": "Simple Linear Regression Model (Population)\nFor the \\(i\\)-th measurement in the target population,\n\\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\]\n\n\\(Y_i\\): the \\(i\\)-th value of the response (random) variable.\n\\(X_i\\): the \\(i\\)-th known fixed value of the predictor.\n\\(\\epsilon_i\\): the \\(i\\)-th random error with assumption \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\).\n\\(\\beta_0\\) and \\(\\beta_1\\) are model coefficients.\n\\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) are fixed unknown parameters to be estimated from the sample data once we collect them.\n\n\n.question[ What is the distribution of \\(Y_i\\) given a value of \\(X = x\\)?"
  },
  {
    "objectID": "slides/15-regression.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i",
    "href": "slides/15-regression.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i",
    "title": "Regression \n",
    "section": "Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n",
    "text": "Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n\n\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\)    \n\\[\\begin{align*}\n\\mu_{Y_i \\mid X_i} = \\beta_0 + \\beta_1X_i\n\\end{align*}\\]\nThe mean response \\(\\mu_{Y\\mid X}\\) has a straight-line relationship with \\(X\\) given by a population regression line \\[\\mu_{Y\\mid X} = \\beta_0 + \\beta_1X\\]"
  },
  {
    "objectID": "slides/15-regression.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i-1",
    "href": "slides/15-regression.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i-1",
    "title": "Regression \n",
    "section": "Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n",
    "text": "Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n\n\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) \\[\\begin{align*}\n\\mathrm{Var}(Y_i \\mid X_i) &= \\mathrm{Var}(\\epsilon_i) = \\sigma^2\n\\end{align*}\\] The variance of \\(Y\\) does not depend on \\(X\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe variation of Y is the same no matter what value of x is."
  },
  {
    "objectID": "slides/15-regression.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i-2",
    "href": "slides/15-regression.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i-2",
    "title": "Regression \n",
    "section": "Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n",
    "text": "Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n\n\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) \\[\\begin{align*}\nY_i \\mid X_i \\stackrel{indep}{\\sim} N(\\beta_0 + \\beta_1X_i, \\sigma^2)\n\\end{align*}\\] For any fixed value of \\(X_i = x_i\\), the response \\(Y_i\\) varies with \\(N(\\mu_{Y_i\\mid x_i}, \\sigma^2)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\n\nJob: Collect data and estimate the unknown parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\)!"
  },
  {
    "objectID": "slides/15-regression.html#idea-of-fitting",
    "href": "slides/15-regression.html#idea-of-fitting",
    "title": "Regression \n",
    "section": "Idea of Fitting",
    "text": "Idea of Fitting\n\nInterested in \\(\\beta_0\\) and \\(\\beta_1\\) in the following sample regression model: \\[\\begin{align*}\ny_i = \\beta_0 + \\beta_1~x_{i} + \\epsilon_i,\n\\end{align*}\\]  \n\n\n\n\nOK. once we collect the data, we have a sample regression model.\nHere I use small x and y to represent the collected data.\nGiven this model, we‚Äôre interested in \\(\\beta_0\\) (population parameter for the intercept) and \\(\\beta_1\\) (population parameter for the slope) because once we know \\(\\beta_0\\) and \\(\\beta_1\\), we know the exact shape of \\(f\\) and we know the relationship of \\(y\\) and \\(x\\), and given any value of \\(x\\), we can predict its corresponding value of \\(y\\) using the regression line \\(\\hat{y}_{i} = \\beta_0 + \\beta_1~x_{i}\\).\nBut again the population parameters are unknown to us.\n\\(\\hat{y}_i = E(Y|X_i=x_i)\\)\n\n\n. . .\n\nUse sample statistics \\(b_0\\) and \\(b_1\\) computed from our sample data to estimate \\(\\beta_0\\) and \\(\\beta_1\\).\n\\(\\hat{y}_{i} = b_0 + b_1~x_{i}\\) is called fitted value of \\(y_i\\), a point estimate of the mean \\(\\mu_{y|x_i}\\) and \\(y_i\\) itself.\n\n\n\n\n\\(b_0\\): intercept of the sample regression line\n\n\\(b_1\\): slope of the sample regression line"
  },
  {
    "objectID": "slides/15-regression.html#fitting-a-regression-line-haty-b_0-b_1x",
    "href": "slides/15-regression.html#fitting-a-regression-line-haty-b_0-b_1x",
    "title": "Regression \n",
    "section": "Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\)\n",
    "text": "Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\)\n\nGiven the sample data \\(\\{ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\},\\)\n\nWhich sample regression line is the best?\nWhat are the best estimators \\(b_0\\) and \\(b_1\\) for \\(\\beta_0\\) and \\(\\beta_1\\)?\n\n\n\n\n\n\n\n\n\n\n\nNow suppose we already get our sample, and now we are trying to use the sample to get a sample regression line, and hopefully, the sample regression line and the population regression line are alike. look similarly.\nSo people usually ask\nWhich sample regression line is the best?\nWhat are the best estimators \\(b_0\\) and \\(b_1\\) for \\(\\beta_0\\) and \\(\\beta_1\\)?\nAfter all, given the data, we can generate so many different straight lines, and we need a criterion to help us determine which line is the best in some sense. Right!"
  },
  {
    "objectID": "slides/15-regression.html#fitting-a-regression-line-haty-b_0-b_1x-1",
    "href": "slides/15-regression.html#fitting-a-regression-line-haty-b_0-b_1x-1",
    "title": "Regression \n",
    "section": "Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\)\n",
    "text": "Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\)\n\nGiven the sample data \\(\\{ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\},\\)\n\nWhich sample regression line is the best?\nWhat are the best estimators \\(b_0\\) and \\(b_1\\) for \\(\\beta_0\\) and \\(\\beta_1\\)?"
  },
  {
    "objectID": "slides/15-regression.html#what-does-best-mean-ordinary-least-squares-ols",
    "href": "slides/15-regression.html#what-does-best-mean-ordinary-least-squares-ols",
    "title": "Regression \n",
    "section": "What does ‚Äúbest‚Äù mean? Ordinary Least Squares (OLS)",
    "text": "What does ‚Äúbest‚Äù mean? Ordinary Least Squares (OLS)\n\nChoose \\(b_0\\) and \\(b_1\\), or sample regression line \\(b_0 + b_1x\\) that minimizes the sum of squared residuals \\(SS_{res}\\)\n\n\n\\[SS_{res} = e_1^2 + e_2^2 + \\dots + e_n^2 = \\sum_{i = 1}^n e_i^2,\\] where the residual \\(e_i = y_i - \\hat{y}_i = y_i - (b_0 + b_1x_i)\\).\n\n\n. . .\n\nIf \\(b_0\\) and \\(b_1\\) are the best estimators,\n\n\\[\\small{\\begin{align} SS_{res} &= (y_1 - b_0 - b_1x_1)^2 + (y_2 - b_0 - b_1x_2)^2 + \\dots + (y_n - b_0 - b_1x_n)^2\\\\ &= \\sum_{i=1}^n(y_i - b_0 - b_1x_i)^2 \\end{align}}\\] that is the smallest comparing to any other \\(SS_{res} = \\sum_{i=1}^n(y_i - a_0 - a_1x_i)^2\\) that uses another pair of values \\((a_0, a_1) \\ne (b_0, b_1)\\).\n\n\nNow the question is How do we get \\(b_0\\) and \\(b_1\\) that well estimate \\(\\beta_0\\) and \\(\\beta_1\\)?\nWe choose \\(b_0\\) and \\(b_1\\), or regression line \\(b_0 + b_1x\\) that minimizes the sum of squared residuals.\nIf we define residual as \\(e_i = y_i - \\hat{y}_i\\), then the sum of squared residuals is \\(\\sum_{i = 1}^n e_i^2\\).\nAnd this approach that estimates the population parameters \\(\\beta_0\\) and \\(\\beta_1\\) or the population regression line is called Ordinary Least Squares method."
  },
  {
    "objectID": "slides/15-regression.html#visualizing-residuals",
    "href": "slides/15-regression.html#visualizing-residuals",
    "title": "Regression \n",
    "section": "Visualizing Residuals",
    "text": "Visualizing Residuals\n\n\n\n\n\n\n\n\n\nThat‚Äôs see the idea of Ordinary Least Squares visually. Here just showed the data. - Later we will work on the data set together."
  },
  {
    "objectID": "slides/15-regression.html#visualizing-residuals-cont.",
    "href": "slides/15-regression.html#visualizing-residuals-cont.",
    "title": "Regression \n",
    "section": "Visualizing Residuals (cont.)",
    "text": "Visualizing Residuals (cont.)\n\n\n\n\n\n\n\n\n\n\nAll right, with the data, this figure also shows the least squares regression line, and the fitted value of \\(y\\) for each \\(x\\) in the training data, which are those red points.\nThe fitted values of y are right on the regression line.\nNow the question is, how do we find this line?\nGiven a line, we can have predicted values of y, right?\nThen what is residual on the plot? The residual will be the difference between the true observation y and the fitted value of y given any value of x.\nSo a residual in the plot will be a vertical bar at the value of x with two ends of the bar \\(y\\) and \\(\\hat{y}\\), right?\n(Show on board)\n(add \\(y_i = b_0+b_1x_i\\) and residual line)"
  },
  {
    "objectID": "slides/15-regression.html#visualizing-residuals-cont.-1",
    "href": "slides/15-regression.html#visualizing-residuals-cont.-1",
    "title": "Regression \n",
    "section": "Visualizing Residuals (cont.)",
    "text": "Visualizing Residuals (cont.)\n\n\n\n\n\n\n\n\n\n\nHere shows all the residuals in vertical bars.\nleast squares line is the line such that the sum of all the squared residuals is minimized.\nWhy we square the residuals?\nIt‚Äôs mathematically more convenient.\nSquaring emphasizes larger differences"
  },
  {
    "objectID": "slides/15-regression.html#least-squares-estimates-lse",
    "href": "slides/15-regression.html#least-squares-estimates-lse",
    "title": "Regression \n",
    "section": "Least Squares Estimates (LSE)",
    "text": "Least Squares Estimates (LSE)\n\nThe least squares approach choose \\(b_0\\) and \\(b_1\\) to minimize the \\(SS_{res}\\), i.e.,\n\n\\[(b_0, b_1) = \\arg \\min_{\\alpha_0, \\alpha_1} \\sum_{i=1}^n(y_i - \\alpha_0 - \\alpha_1x_i)^2\\]   \n\n. . .\nMATH 1450/1455 ‚Ä¶\n\\[\\color{red}{b_0 = \\overline{y} - b_1\\overline{x}}\\]  \n\\[\\color{red}{b_1 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^n(x_i - \\overline{x})^2} = r \\frac{\\sqrt{S_{yy}}}{\\sqrt{S_{xx}}}},\\] where \\(S_{xx} = \\sum_{i=1}^n(x_i - \\overline{x})^2\\) and \\(S_{yy} = \\sum_{i=1}^n(y_i - \\overline{y})^2\\)\n. . .\n\nWhat can we learn from the formula of \\(b_0\\) and \\(b_1\\)?\n\n\n\nThe LS regression line passes through the centroid.\n\n\\(b_1\\) is kinda like a scaled covariance of X and Y."
  },
  {
    "objectID": "slides/15-regression.html#r-lab-mpg-data",
    "href": "slides/15-regression.html#r-lab-mpg-data",
    "title": "Regression \n",
    "section": "\nR Lab: mpg Data",
    "text": "R Lab: mpg Data\n\nlibrary(ggplot2)  ## use data mpg in ggplot2 package\nmpg\n\n# A tibble: 234 √ó 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto‚Ä¶ f        18    29 p     comp‚Ä¶\n 2 audi         a4           1.8  1999     4 manu‚Ä¶ f        21    29 p     comp‚Ä¶\n 3 audi         a4           2    2008     4 manu‚Ä¶ f        20    31 p     comp‚Ä¶\n 4 audi         a4           2    2008     4 auto‚Ä¶ f        21    30 p     comp‚Ä¶\n 5 audi         a4           2.8  1999     6 auto‚Ä¶ f        16    26 p     comp‚Ä¶\n 6 audi         a4           2.8  1999     6 manu‚Ä¶ f        18    26 p     comp‚Ä¶\n 7 audi         a4           3.1  2008     6 auto‚Ä¶ f        18    27 p     comp‚Ä¶\n 8 audi         a4 quattro   1.8  1999     4 manu‚Ä¶ 4        18    26 p     comp‚Ä¶\n 9 audi         a4 quattro   1.8  1999     4 auto‚Ä¶ 4        16    25 p     comp‚Ä¶\n10 audi         a4 quattro   2    2008     4 manu‚Ä¶ 4        20    28 p     comp‚Ä¶\n# ‚Ñπ 224 more rows"
  },
  {
    "objectID": "slides/15-regression.html#r-lab-highway-mpg-hwy-vs.-displacement-displ",
    "href": "slides/15-regression.html#r-lab-highway-mpg-hwy-vs.-displacement-displ",
    "title": "Regression \n",
    "section": "\nR Lab: Highway MPG hwy vs.¬†Displacement displ\n",
    "text": "R Lab: Highway MPG hwy vs.¬†Displacement displ\n\n\nplot(x = mpg$displ, y = mpg$hwy,\n     las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")\n\n\n\n\n\n\n\n\n\nUsually, when we get data, the first thing is plotting your data, doing some exploratory data analysis, and see if there is useful information out there that may help us build an appropriate model.\nTo make a scatter plot, we can simply use the plot() function, and put displ in x axis and hwy in the y axis.\nTo grab a variable or a column of a data frame, we can use the dollar sign, the same way as a list extracting an element.\nThe rest of arguments are optional, they are just used to decorate your plot. You can generate a plot without specifying any of them.\nAnd because it seems to a linear trend downwards. We could fit a simple linear regression to the data. Right"
  },
  {
    "objectID": "slides/15-regression.html#r-lab-fit-simple-linear-regression",
    "href": "slides/15-regression.html#r-lab-fit-simple-linear-regression",
    "title": "Regression \n",
    "section": "\nR Lab: Fit Simple Linear Regression",
    "text": "R Lab: Fit Simple Linear Regression\n\n\n\n\nreg_fit &lt;- lm(formula = hwy ~ displ, \n              data = mpg)\nreg_fit\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nCoefficients:\n(Intercept)        displ  \n      35.70        -3.53  \n\ntypeof(reg_fit)\n\n[1] \"list\"\n\n\n\n\n## all elements in reg_fit\nnames(reg_fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n## use $ to extract an element of a list\nreg_fit$coefficients\n\n(Intercept)       displ \n      35.70       -3.53 \n\n\n\n\n\n. . .\n\n\\(\\widehat{hwy}_{i} = b_0 + b_1 \\times displ_{i} =  35.7 - 3.5 \\times displ_{i}\\)\n\\(b_1\\): For one unit (litre) increase of the displacement, we expect the highway MPG to be decreased, on average, by 3.5.\n\n\n\nIn R, to fit a linear regression model, it cannot be easier.\nWe just need to use the command lm(). We put a formula in the function, y ~ x, and let R know which data set you are considering.\nThat‚Äôs it. And I save the fitted result in an object called reg_fit.\nYou can see lm() function returns a list.\nWe can grab the coefficient estimates this way.\nSo your sample regression line is like this."
  },
  {
    "objectID": "slides/15-regression.html#r-lab-fitted-values-of-y",
    "href": "slides/15-regression.html#r-lab-fitted-values-of-y",
    "title": "Regression \n",
    "section": "\nR Lab Fitted Values of \\(y\\)\n",
    "text": "R Lab Fitted Values of \\(y\\)\n\n\n## the first 5 observed response value y\nmpg$hwy[1:5]\n\n[1] 29 29 31 30 26\n\n## the first 5 fitted value y_hat\nhead(reg_fit$fitted.values, 5)\n\n   1    2    3    4    5 \n29.3 29.3 28.6 28.6 25.8 \n\n## the first 5 predictor value x\nmpg$displ[1:5]\n\n[1] 1.8 1.8 2.0 2.0 2.8\n\nlength(reg_fit$fitted.values)\n\n[1] 234"
  },
  {
    "objectID": "slides/15-regression.html#r-lab-add-a-regression-line",
    "href": "slides/15-regression.html#r-lab-add-a-regression-line",
    "title": "Regression \n",
    "section": "\nR Lab Add a Regression Line",
    "text": "R Lab Add a Regression Line\n\nplot(x = mpg$displ, y = mpg$hwy, las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")\nabline(reg_fit, col = \"#FFCC00\", lwd = 3)"
  },
  {
    "objectID": "slides/15-regression.html#estimation-for-sigma2",
    "href": "slides/15-regression.html#estimation-for-sigma2",
    "title": "Regression \n",
    "section": "Estimation for \\(\\sigma^2\\)\n",
    "text": "Estimation for \\(\\sigma^2\\)\n\n\n\n\nThink of \\(\\sigma^2\\) as variance around the line or the mean square (prediction) error.\nThe estimate of \\(\\sigma^2\\) is the mean square residual \\(MS_{res}\\):\n\n\\[\\hat{\\sigma}^2 = MS_{res} = \\frac{SS_{res}}{n-2} = \\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}\\]\n\n\n\\(MS_{res}\\) is often shown in computer output as \\(\\texttt{MS(Error)}\\) or \\(\\texttt{MS(Residual)}\\). \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo we are done estimation for \\(beta\\). Let‚Äôs talk about the estimation of \\(\\sigma^2\\).\nThe estimate of \\(\\sigma^2\\), denoted as \\(\\hat{\\sigma}^2\\) or \\(s_{\\epsilon}^2\\), based on the sample data is residual sum of squares divided by \\(n-2\\), the degrees of freedom\nIt can be shown that \\(E(SS_{res}) = (n-2)\\sigma^2\\). That is, \\(\\hat{\\sigma}^2\\) is an unbiased estimator for \\(\\sigma^2\\). üëç"
  },
  {
    "objectID": "slides/15-regression.html#r-lab-standard-error-of-regression",
    "href": "slides/15-regression.html#r-lab-standard-error-of-regression",
    "title": "Regression \n",
    "section": "\nR Lab Standard Error of Regression",
    "text": "R Lab Standard Error of Regression\n\n(summ_reg_fit &lt;- summary(reg_fit))\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.104 -2.165 -0.224  2.059 15.010 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   35.698      0.720    49.5   &lt;2e-16 ***\ndispl         -3.531      0.195   -18.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.84 on 232 degrees of freedom\nMultiple R-squared:  0.587, Adjusted R-squared:  0.585 \nF-statistic:  329 on 1 and 232 DF,  p-value: &lt;2e-16\n\n\n\n\nHow do we get sigma_hat in R?\nWell you could grab residuals and df from lm fit result reg_fit, and use the formula to calculate the sigma_hat. sqrt(sum(reg_fit\\(residuals^2) / reg_fit\\)df.residual)\nIf you want R to do the calculation for you, you can get the summary of the fitted result reg_fit.\nThen the sigma hat is right here."
  },
  {
    "objectID": "slides/15-regression.html#r-lab-standard-error-of-regression-1",
    "href": "slides/15-regression.html#r-lab-standard-error-of-regression-1",
    "title": "Regression \n",
    "section": "\nR Lab Standard Error of Regression",
    "text": "R Lab Standard Error of Regression\n\n# lots of fitted information saved in summary(reg_fit)!\nnames(summ_reg_fit)\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n# residual standard error (sigma_hat)\nsumm_reg_fit$sigma\n\n[1] 3.84\n\n# from reg_fit\nsqrt(sum(reg_fit$residuals^2) / reg_fit$df.residual)\n\n[1] 3.84\n\n\n\n\nHere shows the fitted information saved in summary(reg_fit)\nYou see sigma is right here. So you just extract that value if you need."
  },
  {
    "objectID": "slides/15-regression.html#confidence-interval-for-beta_1",
    "href": "slides/15-regression.html#confidence-interval-for-beta_1",
    "title": "Regression \n",
    "section": "Confidence Interval for \\(\\beta_1\\)\n",
    "text": "Confidence Interval for \\(\\beta_1\\)\n\n\n\\(\\frac{b_1 - \\beta_1}{\\sqrt{\\hat{\\sigma}^2/S_{xx}}} \\sim t_{n-2}\\)\n\n. . .\n\n\n\\((1-\\alpha)100\\%\\) CI for \\(\\beta_1\\) is \\(b_1 \\pm t_{\\alpha/2, n-2}\\sqrt{\\hat{\\sigma}^2/S_{xx}}\\)\n\n\n. . .\n\nconfint(reg_fit, level = 0.95)\n\n            2.5 % 97.5 %\n(Intercept) 34.28  37.12\ndispl       -3.91  -3.15"
  },
  {
    "objectID": "slides/15-regression.html#hypothesis-testing-beta_1",
    "href": "slides/15-regression.html#hypothesis-testing-beta_1",
    "title": "Regression \n",
    "section": "Hypothesis Testing: \\(\\beta_1\\)\n",
    "text": "Hypothesis Testing: \\(\\beta_1\\)\n\n\n \\(H_0: \\beta_1 = \\beta_1^0 \\quad H_1: \\beta_1 \\ne \\beta_1^0\\)  \nTest statistic: Under \\(H_0\\),\n\n\\[t_{test} = \\frac{b_1 - \\color{red}{\\beta_1^0}}{\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}} \\sim t_{n-2}\\]\n\nReject \\(H_0\\) in favor of \\(H_1\\) if\n\n\\(|t_{test}| &gt; t_{\\alpha/2, \\, n-2}\\)\n\\(\\text{p-value} = 2P(t_{n-2} &gt; |t_{test}|) &lt; \\alpha\\)\n\n\n\n\n\nin addition to estimation, we may be interested in testing.\nTo do the testing on \\(\\beta_1\\), the testing procedure is basically the same as the procedure for population mean \\(\\mu\\) we reviewed last week.\nUsually \\(\\beta_1^0 = 0\\), but we may be interested in other values.\ngood growth \\(\\beta_1 &gt; 2\\)"
  },
  {
    "objectID": "slides/15-regression.html#r-lab-testing-on-beta_0-and-beta_1",
    "href": "slides/15-regression.html#r-lab-testing-on-beta_0-and-beta_1",
    "title": "Regression \n",
    "section": "\nR Lab Testing on \\(\\beta_0\\) and \\(\\beta_1\\)\n",
    "text": "R Lab Testing on \\(\\beta_0\\) and \\(\\beta_1\\)\n\n\nsumm_reg_fit$coefficients\n\n            Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept)    35.70      0.720    49.6 2.12e-125\ndispl          -3.53      0.195   -18.2  2.04e-46\n\n\n\nTesting \\(H_0: \\beta_0 = 0\\) and \\(H_0: \\beta_1 = 0\\)"
  },
  {
    "objectID": "slides/15-regression.html#interpretation-of-testing-results",
    "href": "slides/15-regression.html#interpretation-of-testing-results",
    "title": "Regression \n",
    "section": "Interpretation of Testing Results",
    "text": "Interpretation of Testing Results\n\n \\(H_0: \\beta_1 = 0 \\quad H_1: \\beta_1 \\ne 0\\) \nFailing to reject \\(H_0: \\beta_1 = 0\\) implies there is no linear relationship between \\(Y\\) and \\(X\\).\n\n\n\nSo back to the testing on \\(\\beta_1\\).\nIf we do not reject \\(H_0\\), it implies there is no linear relationship between \\(Y\\) and \\(X\\). Right? Because \\(\\beta_1\\), the slope of the regression line is pretty much 0.\n\n\n. . .\n\n\n\n\n\n\n\n\n\n\nBut it actually has two cases. One \\(x\\) and \\(y\\) may have no relationship at all, or they don‚Äôt have linear relationship but have another kind of relationship, like quadratic.\nSo we have to be careful when we interpret the testing result. OK.\n\n\n. . .\n\nIf we reject \\(H_0: \\beta_1 = 0\\), does it mean \\(X\\) and \\(Y\\) are linearly related?\n\n\nWe have to be more careful and precise on what we are claiming."
  },
  {
    "objectID": "slides/15-regression.html#test-of-significance-of-regression",
    "href": "slides/15-regression.html#test-of-significance-of-regression",
    "title": "Regression \n",
    "section": "Test of Significance of Regression",
    "text": "Test of Significance of Regression\n\n\nRejecting \\(H_0: \\beta_1 = 0\\) could mean\n\nthe straight-line model is adequate\nbetter results could be obtained with a more complicated model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRejecting \\(H_0: \\beta_1 = 0\\) could mean\n\nthe straight-line model is adequate\nbetter results could be obtained with a more complicated model even though there is a linear effect of \\(x\\)"
  },
  {
    "objectID": "slides/15-regression.html#x---y-relationship-explains-some-deviation",
    "href": "slides/15-regression.html#x---y-relationship-explains-some-deviation",
    "title": "Regression \n",
    "section": "\n\\(X\\) - \\(Y\\) Relationship Explains Some Deviation",
    "text": "\\(X\\) - \\(Y\\) Relationship Explains Some Deviation\n\nSuppose we only have data \\(Y\\) and have no information about \\(X\\) and relationship between \\(X\\) and \\(Y\\). How do we predict a value of \\(Y\\)?\n\n\n\nFor example, suppose we only have MPG information for the sample of cars and have no info about displacement and their relationship. How do we predict a car‚Äôs MPG?\n\n\n. . .\n\nOur best guess would be \\(\\overline{y}\\) if the data have no pattern, i.e., \\(\\hat{y}_i = \\overline{y}\\).\nTreat \\(X\\) and \\(Y\\) as uncorrelated.\nThe (total) deviation from the mean is \\((y_i - \\overline{y})\\)\n\n. . .\n\nIf \\(X\\) and \\(Y\\) are linearly related, fitting a linear regression model helps us predict the value of \\(Y\\) when the value of \\(X\\) is provided.\n\\(\\hat{y}_i = b_0 + b_1x_i\\) is closer to \\(y_i\\) than \\(\\overline{y}\\).\nThe regression model explains some deviation of \\(y\\).\n\n\n\nWhen we have no information about the relationship between \\(X\\) and \\(Y\\), to predict a value of \\(y\\) using the same value given any value of \\(x\\).\nWhen \\(X\\) and \\(Y\\) are uncorrelated, the regression model is not helping predict \\(Y\\) because \\(X\\) provides no information about \\(Y\\).\nIt means \\(b_1 = 0\\) and \\(\\hat{y}_i = \\bar{y}\\) for all values of \\(X\\).\nThe result is the same as the one when we only have data of \\(Y\\).\nThis prediction deviation \\((y_i - \\overline{y})\\) is generally the biggest deviation we can have when we have no information about how y varies or how y is affected by others."
  },
  {
    "objectID": "slides/15-regression.html#partition-of-deviation",
    "href": "slides/15-regression.html#partition-of-deviation",
    "title": "Regression \n",
    "section": "Partition of Deviation",
    "text": "Partition of Deviation\n\nTotal deviation = Deviation explained by regression + Unexplained deviation\n\\((y_i - \\overline{y}) = (\\hat{y}_i - \\overline{y}) + (y_i - \\hat{y}_i)\\)\n\\((19 - 9) = (13 - 9) + (19 - 13)\\)\n\n\n\n\n\n\n\n\n\n\n\nIf \\(X\\) and \\(Y\\) are actually linearly related, fitting a linear regression helps us predict the value of \\(Y\\) when the value of \\(X\\) is provided.\nIt means that \\(\\hat{y}_i = b_0 + b_1x_i\\) is closer to \\(y_i\\) than \\(\\overline{y}\\).\nIntuitively, the regression model explains some deviation of \\(y\\) by the predictor \\(X\\).\n\\(X\\) does contain some useful and valuable information for predicting value of \\(Y\\).\nSo, we can actually Partition the total deviation into two parts.\nPartition of deviation: Total deviation = Explained deviation by regression + Unexplained deviation\n\\((y_i - \\overline{y}) = (\\hat{y}_i - \\overline{y}) + (y_i - \\hat{y}_i)\\)"
  },
  {
    "objectID": "slides/15-regression.html#sum-of-squares-ss",
    "href": "slides/15-regression.html#sum-of-squares-ss",
    "title": "Regression \n",
    "section": "Sum of Squares (SS)",
    "text": "Sum of Squares (SS)\n\n\\(\\sum_{i=1}^n(y_i - \\overline{y})^2 = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 + \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\)\nTotal SS \\((SS_T)\\) = Regression SS \\((SS_R)\\) + Residual SS \\((SS_{res})\\)\n\\(df_T = df_R + df_{res}\\)\n\\(\\color{blue}{(n-1) = 1 +(n-2)}\\)\n\n\n\nTotal variability = variability explained by regression + unexplained variability\n\\(\\sum_{i=1}^n(y_i - \\overline{y})^2 = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 + \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 + 2\\sum_{i=1}^n(\\hat{y}_i - \\overline{y})(y_i - \\hat{y}_i)\\)\n\n\n \\(df_T = n - 1\\): lose 1 df with constraint \\(\\sum_{i=1}^n(y_i - \\overline{y}) = 0\\)\n\n\n \\(df_R = 1\\): all \\(\\hat{y}_i\\) are on the regression line with 2 dfs (intercept and slope), but with constraint \\(\\sum_{i=1}^n(\\hat{y}_i - \\overline{y}) = 0\\)\n\n\n \\(df_{res} = n - 2\\): lose 2 dfs because \\(\\beta_0\\) and \\(\\beta_1\\) are estimated by \\(b_0\\) and \\(b_1\\), which are linear combo of \\(y_i\\)\n\ndegrees of freedom is the equivalent number of values in the calculation of a statistic that are free to vary.\n\n\\(SS_R = b_1S_{xy}\\) or \\(SS_{res} = SS_T - b_1S_{xy}\\)."
  },
  {
    "objectID": "slides/15-regression.html#anova-for-testing-significance-of-regression",
    "href": "slides/15-regression.html#anova-for-testing-significance-of-regression",
    "title": "Regression \n",
    "section": "ANOVA for Testing Significance of Regression",
    "text": "ANOVA for Testing Significance of Regression\n\n\n\n\n\n\n\n\n\nA larger value of \\(F_{test}\\) indicates that regression is significant.\n\nReject \\(H_0\\) if\n\n\\(F_{test} &gt; F_{\\alpha, 1, n-2}\\)\n\n\\(\\text{p-value} = P(F_{1, n-2} &gt; F_{test}) &lt; \\alpha\\).\n\n\nThe ANOVA is designed to test \\(H_0\\) that all predictors have no value in predicting \\(y\\).\nIn SLR, the \\(F\\)-test of ANOVA gives the same result as a two-sided \\(t\\)-test of \\(H_0: \\beta_1=0\\).\n\n\n\nANOVA is used for testing significance of regression.\nIt is testing if any predictors or regressors have explanatory power for predicting y.\nIn other words, it is testing if the whole regression model is useful or not.\nHere is the ANOVA table.\n\\(H_0: \\beta_1 = 0\\)\nA larger value of \\(F_{test}\\) indicates that regression is significant.\nReject \\(H_0\\) in favor of \\(H_1\\) if \\(F_{test} &gt; F_{\\alpha, 1, n-2}\\) or \\(\\text{p-value} = P(F_{1, n-2} &gt; F_{test}) &lt; \\alpha\\).\nThe ANOVA is designed to test \\(H_0\\) that all predictors have no value in predicting \\(y\\).\nIn SLR, there is only one predictor, and hence the \\(F\\)-test of ANOVA gives the same result as a two-sided \\(t\\)-test of \\(H_0: \\beta_1=0\\).\n\n\nIf we have \\(k \\ge 2\\) predictors, the \\(F\\)-test is testing \\(H_0: \\beta_1=\\beta_2=\\cdots=\\beta_k=0\\)."
  },
  {
    "objectID": "slides/15-regression.html#r-lab-anova-table",
    "href": "slides/15-regression.html#r-lab-anova-table",
    "title": "Regression \n",
    "section": "\nR Lab ANOVA Table",
    "text": "R Lab ANOVA Table\n\nanova(reg_fit)\n\nAnalysis of Variance Table\n\nResponse: hwy\n           Df Sum Sq Mean Sq F value Pr(&gt;F)    \ndispl       1   4848    4848     329 &lt;2e-16 ***\nResiduals 232   3414      15                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\n\n\n\nFor \\(H_0: \\beta_1 = 0\\) in SLR, \\(t_{test}^2 = F_{test}\\).\n\n\n\n            Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept)    35.70      0.720    49.6 2.12e-125\ndispl          -3.53      0.195   -18.2  2.04e-46\n\n\n[1] 329\n\n\n\nFor \\(H_0: \\beta_1 = 0\\), \\(t_{test}^2 = \\left(\\frac{b_1}{\\sqrt{MS_{res}/S_{xx}}}\\right)^2 = \\frac{b_1^2S_{xx}}{MS_{res}} = \\frac{b_1S_{xy}}{MS_{res}} = \\frac{MS_R}{MS_{res}} = F_{test}\\)"
  },
  {
    "objectID": "slides/15-regression.html#coefficient-of-determination",
    "href": "slides/15-regression.html#coefficient-of-determination",
    "title": "Regression \n",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\n\nThe coefficient of determination \\((R^2)\\) is the proportion of the variation in \\(y\\) that is explained by the regression model:\n\n\\[R^2 = \\frac{SS_R}{SS_T} =\\frac{SS_T - SS_{res}}{SS_T} = 1 - \\frac{SS_{res}}{SS_T}\\]\n\n\\(R^2\\) as the proportionate reduction of total variation associated with the use of \\(X\\).\n(a) \\(\\hat{y}_i = y_i\\) and \\(\\small SS_{res} =  \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = 0\\). (b) \\(\\hat{y}_i = \\overline{y}\\) and \\(\\small SS_R = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2  = 0\\).\n\n\n\n\n\n\n\n\n\n\n\n\ncoefficient of determination can be used to measure the quality of our regression or the explanatory power of regressors.\nHere (a) and (b) are two extreme cases.\nIn (a), the fitted value = the true observation. So the regression model explains all the variation in \\(Y\\), and hence \\(R^2 = 1\\).\nIn (b), the fitted value = mean of y as if we don‚Äôt have information about \\(x\\) or \\(x\\) is totally useless in predicting \\(Y\\). In this case, the regression model explains no the variation in \\(Y\\), and all variation remain unexplained. So \\(R^2 = 0\\)."
  },
  {
    "objectID": "slides/15-regression.html#r-lab-r2",
    "href": "slides/15-regression.html#r-lab-r2",
    "title": "Regression \n",
    "section": "\nR Lab \\(R^2\\)\n",
    "text": "R Lab \\(R^2\\)\n\n\n\nsumm_reg_fit\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.104 -2.165 -0.224  2.059 15.010 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   35.698      0.720    49.5   &lt;2e-16 ***\ndispl         -3.531      0.195   -18.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.84 on 232 degrees of freedom\nMultiple R-squared:  0.587, Adjusted R-squared:  0.585 \nF-statistic:  329 on 1 and 232 DF,  p-value: &lt;2e-16\n\n\n\n\nsumm_reg_fit$r.squared\n\n[1] 0.587"
  },
  {
    "objectID": "slides/15-regression.html#predicting-the-mean-response",
    "href": "slides/15-regression.html#predicting-the-mean-response",
    "title": "Regression \n",
    "section": "Predicting the Mean Response",
    "text": "Predicting the Mean Response\n\n\n\nWith predictor value \\(x = x_0\\), we want to estimate the mean response \\(\\mu_{y|x_0} = \\beta_0 + \\beta_1 x_0\\).\n\n The mean highway MPG \\(\\mu_{y|x_0}\\) when displacement is \\(x = x_0 = 5.5\\). \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\n\nIf \\(x_0\\) is within the range of \\(x\\), an unbiased point estimator for \\(\\mu_{y|x_0}\\) is\n\n\\[\\hat{\\mu}_{y | x_0} = b_0 + b_1 x_0\\]\n\n. . .\n\n\nThe \\((1-\\alpha)100\\%\\) CI for \\(\\mu_{y|x_0}\\) is\n\n\\[\\hat{\\mu}_{y | x_0} \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}\\]\n\n. . .\n\nDoes the length of the CI for \\(\\mu_{y|x_0}\\) stay the same at any location of \\(x_0\\)?\n\n\n\n\n\n\nWith predictor value \\(x = x_0\\), we want to estimate the mean response \\(E(y\\mid x_0) = \\mu_{y|x_0} = \\beta_0 + \\beta_1 x_0\\).\n\n The mean highway MPG \\(E(y \\mid x_0)\\) when displacement is \\(x = x_0 = 5.5\\). \n\n\nIf \\(x_0\\) is within the range of \\(x\\), an unbiased point estimator for \\(E(y\\mid x_0)\\) is \\[\\widehat{E(y\\mid x_0)} = \\hat{\\mu}_{y | x_0} = b_0 + b_1 x_0\\]\nThe \\((1-\\alpha)100\\%\\) CI for \\(E(y\\mid x_0)\\) is \\(\\boxed{\\hat{\\mu}_{y | x_0} \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}}\\)."
  },
  {
    "objectID": "slides/15-regression.html#section-4",
    "href": "slides/15-regression.html#section-4",
    "title": "Regression \n",
    "section": "",
    "text": "## CI for the mean response\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"confidence\", level = 0.95)\n\n   fit  lwr  upr\n1 16.3 15.4 17.2"
  },
  {
    "objectID": "slides/15-regression.html#predicting-new-observations",
    "href": "slides/15-regression.html#predicting-new-observations",
    "title": "Regression \n",
    "section": "Predicting New Observations",
    "text": "Predicting New Observations\n\nPredict the value of a new observation \\(y_0\\) with \\(x = x_0\\).\n\n The highway MPG of a car \\(y_0(x_0)\\) when its displacement is \\(x = x_0 = 5.5\\). \n\n\n\n. . .\n\nAn unbiased point estimator for \\(y_0(x_0)\\) is\n\n\\[\\hat{y}_0(x_0) = b_0 + b_1 x_0\\]\n. . .\n\nThe \\((1-\\alpha)100\\%\\) prediction interval (PI) for \\(y_0(x_0)\\) is\n\n\\[\\hat{y}_0 \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{{\\color{red}{1}}+ \\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}\\]\n\nWhat is the difference between CI for \\(\\mu_{y | x_0}\\) and PI for \\(y_0(x_0)\\)?\n\n. . .\n\n\nThe PI is wider as it includes the uncertainty about \\(b_0\\), \\(b_1\\) as well as \\(y_0\\) due to error \\(\\epsilon\\).\n\n\n\nPredict the value of a new observation \\(y_0\\) corresponding to a specified value of predictor \\(x = x_0\\).\n\n The highway MPG of a car \\(y_0(x_0)\\) when its displacement is \\(x = x_0 = 5.5\\). \n\n\nAn unbiased point estimator for \\(y_0(x_0)\\) is \\[\\hat{y}_0(x_0) = b_0 + b_1 x_0\\]"
  },
  {
    "objectID": "slides/15-regression.html#r-lab-prediction",
    "href": "slides/15-regression.html#r-lab-prediction",
    "title": "Regression \n",
    "section": "\nR Lab Prediction",
    "text": "R Lab Prediction\n\n\n## CI for the mean response\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"confidence\", level = 0.95)\n\n   fit  lwr  upr\n1 16.3 15.4 17.2\n\n## PI for the new observation\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"predict\", level = 0.95)\n\n   fit  lwr  upr\n1 16.3 8.67 23.9\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf I can only use one point or value to predict \\(y_0\\), the best guess is the predicted value on the regression line.\nAfter all, values around the lines would more likely to be drawn based on our model.\nHere you can understand why uncertainty quantification is important.\nYes, we can predict an new observation value, but the prediction quality is gonna be bad because we are predict a random variable, not a constant, and \\(y_0\\) can vary a lot around the regression line.\nAnd prediction interval gives us an idea of how good or how bad our prediction is."
  },
  {
    "objectID": "slides/15-regression.html#section-5",
    "href": "slides/15-regression.html#section-5",
    "title": "Regression \n",
    "section": "",
    "text": "CI is the shortest when \\(x = \\bar{x}\\). (Also PI)\nPI length looks the same along with \\(x\\) because the \\(\\sigma^2\\) dominates the uncertainty, comparing to the uncertainty about \\(b_0\\) and \\(b_1\\)."
  },
  {
    "objectID": "slides/05-data-discription-slides.html#descriptive-statistics-data-summary",
    "href": "slides/05-data-discription-slides.html#descriptive-statistics-data-summary",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Descriptive Statistics (Data Summary)",
    "text": "Descriptive Statistics (Data Summary)\n\nBefore doing inferential statistics, let‚Äôs first learn to understand our data by describing or summarizing it using a table, graph, or some important measures, so that appropriate methods can be performed for better inference results."
  },
  {
    "objectID": "slides/05-data-discription-slides.html#frequency-table-for-categorical-variable",
    "href": "slides/05-data-discription-slides.html#frequency-table-for-categorical-variable",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Frequency Table for Categorical Variable",
    "text": "Frequency Table for Categorical Variable\n\nA frequency table (frequency distribution) lists variable values individually for categorical data along with their corresponding number of times occurred in the data (frequencies or counts).\nFrequency table for categorical data with \\(n\\) data values:\n\n\n\nCategory name\nFrequency\nRelative Frequency\n\n\n\n\\(C_1\\)\n\\(f_1\\)\n\\(f_1/n\\)\n\n\n\\(C_2\\)\n\\(f_2\\)\n\\(f_2/n\\)\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\\(C_k\\)\n\\(f_k\\)\n\\(f_k/n\\)"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#frequency-table-for-categorical-variable-1",
    "href": "slides/05-data-discription-slides.html#frequency-table-for-categorical-variable-1",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Frequency Table for Categorical Variable",
    "text": "Frequency Table for Categorical Variable\n\nA frequency table (frequency distribution) lists variable values individually for categorical data along with their corresponding number of times occurred in the data (frequencies or counts).\nExample: A categorical variable color that has three categories\n\n\n\nCategory name\nFrequency\nRelative Frequency\n\n\n\nRed üî¥\n8\n8/50 = 0.16\n\n\nBlue üîµ\n26\n26/50 = 0.52\n\n\nBlack ‚ö´\n16\n16/50 = 0.32"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#r-packages",
    "href": "slides/05-data-discription-slides.html#r-packages",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "R Packages üì¶",
    "text": "R Packages üì¶\n\n\n\nPackages wrap up reusable R functions, the documentation that describes how to use them, and data sets all together.\nAs of August 2025, there are about 22510 R packages available on CRAN (the Comprehensive R Archive Network)!\n\n\n\n\n\n\n\n\n\n\n\n\npalmerpenguins package"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#categorical-frequency-table-palmerpenguins-package",
    "href": "slides/05-data-discription-slides.html#categorical-frequency-table-palmerpenguins-package",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Categorical Frequency Table palmerpenguins package \n",
    "text": "Categorical Frequency Table palmerpenguins package \n\n\n\n\nlibrary(palmerpenguins)\n\n\nstr(penguins)\n\ntibble [344 √ó 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- penguins[, \"species\"]"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#categorical-frequency-table-species",
    "href": "slides/05-data-discription-slides.html#categorical-frequency-table-species",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Categorical Frequency Table: species\n",
    "text": "Categorical Frequency Table: species\n\n\n## frequency table\ntable(x)\n\nspecies\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n\n\n\n\n\n\n\n\n\n\nIf we want to create a frequency table shown in definition, which R data structure we can use?"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#visualizing-a-frequency-table-bar-chart",
    "href": "slides/05-data-discription-slides.html#visualizing-a-frequency-table-bar-chart",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Visualizing a Frequency Table: Bar Chart",
    "text": "Visualizing a Frequency Table: Bar Chart\n\nbarplot(height = table(x), main = \"Bar Chart\", xlab = \"Penguin Species\")"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#pie-chart",
    "href": "slides/05-data-discription-slides.html#pie-chart",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Pie Chart",
    "text": "Pie Chart\n\npie(x = table(x), main = \"Pie Chart\")"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#frequency-distribution-for-numerical-variables",
    "href": "slides/05-data-discription-slides.html#frequency-distribution-for-numerical-variables",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Frequency Distribution for Numerical Variables",
    "text": "Frequency Distribution for Numerical Variables\n\nDivide the data into \\(k\\) non-overlapping groups of intervals (classes).\nConvert the data into \\(k\\) categories with an associated class interval.\nCount the number of measurements falling in a given class interval (class frequency).\n\n\n\nClass\nClass Interval\nFrequency\nRelative Frequency\n\n\n\n\\(1\\)\n\\([a_1, a_2]\\)\n\\(f_1\\)\n\\(f_1/n\\)\n\n\n\\(2\\)\n\\((a_2, a_3]\\)\n\\(f_2\\)\n\\(f_2/n\\)\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\\(k\\)\n\\((a_k, a_{k+1}]\\)\n\\(f_k\\)\n\\(f_k/n\\)\n\n\n\n\n\n\\((a_2 - a_1) = (a_3 - a_2) = \\cdots = (a_{k+1} - a_k)\\). All class widths are the same!"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#frequency-distribution-for-numerical-variables-1",
    "href": "slides/05-data-discription-slides.html#frequency-distribution-for-numerical-variables-1",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Frequency Distribution for Numerical Variables",
    "text": "Frequency Distribution for Numerical Variables\n\nDivide the data into \\(k\\) non-overlapping groups of intervals (classes).\nConvert the data into \\(k\\) categories with an associated class interval.\nCount the number of measurements falling in a given class interval (class frequency).\n\n\n\nClass\nClass Interval\nFrequency\nRelative Frequency\n\n\n\n\\(1\\)\n\\([80, 100]\\)\n\\(2\\)\n\\(2/50\\)\n\n\n\\(2\\)\n\\((100, 120]\\)\n\\(4\\)\n\\(4/50\\)\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\\(8\\)\n\\((220, 240]\\)\n\\(3\\)\n\\(3/50\\)"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#section-1",
    "href": "slides/05-data-discription-slides.html#section-1",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "",
    "text": "Can our grade conversion be used for creating a frequency distribution?\n\n\n\n\n\nGrade\nPercentage\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[73, 77)\n\n\nC-\n[70, 73)\n\n\nD+\n[65, 70)\n\n\nD\n[60, 65)\n\n\nF\n[0, 60)"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#body-mass-grams-in-data-penguins",
    "href": "slides/05-data-discription-slides.html#body-mass-grams-in-data-penguins",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Body Mass (Grams) in Data penguins\n",
    "text": "Body Mass (Grams) in Data penguins\n\n\nbody_mass &lt;- penguins$body_mass_g\nhead(body_mass, 20)\n\n [1] 3750 3800 3250   NA 3450 3650 3625 4675 3475 4250 3300 3700 3200 3800 4400\n[16] 3700 3450 4500 3325 4200\n\nbody_mass &lt;- body_mass[complete.cases(body_mass)]"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#frequency-distribution-of-body-mass",
    "href": "slides/05-data-discription-slides.html#frequency-distribution-of-body-mass",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Frequency Distribution of Body Mass",
    "text": "Frequency Distribution of Body Mass\n\n\n\n\n\n Class   Class_Intvl Freq Rel_Freq\n     1 2700g - 3000g   11     0.03\n     2 3000g - 3300g   29     0.08\n     3 3300g - 3600g   57     0.17\n     4 3600g - 3900g   57     0.17\n     5 3900g - 4200g   39     0.11\n     6 4200g - 4500g   34     0.10\n     7 4500g - 4800g   34     0.10\n     8 4800g - 5100g   26     0.08\n     9 5100g - 5400g   21     0.06\n    10 5400g - 5700g   22     0.06\n    11 5700g - 6000g   10     0.03\n    12 6000g - 6300g    2     0.01\n\n\n\n\nrange(body_mass)\n\n[1] 2700 6300\n\n\n\n\nAll class widths are the same!\nNumber of classes should not be too big or too small.\nThe lower limit of the 1st class should not be greater than the minimum value of the data.\nThe upper limit of the last class should not be smaller than the maximum value of the data.\n\n\n\n\nWonder how we choose the number of classes or the class width?\n\n\n\n\nR decide the number for us when we visualize the frequency distribution by a histogram."
  },
  {
    "objectID": "slides/05-data-discription-slides.html#visualizing-frequency-distribution-by-a-histogram",
    "href": "slides/05-data-discription-slides.html#visualizing-frequency-distribution-by-a-histogram",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Visualizing Frequency Distribution by a Histogram",
    "text": "Visualizing Frequency Distribution by a Histogram\n\n\nUse default breaks (no need to specify)\n\nhist(x = body_mass, \n     xlab = \"Body Mass (gram)\",\n     main = \"Histogram (Defualt)\")\n\n\n\n\n\n\n\n\n\nUse customized breaks\n\n\nclass_boundary\n\n [1] 2700 3000 3300 3600 3900 4200 4500 4800 5100 5400 5700 6000 6300\n\nhist(x = body_mass, \n     breaks = class_boundary, #&lt;&lt;\n     xlab = \"Body Mass (gram)\",\n     main = \"Histogram (Ours)\")"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#skewness",
    "href": "slides/05-data-discription-slides.html#skewness",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Skewness",
    "text": "Skewness\n\nKey characteristics of distributions includes shape, center and spread.\nSkewness provides a way to summarize the shape of a distribution."
  },
  {
    "objectID": "slides/05-data-discription-slides.html#remembering-skewness",
    "href": "slides/05-data-discription-slides.html#remembering-skewness",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Remembering Skewness",
    "text": "Remembering Skewness\n\n\n\nIs the body mass histogram left skewed or right skewed?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiostatistics for the Biological and Health Sciences p.53"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#scatterplot-for-two-numerical-variables",
    "href": "slides/05-data-discription-slides.html#scatterplot-for-two-numerical-variables",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Scatterplot for Two Numerical Variables",
    "text": "Scatterplot for Two Numerical Variables\n\n\nA scatterplot provides a case-by-case view of data for two numerical variables.\n\n\n\n\nplot(x = penguins$bill_length_mm, y = penguins$bill_depth_mm,\n     xlab = \"Bill Length\", ylab = \"Bill Depth\",\n     pch = 16, col = 4)"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#section-2",
    "href": "slides/05-data-discription-slides.html#section-2",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "",
    "text": "For the penguins data, do\n\nPie chart for variable island.\nHistogram for variable flipper_length_mm. Discuss its shape.\nScatter plot for variables flipper_length_mm and bill_length_mm.\n\n\n\n\n\n\n\n\n\n\n\n  \n    ‚àí\n    +\n \n 02:00"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#numerical-summaries-of-data",
    "href": "slides/05-data-discription-slides.html#numerical-summaries-of-data",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Numerical Summaries of Data",
    "text": "Numerical Summaries of Data\n\n\n\n\n\n\n\n\n\n\n\nIf you need to choose one value that represents the entire data, what value would you choose?\n\n\n\n\nMeasure of Center: We typically use the middle point. (What does ‚Äúmiddle‚Äù mean?)\nMeasure of Variation: What values tell us how much variation a variable has?"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#measures-of-center-mean",
    "href": "slides/05-data-discription-slides.html#measures-of-center-mean",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Measures of Center: Mean",
    "text": "Measures of Center: Mean\n\nThe (arithmetic) mean or average is adding up all of the values, then dividing by the total number of them.\nLet \\(x_1, x_2, \\dots, x_n\\) denote the measurements observed in a sample of size \\(n\\). Then the sample mean is defined as \\[\\overline{x} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{x_1 + x_2 + \\dots + x_n}{n}\\]\nIn the body mass example, \\[\\overline{x} = \\frac{3750 + 3800 + \\cdots + 3775}{342} = 4202\\] \n\n\nmean(body_mass)\n\n[1] 4202\n\n\n\nMean balances data Values. It means that the sum of deviations from the mean is 0."
  },
  {
    "objectID": "slides/05-data-discription-slides.html#balancing-point",
    "href": "slides/05-data-discription-slides.html#balancing-point",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Balancing Point",
    "text": "Balancing Point\n\nThink of the mean as the balancing point of the distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean is a measure of center in the sense that it can be viewed as the balancing point of the distribution.\nThis is the distribution of the interest rate.\nAnd if you imagine that we put all those data points on a see-saw, the sample mean is the balancing point that keeps the see-saw balanced horizontally."
  },
  {
    "objectID": "slides/05-data-discription-slides.html#measures-of-center-median",
    "href": "slides/05-data-discription-slides.html#measures-of-center-median",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Measures of Center: Median",
    "text": "Measures of Center: Median\n\nMedian: the middle value when data values are sorted.\nHalf of the values are less than or equal to the median, and the other half are greater than it.\nTo find the median, we first sort the values.\n\n\\(n\\) is odd: the median is located in the exact middle of the ordered values.\n\n Data: (0, 2, 10, 14, 8) \n Sorted Data: (0, 2, 8, 10, 14) \n The median is \\(8\\) \n\n\n\n\n\n\n\\(n\\) is even: the median is the average of the two middle numbers.\n\n Data: (0, 2, 10, 14, 8, 12) \n Sorted Data: (0, 2, 8, 10, 12, 14) \n The median is \\(\\frac{8 + 10}{2} = 9\\) \n\n\n\n\nThe Median is the middle value when data values are sorted from the lowest to highest.\nHalf of the values are less than or equal to the median, and the other half are greater than it.\nTo find the median (well without using any technology or computer), we first sort the values.\nIf \\(n\\) is odd, the median is located in the exact middle of the ordered values.\nThere are two on its right and two on its left. it is right on the middle.\nIf \\(n\\) is even, the median is the average of the two middle numbers.\n8 or 10 itself is not right on the middle."
  },
  {
    "objectID": "slides/05-data-discription-slides.html#calculate-median-in-r",
    "href": "slides/05-data-discription-slides.html#calculate-median-in-r",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Calculate Median in R",
    "text": "Calculate Median in R\n\nmedian(body_mass)  ## Compute the median using command median()\n\n[1] 4050\n\n\n\n\n(sort_mass &lt;- sort(body_mass))  ## sort data\n\n  [1] 2700 2850 2850 2900 2900 2900 2900 2925 2975 3000 3000 3050 3050 3050 3050\n [16] 3075 3100 3150 3150 3150 3150 3175 3175 3200 3200 3200 3200 3200 3250 3250\n [31] 3250 3250 3250 3275 3300 3300 3300 3300 3300 3300 3325 3325 3325 3325 3325\n [46] 3350 3350 3350 3350 3350 3400 3400 3400 3400 3400 3400 3400 3400 3425 3425\n [61] 3450 3450 3450 3450 3450 3450 3450 3450 3475 3475 3475 3500 3500 3500 3500\n [76] 3500 3500 3500 3525 3525 3550 3550 3550 3550 3550 3550 3550 3550 3550 3575\n [91] 3600 3600 3600 3600 3600 3600 3600 3625 3650 3650 3650 3650 3650 3650 3675\n[106] 3675 3700 3700 3700 3700 3700 3700 3700 3700 3700 3700 3700 3725 3725 3725\n[121] 3750 3750 3750 3750 3750 3775 3775 3775 3775 3800 3800 3800 3800 3800 3800\n[136] 3800 3800 3800 3800 3800 3800 3825 3850 3875 3900 3900 3900 3900 3900 3900\n[151] 3900 3900 3900 3900 3950 3950 3950 3950 3950 3950 3950 3950 3950 3950 3975\n[166] 4000 4000 4000 4000 4000 4050 4050 4050 4050 4050 4050 4075 4100 4100 4100\n[181] 4100 4100 4150 4150 4150 4150 4150 4150 4200 4200 4200 4200 4200 4250 4250\n[196] 4250 4250 4250 4275 4300 4300 4300 4300 4300 4300 4300 4300 4350 4350 4375\n[211] 4400 4400 4400 4400 4400 4400 4400 4400 4450 4450 4450 4450 4450 4475 4500\n[226] 4500 4500 4550 4550 4575 4600 4600 4600 4600 4600 4625 4625 4650 4650 4650\n[241] 4650 4650 4675 4700 4700 4700 4700 4700 4700 4725 4725 4725 4750 4750 4750\n[256] 4750 4750 4775 4800 4800 4800 4850 4850 4850 4850 4875 4875 4875 4900 4900\n[271] 4925 4925 4950 4950 4975 5000 5000 5000 5000 5000 5000 5050 5050 5050 5100\n[286] 5100 5100 5150 5150 5200 5200 5200 5200 5250 5250 5250 5300 5300 5300 5300\n[301] 5350 5350 5350 5400 5400 5400 5400 5400 5450 5500 5500 5500 5500 5500 5550\n[316] 5550 5550 5550 5550 5550 5600 5600 5650 5650 5650 5700 5700 5700 5700 5700\n[331] 5750 5800 5800 5850 5850 5850 5950 5950 6000 6000 6050 6300\n\nlength(body_mass)  ## Check sample size is odd or even\n\n[1] 342\n\n(sort_mass[171] + sort_mass[172]) / 2  ## Verify the answer\n\n[1] 4050\n\n\n\n\n\n(body_mass[171] + body_mass[172]) / 2  ## Using un-sorted data leads to a wrong answer!!\n\n[1] 5525"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#measures-of-center-mode",
    "href": "slides/05-data-discription-slides.html#measures-of-center-mode",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Measures of Center: Mode",
    "text": "Measures of Center: Mode\n\nMode: the value that occurs most frequently.\nFor continuous numerical data, it is common to have no observations with the same value.\nPractical definition: A mode is represented by a prominent peak in the distribution.\n\n\n## Create a frequency table \ntable_data &lt;- table(body_mass)\n\n\n\n## Sort the table to find the mode that occurs most frequently\n## the number that happens most frequently will be the first one\nsort(table_data, decreasing = TRUE)\n\nbody_mass\n3800 3700 3900 3950 3550 3400 3450 4300 4400 3500 3600 3300 3650 4050 4150 4700 \n  12   11   10   10    9    8    8    8    8    7    7    6    6    6    6    6 \n5000 5550 3200 3250 3325 3350 3750 4000 4100 4200 4250 4450 4600 4650 4750 5400 \n   6    6    5    5    5    5    5    5    5    5    5    5    5    5    5    5 \n5500 5700 2900 3050 3150 3775 4850 5200 5300 3475 3725 4500 4725 4800 4875 5050 \n   5    5    4    4    4    4    4    4    4    3    3    3    3    3    3    3 \n5100 5250 5350 5650 5850 2850 3000 3175 3425 3525 3675 4350 4550 4625 4900 4925 \n   3    3    3    3    3    2    2    2    2    2    2    2    2    2    2    2 \n4950 5150 5600 5800 5950 6000 2700 2925 2975 3075 3100 3275 3575 3625 3825 3850 \n   2    2    2    2    2    2    1    1    1    1    1    1    1    1    1    1 \n3875 3975 4075 4275 4375 4475 4575 4675 4775 4975 5450 5750 6050 6300 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n\n\n\nMathematically, the mode is the value in the data that occurs most frequently.\nFor continous numerical data that take on infinitely many possible values, it is common to have no observations with the same value in a data set, making this definition impractical.\nPractical definition: A mode is represented by a prominent peak in the distribution."
  },
  {
    "objectID": "slides/05-data-discription-slides.html#comparison-of-mean-median-and-mode",
    "href": "slides/05-data-discription-slides.html#comparison-of-mean-median-and-mode",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Comparison of Mean, Median and Mode",
    "text": "Comparison of Mean, Median and Mode\n\nMean is sensitive to extreme values (outliers).\nMedian/mode is more robust than mean.\n\n\nhead(data_extreme, 10)\n\n [1] 37500  3800  3250  3450  3650  3625  4675  3475  4250  3300\n\n\n\nmean(data_extreme)  ## Large mean! Original mean is 4202\n\n[1] 4300\n\nmedian(data_extreme)  ## Median does not change!\n\n[1] 4050\n\nnames(sort(table(data_extreme), decreasing = TRUE))[1] ## Mode does not change too!\n\n[1] \"3800\"\n\n\n\nMean is sensitive to extreme values (outliers).\nMedian/mode is more robust than mean, meaning that median and mode will not change or not change much if few data values change."
  },
  {
    "objectID": "slides/05-data-discription-slides.html#comparison-of-mean-median-and-mode-1",
    "href": "slides/05-data-discription-slides.html#comparison-of-mean-median-and-mode-1",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Comparison of Mean, Median and Mode",
    "text": "Comparison of Mean, Median and Mode\n\nMode is applicable for both categorical and numerical data, while median and mean work for numerical data only.\nThere may be more than one mode, but there is only one median and one mean.\n\n\n\nMode is applicable for both categorical and numerical data, while median and mean work for numerical data only.\n\nShow Categorical homeownership freq table\nShow int_rate mode\n\n\nThere may be more than one mode, but there is only one median and one mean.\nmean is sensitive to extreme values. The values at the long tails are extreme values comparing to the values in the middle.\nThis forces the mean to be pulled toward the left in order to balance the data.\nMode happens at the peak of the distribution"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#measures-of-variation",
    "href": "slides/05-data-discription-slides.html#measures-of-variation",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Measures of Variation",
    "text": "Measures of Variation\n\n\nDone the discussion of measure of center.\nNow let‚Äôs see how we quantify the variation of data.\nHere we have three frequency distributions with the same mean, median and mode because their distribution is symmetric.\nBut from top to bottom, the data have small, median and large variation.\nBasically larger variation means the data values spread out more.\nSo although the data mean is right here, but in general, data values are not very close to the mean. Their deviation from the sample mean is large. OK."
  },
  {
    "objectID": "slides/05-data-discription-slides.html#measures-of-variation-p-th-percentile",
    "href": "slides/05-data-discription-slides.html#measures-of-variation-p-th-percentile",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Measures of Variation: p-th percentile",
    "text": "Measures of Variation: p-th percentile\n\n\n\n\np-th percentile (quantile): a data value such that\n\nat most \\(p\\%\\) of the values are below it\nat most \\((1-p)\\%\\) of the values are above it\n\n\n\n\n\nTwo datasets with the same mean 20.\n\nOne data set has 99-th percentile = 30, and 1-st percentile = 10.\nThe other has 99-th percentile = 40, and 1-st percentile = 0.\n\n\nWhich data have larger variation?\n\n\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/ACT_(test)\n\n\n\n\n\n\nThe first Measure of Variation is p-th percentile\n\np-th percentile (quantile): a data value such that\n\nat most \\(p\\%\\) of the values are below it\nat most \\((1-p)\\%\\) of the values are above it"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#measures-of-variation-interquartile-range-iqr",
    "href": "slides/05-data-discription-slides.html#measures-of-variation-interquartile-range-iqr",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Measures of Variation: Interquartile Range (IQR)",
    "text": "Measures of Variation: Interquartile Range (IQR)\n\nFirst Quartile (Q1): the 25-th percentile\nSecond Quartile (Q2): the 50-th percentile (Median)\nThird Quartile (Q3): the 75-th percentile\nInterquartile Range (IQR): Q3 - Q1\n\n\n\n\n## Use quantile() to find any percentile \n## through specifying the probability\nquantile(x = body_mass, \n         probs = c(0.25, 0.5, 0.75))\n\n 25%  50%  75% \n3550 4050 4750 \n\n## IQR by definition\nquantile(x = body_mass, probs = 0.75) - \n  quantile(x = body_mass, probs = 0.25) \n\n 75% \n1200 \n\n\n\n\n## IQR()\nIQR(body_mass)  \n\n[1] 1200\n\n## summary() to get the numeric summary\nsummary(body_mass)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2700    3550    4050    4202    4750    6300 \n\n\n\nLarger IQR means more or less variation?\n\n\nThe range of the middle 50% of the data."
  },
  {
    "objectID": "slides/05-data-discription-slides.html#variance-and-standard-deviation",
    "href": "slides/05-data-discription-slides.html#variance-and-standard-deviation",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Variance and Standard Deviation",
    "text": "Variance and Standard Deviation\n\nThe distance of an observation from its mean, \\(x_i - \\overline{x}\\), its deviation.\nSample Variance is defined as \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})^2}{n-1} \\]\nSample Standard Deviation (SD) is defined as the square root of the variance \\[ s = \\sqrt{\\frac{\\sum_{i=1}^n(x_i - \\overline{x})^2}{n-1}} \\] \nVariance is the average of squared deviation from the sample mean \\(\\overline{x}\\) or the mean squared deviation from the mean.\nSD is the root mean squared deviation from the mean. It measures, on average, how far the data spread out around the average."
  },
  {
    "objectID": "slides/05-data-discription-slides.html#compute-variance-and-sd",
    "href": "slides/05-data-discription-slides.html#compute-variance-and-sd",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Compute Variance and SD",
    "text": "Compute Variance and SD\n\nvar(body_mass)\n\n[1] 643131\n\nsqrt(var(body_mass))\n\n[1] 802\n\nsd(body_mass)\n\n[1] 802"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#visualizing-data-variation-boxplot",
    "href": "slides/05-data-discription-slides.html#visualizing-data-variation-boxplot",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Visualizing Data Variation: Boxplot",
    "text": "Visualizing Data Variation: Boxplot\nWhen plotting the whiskers,\n\nminimum value in the data means the minimal value that is not an potential outlier.\nmaximum value in the data means the maximal value that is not an potential outlier.\n\n\n\n\n\n\nhttps://www.leansigmacorporation.com/box-plot-with-minitab/\n\n\n\n\n\nTo Visualize Data Variation, we can make a so-called Boxplot.\nThe plot has a box in the middle, and so-called whiskers that are these two straight lines connected to the box.\nLet‚Äôs look at the box first. We have 3 vertical lines here. The lines from left to right indicate Q1, Q2 or the median, and Q3.\nSo the length of the box shows the IQR.\nNow let‚Äôs look at the whiskers.\nThe upper limit of the whisker is the smaller one of the maximum of values and Q3 + 1.5 IQR\nThe lower limit of the whisker on the left is the larger one of the minimum of values and Q1 - 1.5 IQR\nFor any data values that are greater than Q3 + 1.5 IQR or smaller than Q1 - 1.5 IQR, we show them as a point.\nBasically those points are far from the center of the data, and we could potentially treat them as extreme values or outliers."
  },
  {
    "objectID": "slides/05-data-discription-slides.html#body-mass-boxplot",
    "href": "slides/05-data-discription-slides.html#body-mass-boxplot",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Body Mass Boxplot",
    "text": "Body Mass Boxplot\n\n\nHere shows the boxplot of the interest rate data.\nYou can see that there are two very high interest rates in the data. They are 25% or more.\nAnd the median interest rate is just about 10%. And so these two data points are suspected outliers. They are way far from the the median or the most data points.\nIf"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#boxplot-in-r",
    "href": "slides/05-data-discription-slides.html#boxplot-in-r",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Boxplot in R",
    "text": "Boxplot in R\n\n\n\nboxplot(body_mass, ylab = \"Body Mass (g)\")\n\n\n\n\n\n\n\n\n\nrange(body_mass)\n\n[1] 2700 6300\n\nQ3 &lt;- quantile(body_mass, probs = 0.75, \n               names = FALSE)\nQ1 &lt;- quantile(body_mass, probs = 0.25, \n               names = FALSE)\nIQR &lt;- Q3 - Q1\nQ1 - 1.5 * IQR\n\n[1] 1750\n\nQ3 + 1.5 * IQR\n\n[1] 6550\n\n\n\n\nIn R, we can create a boxplot using boxplot(). sort(int_rate, decreasing = TRUE)[1:5] sort(int_rate)[1:5]"
  },
  {
    "objectID": "slides/05-data-discription-slides.html#section-3",
    "href": "slides/05-data-discription-slides.html#section-3",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "",
    "text": "For the penguins data,\n\nMake a boxplot for the variable bill_depth_mm.\nCompute the minimum, Q1, Q2, Q3, and maximum values of bill_depth_mm. (Hint: summary() function is pretty useful! üëç)\n\n\n\n\n\n\n\n\n\n\n\n  \n    ‚àí\n    +\n \n 02:00"
  },
  {
    "objectID": "slides/05-data-discription.html#descriptive-statistics-data-summary",
    "href": "slides/05-data-discription.html#descriptive-statistics-data-summary",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Descriptive Statistics (Data Summary)",
    "text": "Descriptive Statistics (Data Summary)\n\nBefore doing inferential statistics, let‚Äôs first learn to understand our data by describing or summarizing it using a table, graph, or some important measures, so that appropriate methods can be performed for better inference results."
  },
  {
    "objectID": "slides/05-data-discription.html#frequency-table-for-categorical-variable",
    "href": "slides/05-data-discription.html#frequency-table-for-categorical-variable",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Frequency Table for Categorical Variable",
    "text": "Frequency Table for Categorical Variable\n\nA frequency table (frequency distribution) lists variable values individually for categorical data along with their corresponding number of times occurred in the data (frequencies or counts).\nFrequency table for categorical data with \\(n\\) data values:\n\n\n\nCategory name\nFrequency\nRelative Frequency\n\n\n\n\\(C_1\\)\n\\(f_1\\)\n\\(f_1/n\\)\n\n\n\\(C_2\\)\n\\(f_2\\)\n\\(f_2/n\\)\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\\(C_k\\)\n\\(f_k\\)\n\\(f_k/n\\)"
  },
  {
    "objectID": "slides/05-data-discription.html#frequency-table-for-categorical-variable-1",
    "href": "slides/05-data-discription.html#frequency-table-for-categorical-variable-1",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Frequency Table for Categorical Variable",
    "text": "Frequency Table for Categorical Variable\n\nA frequency table (frequency distribution) lists variable values individually for categorical data along with their corresponding number of times occurred in the data (frequencies or counts).\nExample: A categorical variable color that has three categories\n\n\n\nCategory name\nFrequency\nRelative Frequency\n\n\n\nRed üî¥\n8\n8/50 = 0.16\n\n\nBlue üîµ\n26\n26/50 = 0.52\n\n\nBlack ‚ö´\n16\n16/50 = 0.32"
  },
  {
    "objectID": "slides/05-data-discription.html#r-packages",
    "href": "slides/05-data-discription.html#r-packages",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "R Packages üì¶",
    "text": "R Packages üì¶\n\n\n\nPackages wrap up reusable R functions, the documentation that describes how to use them, and data sets all together.\nAs of August 2025, there are about 22510 R packages available on CRAN (the Comprehensive R Archive Network)!\n\n\n\n\n\n\n\n\n\n\n\n\npalmerpenguins package"
  },
  {
    "objectID": "slides/05-data-discription.html#categorical-frequency-table-palmerpenguins-package",
    "href": "slides/05-data-discription.html#categorical-frequency-table-palmerpenguins-package",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Categorical Frequency Table palmerpenguins package \n",
    "text": "Categorical Frequency Table palmerpenguins package \n\n\n\n\nlibrary(palmerpenguins)\n\n\nstr(penguins)\n\ntibble [344 √ó 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\nx &lt;- penguins[, \"species\"]"
  },
  {
    "objectID": "slides/05-data-discription.html#categorical-frequency-table-species",
    "href": "slides/05-data-discription.html#categorical-frequency-table-species",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Categorical Frequency Table: species\n",
    "text": "Categorical Frequency Table: species\n\n\n## frequency table\ntable(x)\n\nspecies\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n\n\n\n\n\n\n\n\n\n\n\nIf we want to create a frequency table shown in definition, which R data structure we can use?"
  },
  {
    "objectID": "slides/05-data-discription.html#categorical-frequency-table-in-r-species",
    "href": "slides/05-data-discription.html#categorical-frequency-table-in-r-species",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Categorical Frequency Table in R: species\n",
    "text": "Categorical Frequency Table in R: species\n\n\nfreq &lt;- table(x)\nrel_freq &lt;- freq / sum(freq)\ncbind(freq, rel_freq)\n\n          freq rel_freq\nAdelie     152    0.442\nChinstrap   68    0.198\nGentoo     124    0.360"
  },
  {
    "objectID": "slides/05-data-discription.html#visualizing-a-frequency-table-bar-chart",
    "href": "slides/05-data-discription.html#visualizing-a-frequency-table-bar-chart",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Visualizing a Frequency Table: Bar Chart",
    "text": "Visualizing a Frequency Table: Bar Chart\n\nbarplot(height = table(x), main = \"Bar Chart\", xlab = \"Penguin Species\")"
  },
  {
    "objectID": "slides/05-data-discription.html#pie-chart",
    "href": "slides/05-data-discription.html#pie-chart",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Pie Chart",
    "text": "Pie Chart\n\npie(x = table(x), main = \"Pie Chart\")"
  },
  {
    "objectID": "slides/05-data-discription.html#frequency-distribution-for-numerical-variables",
    "href": "slides/05-data-discription.html#frequency-distribution-for-numerical-variables",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Frequency Distribution for Numerical Variables",
    "text": "Frequency Distribution for Numerical Variables\n\nDivide the data into \\(k\\) non-overlapping groups of intervals (classes).\nConvert the data into \\(k\\) categories with an associated class interval.\nCount the number of measurements falling in a given class interval (class frequency).\n\n\n\nClass\nClass Interval\nFrequency\nRelative Frequency\n\n\n\n\\(1\\)\n\\([a_1, a_2]\\)\n\\(f_1\\)\n\\(f_1/n\\)\n\n\n\\(2\\)\n\\((a_2, a_3]\\)\n\\(f_2\\)\n\\(f_2/n\\)\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\\(k\\)\n\\((a_k, a_{k+1}]\\)\n\\(f_k\\)\n\\(f_k/n\\)\n\n\n\n\n\n\\((a_2 - a_1) = (a_3 - a_2) = \\cdots = (a_{k+1} - a_k)\\). All class widths are the same!"
  },
  {
    "objectID": "slides/05-data-discription.html#frequency-distribution-for-numerical-variables-1",
    "href": "slides/05-data-discription.html#frequency-distribution-for-numerical-variables-1",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Frequency Distribution for Numerical Variables",
    "text": "Frequency Distribution for Numerical Variables\n\nDivide the data into \\(k\\) non-overlapping groups of intervals (classes).\nConvert the data into \\(k\\) categories with an associated class interval.\nCount the number of measurements falling in a given class interval (class frequency).\n\n\n\nClass\nClass Interval\nFrequency\nRelative Frequency\n\n\n\n\\(1\\)\n\\([80, 100]\\)\n\\(2\\)\n\\(2/50\\)\n\n\n\\(2\\)\n\\((100, 120]\\)\n\\(4\\)\n\\(4/50\\)\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\\(8\\)\n\\((220, 240]\\)\n\\(3\\)\n\\(3/50\\)"
  },
  {
    "objectID": "slides/05-data-discription.html#section-1",
    "href": "slides/05-data-discription.html#section-1",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "",
    "text": "Can our grade conversion be used for creating a frequency distribution?\n\n\n\n\n\nGrade\nPercentage\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[73, 77)\n\n\nC-\n[70, 73)\n\n\nD+\n[65, 70)\n\n\nD\n[60, 65)\n\n\nF\n[0, 60)"
  },
  {
    "objectID": "slides/05-data-discription.html#body-mass-grams-in-data-penguins",
    "href": "slides/05-data-discription.html#body-mass-grams-in-data-penguins",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Body Mass (Grams) in Data penguins\n",
    "text": "Body Mass (Grams) in Data penguins\n\n\nbody_mass &lt;- penguins$body_mass_g\nhead(body_mass, 20)\n\n [1] 3750 3800 3250   NA 3450 3650 3625 4675 3475 4250 3300 3700 3200 3800 4400\n[16] 3700 3450 4500 3325 4200\n\nbody_mass &lt;- body_mass[complete.cases(body_mass)]"
  },
  {
    "objectID": "slides/05-data-discription.html#frequency-distribution-of-body-mass",
    "href": "slides/05-data-discription.html#frequency-distribution-of-body-mass",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Frequency Distribution of Body Mass",
    "text": "Frequency Distribution of Body Mass\n\n\n\n\n\n Class   Class_Intvl Freq Rel_Freq\n     1 2700g - 3000g   11     0.03\n     2 3000g - 3300g   29     0.08\n     3 3300g - 3600g   57     0.17\n     4 3600g - 3900g   57     0.17\n     5 3900g - 4200g   39     0.11\n     6 4200g - 4500g   34     0.10\n     7 4500g - 4800g   34     0.10\n     8 4800g - 5100g   26     0.08\n     9 5100g - 5400g   21     0.06\n    10 5400g - 5700g   22     0.06\n    11 5700g - 6000g   10     0.03\n    12 6000g - 6300g    2     0.01\n\n\n\n\nrange(body_mass)\n\n[1] 2700 6300\n\n\n\n\nAll class widths are the same!\nNumber of classes should not be too big or too small.\nThe lower limit of the 1st class should not be greater than the minimum value of the data.\nThe upper limit of the last class should not be smaller than the maximum value of the data.\n\n\n\n. . .\n\nWonder how we choose the number of classes or the class width?\n\n. . .\n\nR decide the number for us when we visualize the frequency distribution by a histogram."
  },
  {
    "objectID": "slides/05-data-discription.html#visualizing-frequency-distribution-by-a-histogram",
    "href": "slides/05-data-discription.html#visualizing-frequency-distribution-by-a-histogram",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Visualizing Frequency Distribution by a Histogram",
    "text": "Visualizing Frequency Distribution by a Histogram\n\n\nUse default breaks (no need to specify)\n\nhist(x = body_mass, \n     xlab = \"Body Mass (gram)\",\n     main = \"Histogram (Defualt)\")\n\n\n\n\n\n\n\n\n\nUse customized breaks\n\n\nclass_boundary\n\n [1] 2700 3000 3300 3600 3900 4200 4500 4800 5100 5400 5700 6000 6300\n\nhist(x = body_mass, \n     breaks = class_boundary, #&lt;&lt;\n     xlab = \"Body Mass (gram)\",\n     main = \"Histogram (Ours)\")"
  },
  {
    "objectID": "slides/05-data-discription.html#skewness",
    "href": "slides/05-data-discription.html#skewness",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Skewness",
    "text": "Skewness\n\nKey characteristics of distributions includes shape, center and spread.\nSkewness provides a way to summarize the shape of a distribution."
  },
  {
    "objectID": "slides/05-data-discription.html#remembering-skewness",
    "href": "slides/05-data-discription.html#remembering-skewness",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Remembering Skewness",
    "text": "Remembering Skewness\n\n\n\nIs the body mass histogram left skewed or right skewed?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiostatistics for the Biological and Health Sciences p.53"
  },
  {
    "objectID": "slides/05-data-discription.html#scatterplot-for-two-numerical-variables",
    "href": "slides/05-data-discription.html#scatterplot-for-two-numerical-variables",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Scatterplot for Two Numerical Variables",
    "text": "Scatterplot for Two Numerical Variables\n\n\nA scatterplot provides a case-by-case view of data for two numerical variables.\n\n\n\n\nplot(x = penguins$bill_length_mm, y = penguins$bill_depth_mm,\n     xlab = \"Bill Length\", ylab = \"Bill Depth\",\n     pch = 16, col = 4)"
  },
  {
    "objectID": "slides/05-data-discription.html#section-2",
    "href": "slides/05-data-discription.html#section-2",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "",
    "text": "For the penguins data, do\n\nPie chart for variable island.\nHistogram for variable flipper_length_mm. Discuss its shape.\nScatter plot for variables flipper_length_mm and bill_length_mm."
  },
  {
    "objectID": "slides/05-data-discription.html#numerical-summaries-of-data",
    "href": "slides/05-data-discription.html#numerical-summaries-of-data",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Numerical Summaries of Data",
    "text": "Numerical Summaries of Data\n\n\n\n\n\n\n\n\n\n\n\nIf you need to choose one value that represents the entire data, what value would you choose?\n\n\n\n\nMeasure of Center: We typically use the middle point. (What does ‚Äúmiddle‚Äù mean?)\nMeasure of Variation: What values tell us how much variation a variable has?"
  },
  {
    "objectID": "slides/05-data-discription.html#measures-of-center-mean",
    "href": "slides/05-data-discription.html#measures-of-center-mean",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Measures of Center: Mean",
    "text": "Measures of Center: Mean\n\nThe (arithmetic) mean or average is adding up all of the values, then dividing by the total number of them.\nLet \\(x_1, x_2, \\dots, x_n\\) denote the measurements observed in a sample of size \\(n\\). Then the sample mean is defined as \\[\\overline{x} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{x_1 + x_2 + \\dots + x_n}{n}\\]\nIn the body mass example, \\[\\overline{x} = \\frac{3750 + 3800 + \\cdots + 3775}{342} = 4202\\] \n\n\nmean(body_mass)\n\n[1] 4202\n\n\n\n\nMean balances data Values. It means that the sum of deviations from the mean is 0."
  },
  {
    "objectID": "slides/05-data-discription.html#balancing-point",
    "href": "slides/05-data-discription.html#balancing-point",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Balancing Point",
    "text": "Balancing Point\n\nThink of the mean as the balancing point of the distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean is a measure of center in the sense that it can be viewed as the balancing point of the distribution.\nThis is the distribution of the interest rate.\nAnd if you imagine that we put all those data points on a see-saw, the sample mean is the balancing point that keeps the see-saw balanced horizontally."
  },
  {
    "objectID": "slides/05-data-discription.html#measures-of-center-median",
    "href": "slides/05-data-discription.html#measures-of-center-median",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Measures of Center: Median",
    "text": "Measures of Center: Median\n\nMedian: the middle value when data values are sorted.\nHalf of the values are less than or equal to the median, and the other half are greater than it.\nTo find the median, we first sort the values.\n\n\\(n\\) is odd: the median is located in the exact middle of the ordered values.\n\n Data: (0, 2, 10, 14, 8) \n Sorted Data: (0, 2, 8, 10, 14) \n The median is \\(8\\) \n\n\n\n. . .\n\n\n\\(n\\) is even: the median is the average of the two middle numbers.\n\n Data: (0, 2, 10, 14, 8, 12) \n Sorted Data: (0, 2, 8, 10, 12, 14) \n The median is \\(\\frac{8 + 10}{2} = 9\\) \n\n\n\n\n\nThe Median is the middle value when data values are sorted from the lowest to highest.\nHalf of the values are less than or equal to the median, and the other half are greater than it.\nTo find the median (well without using any technology or computer), we first sort the values.\nIf \\(n\\) is odd, the median is located in the exact middle of the ordered values.\nThere are two on its right and two on its left. it is right on the middle.\nIf \\(n\\) is even, the median is the average of the two middle numbers.\n8 or 10 itself is not right on the middle."
  },
  {
    "objectID": "slides/05-data-discription.html#calculate-median-in-r",
    "href": "slides/05-data-discription.html#calculate-median-in-r",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Calculate Median in R",
    "text": "Calculate Median in R\n\nmedian(body_mass)  ## Compute the median using command median()\n\n[1] 4050\n\n\n. . .\n\n(sort_mass &lt;- sort(body_mass))  ## sort data\n\n  [1] 2700 2850 2850 2900 2900 2900 2900 2925 2975 3000 3000 3050 3050 3050 3050\n [16] 3075 3100 3150 3150 3150 3150 3175 3175 3200 3200 3200 3200 3200 3250 3250\n [31] 3250 3250 3250 3275 3300 3300 3300 3300 3300 3300 3325 3325 3325 3325 3325\n [46] 3350 3350 3350 3350 3350 3400 3400 3400 3400 3400 3400 3400 3400 3425 3425\n [61] 3450 3450 3450 3450 3450 3450 3450 3450 3475 3475 3475 3500 3500 3500 3500\n [76] 3500 3500 3500 3525 3525 3550 3550 3550 3550 3550 3550 3550 3550 3550 3575\n [91] 3600 3600 3600 3600 3600 3600 3600 3625 3650 3650 3650 3650 3650 3650 3675\n[106] 3675 3700 3700 3700 3700 3700 3700 3700 3700 3700 3700 3700 3725 3725 3725\n[121] 3750 3750 3750 3750 3750 3775 3775 3775 3775 3800 3800 3800 3800 3800 3800\n[136] 3800 3800 3800 3800 3800 3800 3825 3850 3875 3900 3900 3900 3900 3900 3900\n[151] 3900 3900 3900 3900 3950 3950 3950 3950 3950 3950 3950 3950 3950 3950 3975\n[166] 4000 4000 4000 4000 4000 4050 4050 4050 4050 4050 4050 4075 4100 4100 4100\n[181] 4100 4100 4150 4150 4150 4150 4150 4150 4200 4200 4200 4200 4200 4250 4250\n[196] 4250 4250 4250 4275 4300 4300 4300 4300 4300 4300 4300 4300 4350 4350 4375\n[211] 4400 4400 4400 4400 4400 4400 4400 4400 4450 4450 4450 4450 4450 4475 4500\n[226] 4500 4500 4550 4550 4575 4600 4600 4600 4600 4600 4625 4625 4650 4650 4650\n[241] 4650 4650 4675 4700 4700 4700 4700 4700 4700 4725 4725 4725 4750 4750 4750\n[256] 4750 4750 4775 4800 4800 4800 4850 4850 4850 4850 4875 4875 4875 4900 4900\n[271] 4925 4925 4950 4950 4975 5000 5000 5000 5000 5000 5000 5050 5050 5050 5100\n[286] 5100 5100 5150 5150 5200 5200 5200 5200 5250 5250 5250 5300 5300 5300 5300\n[301] 5350 5350 5350 5400 5400 5400 5400 5400 5450 5500 5500 5500 5500 5500 5550\n[316] 5550 5550 5550 5550 5550 5600 5600 5650 5650 5650 5700 5700 5700 5700 5700\n[331] 5750 5800 5800 5850 5850 5850 5950 5950 6000 6000 6050 6300\n\nlength(body_mass)  ## Check sample size is odd or even\n\n[1] 342\n\n(sort_mass[171] + sort_mass[172]) / 2  ## Verify the answer\n\n[1] 4050\n\n\n. . .\n\n(body_mass[171] + body_mass[172]) / 2  ## Using un-sorted data leads to a wrong answer!!\n\n[1] 5525"
  },
  {
    "objectID": "slides/05-data-discription.html#measures-of-center-mode",
    "href": "slides/05-data-discription.html#measures-of-center-mode",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Measures of Center: Mode",
    "text": "Measures of Center: Mode\n\nMode: the value that occurs most frequently.\nFor continuous numerical data, it is common to have no observations with the same value.\nPractical definition: A mode is represented by a prominent peak in the distribution.\n\n\n## Create a frequency table \ntable_data &lt;- table(body_mass)\n\n. . .\n\n## Sort the table to find the mode that occurs most frequently\n## the number that happens most frequently will be the first one\nsort(table_data, decreasing = TRUE)\n\nbody_mass\n3800 3700 3900 3950 3550 3400 3450 4300 4400 3500 3600 3300 3650 4050 4150 4700 \n  12   11   10   10    9    8    8    8    8    7    7    6    6    6    6    6 \n5000 5550 3200 3250 3325 3350 3750 4000 4100 4200 4250 4450 4600 4650 4750 5400 \n   6    6    5    5    5    5    5    5    5    5    5    5    5    5    5    5 \n5500 5700 2900 3050 3150 3775 4850 5200 5300 3475 3725 4500 4725 4800 4875 5050 \n   5    5    4    4    4    4    4    4    4    3    3    3    3    3    3    3 \n5100 5250 5350 5650 5850 2850 3000 3175 3425 3525 3675 4350 4550 4625 4900 4925 \n   3    3    3    3    3    2    2    2    2    2    2    2    2    2    2    2 \n4950 5150 5600 5800 5950 6000 2700 2925 2975 3075 3100 3275 3575 3625 3825 3850 \n   2    2    2    2    2    2    1    1    1    1    1    1    1    1    1    1 \n3875 3975 4075 4275 4375 4475 4575 4675 4775 4975 5450 5750 6050 6300 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n\n\n\n\nMathematically, the mode is the value in the data that occurs most frequently.\nFor continous numerical data that take on infinitely many possible values, it is common to have no observations with the same value in a data set, making this definition impractical.\nPractical definition: A mode is represented by a prominent peak in the distribution."
  },
  {
    "objectID": "slides/05-data-discription.html#comparison-of-mean-median-and-mode",
    "href": "slides/05-data-discription.html#comparison-of-mean-median-and-mode",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Comparison of Mean, Median and Mode",
    "text": "Comparison of Mean, Median and Mode\n\nMean is sensitive to extreme values (outliers).\nMedian/mode is more robust than mean.\n\n\nhead(data_extreme, 10)\n\n [1] 37500  3800  3250  3450  3650  3625  4675  3475  4250  3300\n\n\n\nmean(data_extreme)  ## Large mean! Original mean is 4202\n\n[1] 4300\n\nmedian(data_extreme)  ## Median does not change!\n\n[1] 4050\n\nnames(sort(table(data_extreme), decreasing = TRUE))[1] ## Mode does not change too!\n\n[1] \"3800\"\n\n\n\n\nMean is sensitive to extreme values (outliers).\nMedian/mode is more robust than mean, meaning that median and mode will not change or not change much if few data values change."
  },
  {
    "objectID": "slides/05-data-discription.html#comparison-of-mean-median-and-mode-1",
    "href": "slides/05-data-discription.html#comparison-of-mean-median-and-mode-1",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Comparison of Mean, Median and Mode",
    "text": "Comparison of Mean, Median and Mode\n\nMode is applicable for both categorical and numerical data, while median and mean work for numerical data only.\nThere may be more than one mode, but there is only one median and one mean.\n\n\n\n\n\n\n\n\n\n\n\nMode is applicable for both categorical and numerical data, while median and mean work for numerical data only.\n\nShow Categorical homeownership freq table\nShow int_rate mode\n\n\nThere may be more than one mode, but there is only one median and one mean.\nmean is sensitive to extreme values. The values at the long tails are extreme values comparing to the values in the middle.\nThis forces the mean to be pulled toward the left in order to balance the data.\nMode happens at the peak of the distribution"
  },
  {
    "objectID": "slides/05-data-discription.html#measures-of-variation",
    "href": "slides/05-data-discription.html#measures-of-variation",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Measures of Variation",
    "text": "Measures of Variation\n\n\n\n\n\n\n\n\n\n\nDone the discussion of measure of center.\nNow let‚Äôs see how we quantify the variation of data.\nHere we have three frequency distributions with the same mean, median and mode because their distribution is symmetric.\nBut from top to bottom, the data have small, median and large variation.\nBasically larger variation means the data values spread out more.\nSo although the data mean is right here, but in general, data values are not very close to the mean. Their deviation from the sample mean is large. OK."
  },
  {
    "objectID": "slides/05-data-discription.html#measures-of-variation-p-th-percentile",
    "href": "slides/05-data-discription.html#measures-of-variation-p-th-percentile",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Measures of Variation: p-th percentile",
    "text": "Measures of Variation: p-th percentile\n\n\n\n\np-th percentile (quantile): a data value such that\n\nat most \\(p\\%\\) of the values are below it\nat most \\((1-p)\\%\\) of the values are above it\n\n\n\n\n\nTwo datasets with the same mean 20.\n\nOne data set has 99-th percentile = 30, and 1-st percentile = 10.\nThe other has 99-th percentile = 40, and 1-st percentile = 0.\n\n\nWhich data have larger variation?\n\n\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/ACT_(test)\n\n\n\n\n\n\n\n\nThe first Measure of Variation is p-th percentile\n\np-th percentile (quantile): a data value such that\n\nat most \\(p\\%\\) of the values are below it\nat most \\((1-p)\\%\\) of the values are above it"
  },
  {
    "objectID": "slides/05-data-discription.html#measures-of-variation-interquartile-range-iqr",
    "href": "slides/05-data-discription.html#measures-of-variation-interquartile-range-iqr",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Measures of Variation: Interquartile Range (IQR)",
    "text": "Measures of Variation: Interquartile Range (IQR)\n\nFirst Quartile (Q1): the 25-th percentile\nSecond Quartile (Q2): the 50-th percentile (Median)\nThird Quartile (Q3): the 75-th percentile\nInterquartile Range (IQR): Q3 - Q1\n\n\n\n\n## Use quantile() to find any percentile \n## through specifying the probability\nquantile(x = body_mass, \n         probs = c(0.25, 0.5, 0.75))\n\n 25%  50%  75% \n3550 4050 4750 \n\n## IQR by definition\nquantile(x = body_mass, probs = 0.75) - \n  quantile(x = body_mass, probs = 0.25) \n\n 75% \n1200 \n\n\n\n\n## IQR()\nIQR(body_mass)  \n\n[1] 1200\n\n## summary() to get the numeric summary\nsummary(body_mass)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2700    3550    4050    4202    4750    6300 \n\n\n\nLarger IQR means more or less variation?\n\n\n\n\nThe range of the middle 50% of the data."
  },
  {
    "objectID": "slides/05-data-discription.html#variance-and-standard-deviation",
    "href": "slides/05-data-discription.html#variance-and-standard-deviation",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Variance and Standard Deviation",
    "text": "Variance and Standard Deviation\n\nThe distance of an observation from its mean, \\(x_i - \\overline{x}\\), its deviation.\nSample Variance is defined as \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})^2}{n-1} \\]\nSample Standard Deviation (SD) is defined as the square root of the variance \\[ s = \\sqrt{\\frac{\\sum_{i=1}^n(x_i - \\overline{x})^2}{n-1}} \\] \nVariance is the average of squared deviation from the sample mean \\(\\overline{x}\\) or the mean squared deviation from the mean.\nSD is the root mean squared deviation from the mean. It measures, on average, how far the data spread out around the average."
  },
  {
    "objectID": "slides/05-data-discription.html#compute-variance-and-sd",
    "href": "slides/05-data-discription.html#compute-variance-and-sd",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Compute Variance and SD",
    "text": "Compute Variance and SD\n\nvar(body_mass)\n\n[1] 643131\n\nsqrt(var(body_mass))\n\n[1] 802\n\nsd(body_mass)\n\n[1] 802"
  },
  {
    "objectID": "slides/05-data-discription.html#visualizing-data-variation-boxplot",
    "href": "slides/05-data-discription.html#visualizing-data-variation-boxplot",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Visualizing Data Variation: Boxplot",
    "text": "Visualizing Data Variation: Boxplot\nWhen plotting the whiskers,\n\nminimum value in the data means the minimal value that is not an potential outlier.\nmaximum value in the data means the maximal value that is not an potential outlier.\n\n\n\n\n\n\nhttps://www.leansigmacorporation.com/box-plot-with-minitab/\n\n\n\n\n\n\nTo Visualize Data Variation, we can make a so-called Boxplot.\nThe plot has a box in the middle, and so-called whiskers that are these two straight lines connected to the box.\nLet‚Äôs look at the box first. We have 3 vertical lines here. The lines from left to right indicate Q1, Q2 or the median, and Q3.\nSo the length of the box shows the IQR.\nNow let‚Äôs look at the whiskers.\nThe upper limit of the whisker is the smaller one of the maximum of values and Q3 + 1.5 IQR\nThe lower limit of the whisker on the left is the larger one of the minimum of values and Q1 - 1.5 IQR\nFor any data values that are greater than Q3 + 1.5 IQR or smaller than Q1 - 1.5 IQR, we show them as a point.\nBasically those points are far from the center of the data, and we could potentially treat them as extreme values or outliers."
  },
  {
    "objectID": "slides/05-data-discription.html#body-mass-boxplot",
    "href": "slides/05-data-discription.html#body-mass-boxplot",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Body Mass Boxplot",
    "text": "Body Mass Boxplot\n\n\n\n\n\n\n\n\n\n\nHere shows the boxplot of the interest rate data.\nYou can see that there are two very high interest rates in the data. They are 25% or more.\nAnd the median interest rate is just about 10%. And so these two data points are suspected outliers. They are way far from the the median or the most data points.\nIf"
  },
  {
    "objectID": "slides/05-data-discription.html#boxplot-in-r",
    "href": "slides/05-data-discription.html#boxplot-in-r",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "Boxplot in R",
    "text": "Boxplot in R\n\n\n\nboxplot(body_mass, ylab = \"Body Mass (g)\")\n\n\n\n\n\n\n\n\n\nrange(body_mass)\n\n[1] 2700 6300\n\nQ3 &lt;- quantile(body_mass, probs = 0.75, \n               names = FALSE)\nQ1 &lt;- quantile(body_mass, probs = 0.25, \n               names = FALSE)\nIQR &lt;- Q3 - Q1\nQ1 - 1.5 * IQR\n\n[1] 1750\n\nQ3 + 1.5 * IQR\n\n[1] 6550\n\n\n\n\n\n\nIn R, we can create a boxplot using boxplot(). sort(int_rate, decreasing = TRUE)[1:5] sort(int_rate)[1:5]"
  },
  {
    "objectID": "slides/05-data-discription.html#section-3",
    "href": "slides/05-data-discription.html#section-3",
    "title": "Describing Data üë®‚Äçüíª",
    "section": "",
    "text": "For the penguins data,\n\nMake a boxplot for the variable bill_depth_mm.\nCompute the minimum, Q1, Q2, Q3, and maximum values of bill_depth_mm. (Hint: summary() function is pretty useful! üëç)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 4720 (MSSC 5720) Statistical Methods Spring 2026",
    "section": "",
    "text": "This schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nMaterials\nReading\nR Code\nHomework\nWorksheet\nQuiz\nExam/Project\n\n\n\n1\nTue, Jan 13\nSyllabus/Overview of Statistics\nüñ•Ô∏è syllabusüñ•Ô∏è statistics and data\n\nSyllabusIS Ch1GenAI Video\n\n\n\n\n\n\n\n\n\nThu, Jan 15\nStatistical Studies and Data Collection; Data Type\n\nIS Ch 2\n\n‚úçÔ∏è HW 1 (due 1/23)\n\n\n\n\n\n2\nTue, Jan 20\nR Software and Computation\nüñ•Ô∏è R and RStudioüñ•Ô∏è data type and structure\nIS Ch 3\n\n (data type and structure)\n\n\n\n\n\n\n\nThu, Jan 22\nSummarizing Data: Centrality\nüñ•Ô∏è data summaryüëâhistogram breaksüëâdata extremes\nIS Ch 5 - 6\n\n (data graphics)\n\n\n\n\n\n\n3\nTue, Jan 27\nSummarizing Data: Variation\n\n\n\n (data numerics)\n\n\n\n\n\n\n\nThu, Jan 29\nProbability Fundamentals\nüñ•Ô∏è probabilityüëâvenn diagram\nIS Ch 7 - 8\n\n‚úçÔ∏è HW 2 (due 2/6)\n\n\n\n\n\n4\nTue, Feb 3\nConditional Probability, Independence, Bayes Formula\n\n\n\n\n\n\n\n\n\n\nThu, Feb 5\nRandom Variables, Discrete Probability Distributions\nüñ•Ô∏è distribution üëâbinomialüëâpoisson\nIS Ch 9 - 10\n\n (discrete distribution)\n‚úçÔ∏è HW 3 (due 2/13)\n\n‚úÖ Quiz 1\n\n\n\n5\nTue, Feb 10\nContinuous Probability Distributions\nüëânormal\nIS Ch 11\n\n (continuous distribution)\n\n\n\n\n\n\n\nThu, Feb 12\nSampling Distributions\nüñ•Ô∏è sampling-dist-clt\nIS Ch 12\n\n (sampling distribution and CLT)\n\n\n\n\n\n\n6\nTue, Feb 17\nCentral Limit Theorem (CLT)\nüëâCLT üëâRolling dice\nIS Ch 13\n\n\n\n\n\n\n\n\nThu, Feb 19\nConfidence Intervals\nüñ•Ô∏è confidence interval  üëâCI\nIS Ch 14\n\n (confidence interval)\n‚úçÔ∏è HW 4 (due 2/27)\n\n\n\n\n\n7\nTue, Feb 24\nConfidence Intervals\nüëâStudent-t\n\n\n\n\n\n\n\n\n\nThu, Feb 26\nHypothesis Testing\nüñ•Ô∏è testing üëâtesting\nIS Ch 16\n\n\n\n‚úÖ Quiz 2\n\n\n\n8\nTue, Mar 3\nIn-class Exam 1\n\n\n\n\n\n\n‚úÖ Take-home Exam 1\n\n\n\nThu, Mar 5\nHypothesis Testing\n\n\n\n (testing)\n\n\n\n\n\n\n9\nTue, Mar 10\nNO CLASS: Spring break\n\n\n\n\n\n\n\n\n\n\nThu, Mar 12\nNO CLASS: Spring break\n\n\n\n\n\n\n\n\n\n10\nTue, Mar 17\nHypothesis Testing\n\n\n\n\n\n\n\n\n\n\nThu, Mar 19\nTwo Population Means: Matched Pairs\nüñ•Ô∏è two samples\nIS Ch 17\n\n (two samples)\n‚úçÔ∏è HW 5 (due 3/27)\n\n\n\n\n\n11\nTue, Mar 24\nTwo Population Means: Independent Samples\n\n\n\n\n\n\n\n\n\n\nThu, Mar 26\nInference for One Population Variance\nüñ•Ô∏è variances  üëâchi-squared\nIS Ch 18\n\n (variances)\n‚úçÔ∏è HW 6 (due 4/3)\n\n‚úÖ Quiz 3\n\n\n\n12\nTue, Mar 31\nInference about Two Population Variances\nüëâ f\n\n\n\n\n\n\n\n\n\nThu, Apr 2\nNO CLASS: Easter break\n\n\n\n\n\n\n\n\n\n13\nTue, Apr 7\nAnalysis of Variance (ANOVA) Rationale\nüñ•Ô∏è ANOVA\nIS Ch 24\n\n (ANOVA data)\n\n\n\n\n\n\n\nThu, Apr 9\nAnalysis of Variance (ANOVA) Procedure\n\n\n\n‚úçÔ∏è HW 7 (due 4/17)\n\n\n\n\n\n14\nTue, Apr 14\nCorrelation; Overview of Regression\nüñ•Ô∏è correlation  üñ•Ô∏è regression  üëâ regression fitting\n\nIS Ch 27.1; IS Ch 27.2\n\n\n (correlation and regression)\n\n\n\n\n\n\n\nThu, Apr 16\nSimple Linear Regression: Fitting and Inference\nüëâ sampling distribution of coefficients\nIS Ch 27.3 - 27.5\n\n\n\n‚úÖ Quiz 4\n\n\n\n15\nTue, Apr 21\nIn-class Exam 2\n\n\n\n\n\n\n‚úÖ Take-home Exam 2\n\n\n\nThu, Apr 23\nSimple Linear Regression: Prediction\n\nIS Ch 27.6\n‚úçÔ∏è HW 8 (due 5/1)\n\n\n\n\n\n\n16\nTue, Apr 28\nTo Be Determined\n\n\n\n\n\n\nüìÇ Final Project Guideline\n\n\n\nTue, Apr 30\nTo Be Determined\n\n\n\n\n\n\n\n\n\n17\nFri, May 8\nFinal Project\n\n\n\n\n\n\n\n\n\n\n\nDr.¬†Yu reserves the right to make changes to the schedule.",
    "crumbs": [
      "Course Information",
      "Schedule"
    ]
  },
  {
    "objectID": "ojs/model-reg-beta-ojs.html",
    "href": "ojs/model-reg-beta-ojs.html",
    "title": "Sampling Distribution of Beta 0 and Beta 1 of Simple Linear Regression",
    "section": "",
    "text": "viewof n_1 = Inputs.range([10, 1000], {value: 100, step: 1, label: tex`\\text{sample size, }n`});\nviewof beta0_1 = Inputs.range([-2, 2], {value: -1, step: 0.1, label: tex`\\text{intercept, }\\beta_0`})\nviewof beta1_1 = Inputs.range([-2, 2], {value: 1, step: 0.1, label: tex`\\text{slope, }\\beta_1`})\nviewof sigmaepsilon_1 = Inputs.range([0, 1], {value: 0.5, step: 0.1, label: tex`\\text{sd of error, }\\sigma`})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof sampleno_1 = Scrubber(d3.ticks(1, 1000, 1000), {\n  autoplay: false,\n  loop: false,\n  delay: 1000,\n  initial: 199,\n  format: x =&gt; `dataset no: ${x.toFixed(0)}`\n})\n\n\n\n\n\n\n\nestimates_1 = tex`\\text{Estimated intercept: } b_0 = ${data_estimates_1[1][sampleno_1-1].intercept.toPrecision(4)} \\\\ \\text{Estimated slope: } b_1 =${data_estimates_1[1][sampleno_1-1].lutning.toPrecision(4)}`\n\n\n\n\n\n\n\nimport {lm} from \"@chrispahm/linear-models-in-observable-notebooks\"\nimport {summary} from \"@chrispahm/linear-models-in-observable-notebooks\"\nimport {Histogram} from \"@d3/histogram\"\nimport {Scrubber} from \"@mbostock/scrubber\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njstat = require('jstat')\n\n\n\n\n\n\n\nmath = require('mathjs')\n\n\n\n\n\n\n\nfunction simulate_and_estimate(n, beta0, beta1, sigmaepsilon, nrep){\n  const x = d3.range(-1, 1, 2/n);\n  const xbar = d3.mean(x)\n  const SSx = d3.sum(math.dotPow(math.subtract(x, xbar), 2))\n  var data = [];\n  var coef = [];\n  for (let j = 1; j &lt;= nrep; j++){\n    var dataset = x.map(x =&gt; ({sample_id: j, x: x, y: beta0 + beta1*x + jstat.normal.sample(0,sigmaepsilon)}))\n    data.push(dataset);\n    let y = dataset.map(d =&gt; d.y)\n    let ybar = d3.mean(y)\n    let SSxy = d3.sum(math.dotMultiply(math.subtract(x, xbar), math.subtract(y, ybar))  )\n    let b1 = SSxy/SSx\n    let b0 = ybar - b1*xbar\n    //let linear_model = lm('y ~ x', dataset);\n    coef.push(\n      {\n        sample_id: j,\n        //intercept: linear_model.coefficients[0],\n        //lutning: linear_model.coefficients[1],\n        intercept: b0,\n        lutning: b1\n      }\n    )\n  }\n  return [data, coef]\n}\n\n\n\n\n\n\n\ndata_estimates_1 = simulate_and_estimate(n_1, beta0_1, beta1_1, sigmaepsilon_1, 1000)\n\n\n\n\n\n\n\n\n\n\n\n\nsamplingplots = html`&lt;html&gt;\n &lt;head&gt;\n &lt;/head&gt;\n &lt;body&gt;\n    &lt;div class=\"container\" style=\"display: flex; height: 800px;\"&gt;\n        &lt;div style=\"width: 50%\"&gt;\n            ${histintercept}\n        &lt;/div&gt;\n        &lt;div style=\"width: 50%\"&gt;\n            ${histlutning}\n        &lt;/div&gt;\n    &lt;/div&gt;\n &lt;/body&gt;\n&lt;/html&gt;\n`\n\n\n\n\n\n\n\nb0samp_range = [d3.min(data_estimates_1[1].map( d =&gt; d.intercept)),d3.max(data_estimates_1[1].map( d =&gt; d.intercept))]\n\nb1samp_range = [d3.min(data_estimates_1[1].map( d =&gt; d.lutning)),d3.max(data_estimates_1[1].map( d =&gt; d.lutning))]\n\nsampdist_theo = {\n  const x = d3.range(-1, 1, 2/n_1);\n  const xbar = d3.mean(x)\n  const sumsquares = d3.sum(math.dotPow(math.subtract(x, xbar), 2))\n  const stdb0 = sigmaepsilon_1*math.sqrt((1/n_1) + ((xbar**2)/sumsquares))\n  const stdb1 = sigmaepsilon_1/math.sqrt(sumsquares)\n  const x_b0 = d3.range(beta0_1 - 4*stdb0, beta0_1 + 4*stdb0, 1/500);\n  const x_b1 = d3.range(beta1_1 - 4*stdb1, beta1_1 + 4*stdb1, 1/500);\n  var pdfb0 = [];\n  var pdfb1 = [];\n  for (let j = 1; j &lt;= x_b0.length; j++){\n    pdfb0.push({\n      x: x_b0[j-1], \n      pdf: jstat.normal.pdf(x_b0[j-1], beta0_1, stdb0)\n    })\n  }\n  for (let j = 1; j &lt;= x_b1.length; j++){\n    pdfb1.push({\n      x: x_b1[j-1], \n      pdf: jstat.normal.pdf(x_b1[j-1], beta1_1, stdb1),\n    })\n  }\n  return  [pdfb0, pdfb1]\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhistintercept = Plot.plot({\n  width: 400, // or a dynamic value based on `width` variable\n  height: 300,\n  style: {fontSize: \"16px\"},\n  caption:html`\n  &lt;span style=\"color:midnightblue\"&gt;Sampling distribution for b&lt;sub&gt;0&lt;/sub&gt; &lt;/span&gt; &lt;br&gt;\n  &lt;span style=\"color:steelblue\"&gt;Histogram of estimates&lt;/span&gt;&lt;br&gt;\n&#128992; Population &beta;&lt;sub&gt;0&lt;/sub&gt; &lt;br&gt;\n&#128309; Estimated b&lt;sub&gt;0&lt;/sub&gt; for the current data.\n  `,\n  y: {axis: false},\n  x: {domain: [beta0_1 -0.5, beta0_1 + 0.5]},\n  marks: [\n    Plot.rectY(data_estimates_1[1].filter((d,i) =&gt; i&lt;= (sampleno_1-1)), Plot.binX({y: d =&gt; (d.length/sampleno_1)/(10/sampleno_1)}, {x: \"intercept\", thresholds: d3.range(b0samp_range[0], b0samp_range[1], 10/sampleno_1), fill: \"steelblue\", opacity: 0.5}) ),\n    Plot.ruleY([0]),\n    Plot.dot([{beta0: beta0_1}], {x: \"beta0\", y: 0, r: 5, fill: \"orange\"}),\n    Plot.dot([data_estimates_1[1][sampleno_1-1]], {x: \"intercept\", y: 0, r: 5, fill: \"steelblue\"}),\n    Plot.lineY(sampdist_theo[0], {x: \"x\", y: \"pdf\", stroke: \"midnightblue\", strokeWidth: 2.5})\n  ],\n})\n\n\n\n\n\n\n\nhistlutning = Plot.plot({\n  width: 400, // or a dynamic value based on `width` variable\n  height: 300,\n  style: {fontSize: \"16px\"},\n  caption:html` \n  &lt;span style=\"color:midnightblue\"&gt;Sampling distribution for b&lt;sub&gt;1&lt;/sub&gt;&lt;/span&gt; &lt;br&gt;\n  &lt;span style=\"color:steelblue\"&gt;Histogram of estimates&lt;/span&gt;&lt;br&gt;\n&#128992; Population &beta;&lt;sub&gt;1&lt;/sub&gt; &lt;br&gt;\n&#128309; Estimated b&lt;sub&gt;1&lt;/sub&gt; for the current data.\n  `,\n  y: {axis: false},\n  x: {domain: [beta1_1 -0.5, beta1_1 + 0.5]},\n  marks: [\n    Plot.rectY(data_estimates_1[1].filter((d,i) =&gt; i&lt;= (sampleno_1-1)), Plot.binX({y: d =&gt; (d.length/sampleno_1)/(10/sampleno_1)}, {x: \"lutning\", thresholds: d3.range(b1samp_range[0], b1samp_range[1], 10/sampleno_1), fill: \"steelblue\", opacity: 0.5}) ),\n    Plot.ruleY([0]),\n    Plot.dot([{beta1: beta1_1}], {x: \"beta1\", y: 0, r: 5, fill: \"orange\"}),\n    Plot.dot([data_estimates_1[1][sampleno_1-1]], {x: \"lutning\", y: 0, r: 5, fill: \"steelblue\"}),\n    Plot.lineY(sampdist_theo[1], {x: \"x\", y: \"pdf\", stroke: \"midnightblue\", strokeWidth: 2.5})\n  ],\n})"
  },
  {
    "objectID": "ojs/infer-ci-ojs.html",
    "href": "ojs/infer-ci-ojs.html",
    "title": "Simulation of Confidence Interval for Population Mean with Known Variance",
    "section": "",
    "text": "viewof settings = Inputs.form([\n  Inputs.range([-5, 5], {value: 1, label: tex`\\text{pop mean, }\\mu`, step: 0.01}),\n  Inputs.range([0.01, 5], {value: 1, label: tex`\\text{pop stdev, }\\sigma`, step: 0.01}),\n  Inputs.range([0, 99], {value: 90, label: \"confidence level\", step: 1}),\n  Inputs.range([0, 200], {value: 50, label: \"sample size, n\", step: 1})\n  ])\n\n\n\n\n\n\n\nviewof nrep = Scrubber(d3.ticks(1, 1000, 1000), {\n  autoplay: false,\n  loop: false,\n  initial: 1,\n  delay: 350,\n  format: x =&gt; `number of datasets = ${x.toFixed(0)}`\n})\n\n\n\n\n\n\n\ntextinfo = md`\nOut of a total of ${nrep} samples, ${d3.sum(data.map(d =&gt; d.rep &lt;= nrep && (mu &gt;= d.lower && mu &lt;= d.upper)))} (**&lt;span style=\"color:red\"&gt;${(d3.sum(data.map(d =&gt; d.rep &lt;= nrep && (mu &gt;= d.lower && mu &lt;= d.upper)))*100/nrep).toFixed(3)}%&lt;/span&gt;**) of the ${confidence}% confidence intervals contained the true population mean ${tex`\\mu= `} ${mu}.\n`\n\n\n\n\n\n\n\nviewof lockvertical = Inputs.toggle({label: \"Lock vertical axis\", value: false})\n\n\n\n\n\n\n\nplt = Plot.plot({\n  width: 1600, // or a dynamic value based on `width` variable\n  height: 600,\n  style: {fontSize: \"20px\"},\n  y: {\n    label: \"mean\",\n    domain: lockvertical ? [mu- 3, mu + 3] : [mu - 5*sigma/Math.sqrt(nobs),mu + 5*sigma/Math.sqrt(nobs)]\n  },\n  x: {\n    label: \"sample number\",\n    domain:  [0, nrep]\n  },\n  marks: [\n    Plot.ruleX([0]),    \n    Plot.ruleX(data, {\n      filter: d =&gt; (d.rep &lt;= nrep && mu &gt;= d.lower && mu &lt;= d.upper),\n      x: \"rep\",\n      y1: \"lower\",\n      y2: \"upper\",  \n      stroke: \"steelblue\",\n      strokeWidth: 2.5\n    }),\n    Plot.dot(data, {filter: d =&gt; (d.rep &lt;= nrep && mu &gt;= d.lower && mu &lt;= d.upper), x: \"rep\", y: \"xbar\", fill: \"steelblue\", r: 3}),\n\n    Plot.ruleX(data, {\n      filter: d =&gt; d.rep &lt;= nrep && (mu &lt; d.lower || mu &gt; d.upper),\n      x: \"rep\",\n      y1: \"lower\",\n      y2: \"upper\",  \n      stroke: \"orange\",\n      strokeWidth: 3.5\n    }),\n    Plot.dot(data, {filter: d =&gt; d.rep &lt;= nrep && (mu &lt; d.lower || mu &gt; d.upper), x: \"rep\", y: \"xbar\", fill: \"orange\", r: 3}),\n    Plot.ruleY([mu], {stroke: \"#D22B2B\", strokeWidth: 2.5})\n\n  ]\n})\n\n\n\n\n\n\n\n\nFigure¬†1: https://observablehq.com/@mattiasvillani/confidence-interval-for-a-mean\n\n\n\n\n\nimport {Scrubber} from \"@mbostock/scrubber\"\n\n\n\n\n\n\n\nfunction simulate_means(mu, sigma, nobs, nrep){\n  \n  const tvalue = jstat.studentt.inv(1-(1-(confidence/100))/2, nobs-1);\n  var data = [];\n  for (let j = 1; j &lt;= nrep; j++){\n        let sample = d3.range(nobs).map(d =&gt; d3.randomNormal(mu,sigma)())\n        let xbar = d3.mean(sample)\n        let s = jstat.stdev(sample, true) \n        let lower = xbar - tvalue*s/Math.sqrt(nobs)\n        let upper = xbar + tvalue*s/Math.sqrt(nobs)\n        data.push({rep: j, xbar: xbar, lower: lower, upper: upper})\n  }\n  return data\n}\n\n\n\n\n\n\n\njstat = require('jstat')\n\n\n\n\n\n\n\nmu = settings[0]\nsigma = settings[1]\nconfidence = settings[2]\nnobs = settings[3]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata = {\n  nobs;\n  mu;\n  sigma;\n  const d = new Date();\n  let time = d.getTime();\n  const data = simulate_means(mu, sigma, nobs, 1000)\n  return data\n}"
  },
  {
    "objectID": "ojs/prob-normal-ojs.html",
    "href": "ojs/prob-normal-ojs.html",
    "title": "MATH 4720 (MSSC 5720) - Spring 2026",
    "section": "",
    "text": "jstat = require('jstat')\n\n\n\n\n\n\n\nviewof params = Inputs.form([\n      Inputs.range([-3, 3], {value: 0, step: 0.1, label: tex`\\mu:`}),\n      Inputs.range([0.1, 3], {value: 1, step: 0.1, label: tex`\\sigma:`}),\n      Inputs.range([-10, 10], {value: -1.96, step: 0.01, label: \"quantile:\"})\n    ])\n\n\n\n\n\n\n\ndist_quantile = tex`\\text{If } X \\sim N(${params[0]},${params[1]}) \\text{ then } \\\\[1em]`\n\n\n\n\n\n\n\nmoments = tex`\n\\begin{aligned}\n&E( X) = \\mu = ${params[0].toPrecision(3)} \\\\[0.5em]\n&Var( X) =\\sigma^2 = ${(params[1]**2).toPrecision(3)} \\\\[0.5em]\n&P( X\\leq ${params[2]}) =${normcdf.toPrecision(4)}\n\\end{aligned}\n`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport {interval} from '@mootari/range-slider@1781'\n\n\n\n\n\n\n\n\n\n\nplt_pdf = Plot.plot({\n    width: 700, // or a dynamic value based on `width` variable\n    height: 400,\n    color: {\n      legend: true\n    },\n    x: {\n      label: \"x\",\n      axis: true\n    },\n    y: {\n      axis: true,\n      label: \"Normal density value f(x)\",\n      domain: [0,jstat.normal.pdf(params[0], params[0], params[1])]\n    },\n    style: {\n    fontSize: 15,         // overall font size (tick labels + titles)\n    fontFamily: \"sans-serif\"\n    },\n    marks: [\n      Plot.ruleY([0]),\n      Plot.line(normpdf, {x: \"x\", y: \"pdf\", stroke : \"steelblue\", strokeWidth: 2}),\n      Plot.areaY(normpdf, {filter: d =&gt; d.x &lt;= params[2], x: \"x\", y: \"pdf\", fill: \"steelblue\", opacity: 0.2})\n    ]\n  })\n\n\n\n\n\n\n\nnormpdf = {\n  const x = d3.range(-10, 10, 0.01);\n  const normpdf = x.map(x =&gt; ({x: x, pdf: jstat.normal.pdf(x, params[0], params[1])}));\n  return normpdf\n}\n\n\n\n\n\n\n\nnormcdf = jstat.normal.cdf(params[2], params[0], params[1]);\n\n\n\n\n\n\n\nmaxpdf = jstat.normal.pdf(params[0], params[0], params[1])\n\n\n\n\n\n\n\ntextdata = [{x: params[0], y: 0.12*maxpdf, text: \"68%\"},{x: params[0] - 1.5*params[1], y: 0.12*maxpdf, text: \"95%\"},{x: params[0] - 2.5*params[1], y: 0.12*maxpdf, text: \"99.7%\"}]"
  },
  {
    "objectID": "ojs/prob-binom-normal-ojs.html",
    "href": "ojs/prob-binom-normal-ojs.html",
    "title": "MATH 4720 (MSSC 5720) - Spring 2026",
    "section": "",
    "text": "viewof params_binom_normal = Inputs.form([\n      Inputs.range([1, 200], {value: 10, step: 1, label: tex`n`}),\n      Inputs.range([0.001, 1], {value: 0.5, step: 0.001, label: tex`\\pi`}),\n    ])\n\n\n\n\n\n\n\nviewof kvantil_binom_normal = Inputs.range([0, params_binom_normal[0]], {value: 3, step: 1, label: \"quantile\"})\n\n\n\n\n\n\n\nmoments_binom_normal = tex`\n\\text{If } X \\sim \\operatorname{Binom}(${params_binom_normal[0]},${params_binom_normal[1]}) \\text{ then }\\\\[0.5em]\n\n\nE( X) =n  \\pi = ${(params_binom_normal[0]*params_binom_normal[1]).toPrecision(3)} \\\\[0.5em]\nVar( X) =n  \\pi (1-\\pi) =${(params_binom_normal[0]*params_binom_normal[1]*(1-params_binom_normal[1])).toPrecision(3)} \\\\[0.5em]\n\n`\n\n\n\n\n\n\n\ncdfs_normalapprox = \n{\n  var cdf;\n  if (normalapprox){\n      cdf = tex`\n        \\begin{array}{ll}\n        \\text{Exact:} & P( X\\leq ${kvantil_binom_normal}) =${binomcdf.toPrecision(4)} \\\\[0.25em]\n        \\text{Normal approx:} & P( X\\leq ${kvantil_binom_normal}) =${normalapproxcdf.toPrecision(4)} \\\\[0.25em]\n        \\text{Normal approx with correction:} & P( X\\leq ${kvantil_binom_normal}) =${normalapproxcdf_corrected.toPrecision(4)}\n        \\end{array}\n        `\n  }\n  else{\n      cdf = tex`\n        \\begin{array}{ll}\n        \\text{Exact:} & P( X\\leq ${kvantil_binom_normal}) =${binomcdf.toPrecision(4)}\n        \\end{array}\n        `\n  }\n  return cdf;\n}\n\n\n\n\n\n\n\nviewof plotselector = Inputs.checkbox([\"normal\", \"normal with correction\"], {label: \"plot approximation:\", value: [\"normal approximation\", \"normal approximation corrected\"]})\n\n\n\n\n\n\n\nplt_pdf_binom_normal = Plot.plot({\n    color: {\n          legend: true,\n          domain: legenddomain.filter( (d,i) =&gt; boolselect[i]),\n          range: legendcolor.filter( (d,i) =&gt; boolselect[i])\n        },\n    x: {\n      label: \"x\",\n      axis: true,\n      domain: xrange\n    },\n    y: {\n      label: \"P(X = x)\",\n      domain: [0,1.1*jstat.binomial.pdf(params_binom_normal[0]*params_binom_normal[1], params_binom_normal[0],params_binom_normal[1])]\n    },\n  tooltip: {\n    fill: \"b39640\",\n    stroke: \"b39640\",\n    opacity: 1,\n  },\n    marks: [\n      Plot.ruleY([0]),\n      Plot.barY(approxpdf,{x: \"x\", y: \"binompdf\", fill : \"#C1CFE4\", strokeWidth: 0, opacity: 0.8, \n        title: (d) =&gt; `P(X=${d.x}) = ${(d.binompdf).toPrecision(4)}`}),\n      Plot.barY(approxpdf, {filter: d =&gt; d.x &lt;= kvantil_binom_normal, x: \"x\", y: \"binompdf\", fill: \"#98afd2\", opacity: 1,\n        title: (d) =&gt; `P(X=${d.x}) = ${(d.binompdf).toPrecision(4)}`}),\n      \n      Plot.line(approxpdf, {filter: d =&gt; normalapprox, x: \"x\", y: \"normalpdf\", stroke : \"#AB8D61\", strokeWidth: 2}),\n      Plot.dot(approxpdf, {filter: d =&gt; normalapprox, x: \"x\", y: \"normalpdf\", stroke : \"#AB8D61\", strokeWidth: 2}),\n      \n      Plot.line(approxpdf, {filter: d =&gt; normalapproxcorr, x: \"x\", y: \"normalpdfcorr\", stroke : \"#007878\", strokeWidth: 2}),\n      Plot.dot(approxpdf, {filter: d =&gt; normalapproxcorr, x: \"x\", y: \"normalpdfcorr\", stroke : \"#007878\", strokeWidth: 2}),\n      \n      // Plot.ruleX(approxpdf, {filter: d =&gt; poisapprox && d.x &gt;= 0, x: \"x\", y: \"poissonpdf\", stroke: \"#780000\", strokeWidth: 2}),  \n      // Plot.dot(approxpdf, {filter: d =&gt; poisapprox && d.x &gt;= 0, x: \"x\", y: \"poissonpdf\", fill: \"#780000\", r: 5}),\n    ]\n  })\n\n\n\n\n\n\n\n\nFigure¬†1: Source: https://observablehq.com/@mattiasvillani/approximating-the-binomial-distribution\n\n\n\n\n\napproxpdf = {\n  const x = xrange;\n  const approxpdf = x.map(x =&gt; ({\n    x: x, \n    binompdf: jstat.binomial.pdf(x, params_binom_normal[0], params_binom_normal[1]),\n    normalpdf: jstat.normal.pdf(x, params_binom_normal[0]*params_binom_normal[1], Math.sqrt(params_binom_normal[0]*params_binom_normal[1]*(1-params_binom_normal[1]))), \n    normalpdfcorr: jstat.normal.cdf(x + 0.5, params_binom_normal[0]*params_binom_normal[1], Math.sqrt(params_binom_normal[0]*params_binom_normal[1]*(1-params_binom_normal[1])))-jstat.normal.cdf(x - 0.5, params_binom_normal[0]*params_binom_normal[1], Math.sqrt(params_binom_normal[0]*params_binom_normal[1]*(1-params_binom_normal[1]))),\n    // poissonpdf: jstat.poisson.pdf(x, params_binom_normal[0]*params_binom_normal[1])\n  }));\n  return approxpdf\n}\n\n\n\n\n\n\n\nxrange = d3.range(0, high + 1, 1);\n\n\n\n\n\n\n\nhigh = Math.ceil(jstat.normal.inv(0.9999, params_binom_normal[0]*params_binom_normal[1], Math.sqrt(params_binom_normal[0]*params_binom_normal[1]*(1-params_binom_normal[1]))))\n\n\n\n\n\n\n\n\n\n\nlegendcolor = [\"#C1CFE4\", \"#AB8D61\", \"#007878\", \"#780000\"]\n\n\n\n\n\n\n\nboolselect = [true, normalapprox, normalapproxcorr]\n\n\n\n\n\n\n\nnormalapproxcorr = plotselector.includes(\"normal with correction\")\n\n\n\n\n\n\n\nnormalapprox = plotselector.includes(\"normal\")\n\n\n\n\n\n\n\nlegenddomain = [\"binomial\", \"normal\", \"normal with correction\"]\n\n\n\n\n\n\n\nnormalapproxcdf = jstat.normal.cdf(kvantil_binom_normal, params_binom_normal[0]*params_binom_normal[1], Math.sqrt(params_binom_normal[0]*params_binom_normal[1]*(1-params_binom_normal[1])))\n\n\n\n\n\n\n\nbinomcdf_binom_normal = jstat.binomial.cdf(kvantil_binom_normal, params_binom_normal[0], params_binom_normal[1]);\n\n\n\n\n\n\n\nnormalapproxcdf_corrected = jstat.normal.cdf(kvantil_binom_normal+0.5, params_binom_normal[0]*params_binom_normal[1], Math.sqrt(params_binom_normal[0]*params_binom_normal[1]*(1-params_binom_normal[1])))\n\n\n\n\n\n\n\nbinomcdf = jstat.binomial.cdf(kvantil_binom_normal, params_binom_normal[0], params_binom_normal[1]);"
  },
  {
    "objectID": "ojs/prob-binom-pois-ojs.html",
    "href": "ojs/prob-binom-pois-ojs.html",
    "title": "MATH 4720 (MSSC 5720) - Spring 2026",
    "section": "",
    "text": "viewof params_binom_pois = Inputs.form([\n      Inputs.range([1, 200], {value: 10, step: 1, label: tex`n`}),\n      Inputs.range([0.001, 1], {value: 0.5, step: 0.001, label: tex`\\pi`}),\n    ])\n\n\n\n\n\n\n\nviewof kvantil_binom_pois = Inputs.range([0, params_binom_pois[0]], {value: 3, step: 1, label: \"quantile\"})\n\n\n\n\n\n\n\nmoments_binom_pois = tex`\n\\text{If } X \\sim \\operatorname{Binom}(${params_binom_pois[0]},${params_binom_pois[1]}) \\text{ then }\\\\[0.5em]\n\n\nE( X) =n  \\pi = ${(params_binom_pois[0]*params_binom_pois[1]).toPrecision(3)} \\\\[0.5em]\nVar( X) =n  \\pi (1-\\pi) =${(params_binom_pois[0]*params_binom_pois[1]*(1-params_binom_pois[1])).toPrecision(3)} \\\\[0.5em]\n\n`\n\n\n\n\n\n\n\ncdfs_poisapprox = \n{\n  var cdf;\n  if (poisapprox){\n      cdf = tex`\n        \\begin{array}{ll}\n        \\text{Exact:} & P( X\\leq ${kvantil_binom_pois}) =${binomcdf_binom_pois.toPrecision(3)} \\\\[0.25em]\n        \\text{Poisson approx} & P( X\\leq ${kvantil_binom_pois}) =${poissonapproxcdf.toPrecision(3)} \n        \\end{array}\n        `\n  }\n  else{\n      cdf = tex`\n        \\begin{array}{ll}\n        \\text{Exact:} & P( X\\leq ${kvantil_binom_pois}) =${binomcdf_binom_pois.toPrecision(3)}\n        \\end{array}\n        ` \n  }\n  return cdf;\n}\n\n\n\n\n\n\n\nviewof plotselector = Inputs.checkbox([\"poisson\"], {label: \"plot approximation:\", value: [\"poisson approximation\"]})\n\n\n\n\n\n\n\nplt_pdf = Plot.plot({\n    color: {\n          legend: true,\n          domain: legenddomain.filter( (d,i) =&gt; boolselect[i]),\n          range: legendcolor.filter( (d,i) =&gt; boolselect[i])\n        },\n    x: {\n      label: \"x\",\n      axis: true,\n      domain: xrange\n    },\n    y: {\n      label: \"P(X = x)\",\n      domain: [0,1.1*jstat.binomial.pdf(params_binom_pois[0]*params_binom_pois[1], params_binom_pois[0],params_binom_pois[1])]\n    },\n  tooltip: {\n    fill: \"b39640\",\n    stroke: \"b39640\",\n    opacity: 1,\n  },\n    marks: [\n      Plot.ruleY([0]),\n      Plot.barY(approxpdf,{x: \"x\", y: \"binompdf\", fill : \"#C1CFE4\", strokeWidth: 0, opacity: 0.8, \n        title: (d) =&gt; `P(X=${d.x}) = ${(d.binompdf).toPrecision(4)}`}),\n      Plot.barY(approxpdf, {filter: d =&gt; d.x &lt;= kvantil_binom_pois, x: \"x\", y: \"binompdf\", fill: \"#98afd2\", opacity: 1,\n        title: (d) =&gt; `P(X=${d.x}) = ${(d.binompdf).toPrecision(4)}`}),\n      \n      // Plot.line(approxpdf, {filter: d =&gt; normalapprox, x: \"x\", y: \"normalpdf\", stroke : \"#AB8D61\", strokeWidth: 3}),\n      // Plot.dot(approxpdf, {filter: d =&gt; normalapprox, x: \"x\", y: \"normalpdf\", stroke : \"#AB8D61\", strokeWidth: 3}),\n      \n      // Plot.line(approxpdf, {filter: d =&gt; normalapproxcorr, x: \"x\", y: \"normalpdfcorr\", stroke : \"#007878\", strokeWidth: 3}),\n      // Plot.dot(approxpdf, {filter: d =&gt; normalapproxcorr, x: \"x\", y: \"normalpdfcorr\", stroke : \"#007878\", strokeWidth: 3}),\n      \n      Plot.ruleX(approxpdf, {filter: d =&gt; poisapprox && d.x &gt;= 0, x: \"x\", y: \"poissonpdf\", stroke: \"#780000\", strokeWidth: 2}),  \n      Plot.dot(approxpdf, {filter: d =&gt; poisapprox && d.x &gt;= 0, x: \"x\", y: \"poissonpdf\", fill: \"#780000\", r: 5}),\n    ]\n  })\n\n\n\n\n\n\n\napproxpdf = {\n  const x = xrange;\n  const approxpdf = x.map(x =&gt; ({\n    x: x, \n    binompdf: jstat.binomial.pdf(x, params_binom_pois[0], params_binom_pois[1]),\n    // normalpdf: jstat.normal.pdf(x, params[0]*params[1], Math.sqrt(params[0]*params[1]*(1-params[1]))), \n    // normalpdfcorr: jstat.normal.pdf(x + 0.5, params[0]*params[1], Math.sqrt(params[0]*params[1]*(1-params[1]))),\n    poissonpdf: jstat.poisson.pdf(x, params_binom_pois[0]*params_binom_pois[1])\n  }));\n  return approxpdf\n}\n\n\n\n\n\n\n\nxrange = d3.range(0, high + 1, 1);\n\n\n\n\n\n\n\nhigh = Math.ceil(jstat.normal.inv(0.9999, params_binom_pois[0]*params_binom_pois[1], Math.sqrt(params_binom_pois[0]*params_binom_pois[1]*(1-params_binom_pois[1]))))\n\n\n\n\n\n\n\n\n\n\nlegendcolor = [\"#C1CFE4\", \"#AB8D61\", \"#007878\", \"#780000\"]\n\n\n\n\n\n\n\nboolselect = [true, poisapprox]\n\n\n\n\n\n\n\npoisapprox = plotselector.includes(\"poisson\")\n\n\n\n\n\n\n\nlegenddomain = [\"binomial\", \"poisson\"]\n\n\n\n\n\n\n\npoissonapproxcdf = jstat.poisson.cdf(kvantil_binom_pois, params_binom_pois[0]*params_binom_pois[1])\n\n\n\n\n\n\n\nbinomcdf_binom_pois = jstat.binomial.cdf(kvantil_binom_pois, params_binom_pois[0], params_binom_pois[1]);"
  },
  {
    "objectID": "ojs/infer-var-f-ojs.html",
    "href": "ojs/infer-var-f-ojs.html",
    "title": "F distribution",
    "section": "",
    "text": "viewof params_f = Inputs.form([\n      Inputs.range([1, 100], {value: 4, step: 1, label: tex`\\nu_1`}),\n      Inputs.range([1, 100], {value: 4, step: 1, label: tex`\\nu_2`}),\n      Inputs.range([0.01, 10], {value: 2, step: 0.01, label: \"quantile:\"})\n    ])\n\n\n\n\n\n\n\ndist_quantile_f = tex`\\text{If } X \\sim F(${nu1},${nu2}) \\text{ then } `\n\n\n\n\n\n\n\nmoments_f = {\n  if (nu2&gt;4){\n    return tex`\n      \\begin{aligned}\n        &E( X) =\\frac{\\nu_2}{\\nu_2-2} = ${(nu2/(nu2-2)).toPrecision(3)} \\\\[1em]\n        &Var( X) = 2\\Big(\\frac{\\nu_2}{\\nu_2-2}\\Big)^2\\frac{\\nu_1+\\nu_2-2}{\\nu_1(\\nu_2-4)} = \n        ${(2*(nu2/(nu2-2))**2 * ((nu1+nu2-2)/(nu1*(nu2-4)))).toPrecision(3)} \\\\[1em]\n        &P( X\\leq ${quantile}) =${fcdf.toPrecision(4)}\n      \\end{aligned}\n    `\n  }\n  else{\n    if (nu2&gt;2 && nu2&lt;=4){\n      return tex`\n      \\begin{aligned}\n        &E( X) =\\frac{\\nu_2}{\\nu_2-2} = ${(nu2/(nu2-2)).toPrecision(3)} ( \\nu_2&gt;2 \\text{ is needed} )\\\\[1em]\n        &Var( X) = \\text{ does not exist} ( \\nu_2&gt;4 \\text{ is needed} ) \\\\[1em]\n        &P( X\\leq ${quantile}) =${fcdf.toPrecision(4)}\n      \\end{aligned}\n    `\n    }\n    else{\n    return tex`\n      \\begin{aligned}\n        &E( X) =\\text{ does not exist} ( \\nu_2&gt;2 \\text{ is needed} )\\\\[1em]\n        &Var( X) = \\text{ does not exist} ( \\nu_2&gt;4 \\text{ is needed} ) \\\\[1em]\n        &P( X\\leq ${quantile}) =${fcdf.toPrecision(4)}\n      \\end{aligned}\n    `\n    }\n  }\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\nplt_pdf_f = Plot.plot({\n    width: 800, // or a dynamic value based on `width` variable\n    height: 600,\n    style: {fontSize: \"15px\"},\n    color: {\n      legend: true\n    },\n    x: {\n      label: \"x\",\n      axis: true\n    },\n    y: {\n      label: \"f(x)\",\n      //axis: false,\n      //domain: [0,jstat.chisquare.pdf(mode, nu1, nu2)]\n    },\n    marks: [\n      Plot.ruleY([0]),\n      Plot.ruleX([0]),\n      Plot.line(fpdf, {x: \"x\", y: \"pdf\", stroke : \"purple\", strokeWidth: 2}),\n      Plot.areaY(fpdf, {filter: d =&gt; d.x &lt;= quantile, x: \"x\", y: \"pdf\", fill: \"purple\", opacity: 0.2})\n    ]\n  })\n\n\n\n\n\n\n\n\nFigure¬†1: https://observablehq.com/@mattiasvillani/f-distribution\n\n\n\n\n\nnu1 = params_f[0]\nnu2 = params_f[1]\nquantile = params_f[2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfpdf = {\n  const x = d3.range(0.01, jstat.centralF.inv( 0.99, nu1, nu2 ), 0.01);\n  const pdf = x.map(x =&gt; ({x: x, pdf: jstat.centralF.pdf(x, nu1, nu2 )}));\n  return pdf\n}\n\n\n\n\n\n\n\nfcdf = jstat.centralF.cdf(quantile, nu1, nu2);\n\n\n\n\n\n\n\njstat = require('jstat')"
  },
  {
    "objectID": "ojs/prob-binomial-ojs.html",
    "href": "ojs/prob-binomial-ojs.html",
    "title": "Binomial Distribution",
    "section": "",
    "text": "viewof params_binom = Inputs.form([\n      Inputs.range([1, 200], {value: 10, step: 1, label: tex`n`}),\n      Inputs.range([0, 1], {value: 0.5, step: 0.01, label: tex`\\pi`}),\n    ])\n\n\n\n\n\n\n\nviewof kvantil = Inputs.range([1, params_binom[0]], {value: 3, step: 1, label: \"quantile\"})\n\n\n\n\n\n\n\nviewof approx = Inputs.toggle({label: \"show normal approximation\", value: false})\n\n\n\n\n\n\n\njstat = require('jstat')\n\n\n\n\n\n\n\nbinomcdf = jstat.binomial.cdf(kvantil, params_binom[0], params_binom[1]);\n\n\n\n\n\n\n\nnormalapproxpdf = {\n  const x = d3.range(0, params_binom[0], 1);\n  const normalapproxpdf = x.map(x =&gt; ({x: x, pdf: jstat.normal.pdf(x, params_binom[0]*params_binom[1], Math.sqrt(params_binom[0]*params_binom[1]*(1-params_binom[1])))}));\n  return normalapproxpdf\n}\n\n\n\n\n\n\n\nnormalapproxcdf = jstat.normal.cdf(kvantil, params_binom[0]*params_binom[1], Math.sqrt(params_binom[0]*params_binom[1]*(1-params_binom[1])))\n\n\n\n\n\n\n\nnormalapproxcdf_corrected = jstat.normal.cdf(kvantil+0.5, params_binom[0]*params_binom[1], Math.sqrt(params_binom[0]*params_binom[1]*(1-params_binom[1])))\n\n\n\n\n\n\n\nbinompdf = {\n  const x = d3.range(0, params_binom[0]+1, 1);\n  const binompdf = x.map(x =&gt; ({x: x, pdf: jstat.binomial.pdf(x, params_binom[0], params_binom[1]), type: \"exact\"}));\n  const normalapproxpdf = x.map(x =&gt; ({x: x, pdf: jstat.normal.pdf(x, params_binom[0]*params_binom[1], Math.sqrt(params_binom[0]*params_binom[1]*(1-params_binom[1]))),type: \"approx\"}));\n  const data = binompdf.concat(normalapproxpdf)\n  return data\n}\n\n\n\n\n\n\n\nmoments_binom = tex`\n\\text{If } X \\sim \\operatorname{Binom}(${params_binom[0]},${params_binom[1]}) \\text{ then }\\\\[0.5em]\n\nE( X) =n  \\pi = ${(params_binom[0]*params_binom[1]).toPrecision(3)} \\\\[0.5em]\nVar( X) =n  \\pi (1-\\pi) =${(params_binom[0]*params_binom[1]*(1-params_binom[1])).toPrecision(3)} \\\\[0.5em]\n\n`\n\n\n\n\n\n\n\ncdfs = \n{\n  var cdf;\n  if (approx){\n  cdf = tex`\n  \\begin{array}{ll}\n  \\text{Exact binomial:} & P( X\\leq ${kvantil}) =${binomcdf.toPrecision(4)} \\\\[0.25em]\n  \\text{Normal approx:} & P( X\\leq ${kvantil}) =${normalapproxcdf.toPrecision(4)} \\\\[0.25em]\n  \\text{Normal approx - corr:} & P( X\\leq ${kvantil})  =${normalapproxcdf_corrected.toPrecision(4)} \\\\[1em]\n  \\end{array}\n  `}\n  else{\n   cdf = tex`\n  \\begin{array}{ll}\n  \\text{Exact:} & P( X\\leq ${kvantil}) =${binomcdf.toPrecision(4)} \\\\[1em]\n  \\end{array}\n  ` \n  }\n  return cdf;\n}\n\n\n\n\n\n\n\n\n\n\nplt_binom = Plot.plot({\n    width: 700, // or a dynamic value based on `width` variable\n    height: 400,\n    color: { \n      legend: approx\n    },\n    x: {\n      label: \"x\",\n      axis: true,\n      domain: d3.range(0, params_binom[0]+1, 1),\n      labelOffset: 30\n    },\n    y: {\n      label: \"P(X = x)\",\n      domain: [0,1.1*jstat.binomial.pdf(params_binom[0]*params_binom[1], params_binom[0],params_binom[1])]\n    },\n   style: {\n    fontSize: 16,         // overall font size (tick labels + titles)\n    fontFamily: \"sans-serif\"\n    },\n  tooltip: {\n    fill: \"#C04000\",\n    stroke: \"#C04000\",\n    opacity: 1,\n  },\n    marks: [\n      Plot.ruleY([0]),\n      Plot.barY(binompdf,{filter: d =&gt; d.type == \"exact\", x: \"x\", y: \"pdf\", fill : \"orange\", strokeWidth: 0, opacity: 0.8, \n        title: (d) =&gt; `P(X=${d.x}) = ${(d.pdf).toPrecision(4)}`}),\n      Plot.barY(binompdf, {filter: d =&gt; d.type == \"exact\" && d.x &lt;= kvantil, x: \"x\", y: \"pdf\", fill: \"darkorange\", opacity: 1,\n        title: (d) =&gt; `P(X=${d.x}) = ${(d.pdf).toPrecision(4)}`}),\n      Plot.line(normalapproxpdf, {filter: d =&gt; approx, x: \"x\", y: \"pdf\", stroke : \"steelblue\", strokeWidth: 2})\n    ]\n  })\n\n\n\n\n\n\n\n\nFigure¬†1: https://observablehq.com/@mattiasvillani/binomial-distribution"
  },
  {
    "objectID": "ojs/prob-poisson-ojs.html",
    "href": "ojs/prob-poisson-ojs.html",
    "title": "Poisson Distribution",
    "section": "",
    "text": "jstat = require('jstat')\n\n\n\n\n\n\n\nviewof params_pois = Inputs.form([\n      Inputs.range([0.1, 20], {value: 2, step: 0.1, label: tex`\\lambda:`}),\n      Inputs.range([1, 40], {value: 1, step: 1, label: \"quantile:\"})\n    ])\n\n\n\n\n\n\n\ndist_quantile_pois = tex`\\text{If } X \\sim \\operatorname{Poisson}(${params_pois[0]}) \\text{ then } `\n\n\n\n\n\n\n\nmoments_pois = tex`\n\\begin{aligned}\n&\\mathrm{E}( X) =  \\lambda = ${params_pois[0].toPrecision(3)} \\\\[0.5em]\n&\\mathrm{Var}( X) =  \\lambda  = ${params_pois[0].toPrecision(3)} \\\\[0.5em]\n&\\mathrm{P}( X\\leq ${params_pois[1]}) =${poiscdf.toPrecision(4)}\n\\end{aligned}\n`\n\n\n\n\n\n\n\n\n\n\nplt_pois = Plot.plot({\n    width: 700, // or a dynamic value based on `width` variable\n    height: 400,\n    color: {\n      legend: false\n    },\n    x: {\n      label: \"x\",\n      axis: true,\n      labelOffset: 30\n    },\n    y: {\n      label: \"P(X = x)\",\n      axis: true\n    },\n   style: {\n    fontSize: 16,         // overall font size (tick labels + titles)\n    fontFamily: \"sans-serif\"\n    },\n  tooltip: {\n    fill: \"#097969\",\n    stroke: \"#097969\",\n    opacity: 1,\n  },\n    marks: [\n      Plot.ruleY([0]),\n      Plot.barY(poispdf,{x: \"x\", y: \"pdf\", fill: \"#C1E1C1\", strokeWidth: 0, opacity: 1,\n                title: (d) =&gt; `P(X=${d.x}) = ${(d.pdf).toPrecision(4)}`}),\n      Plot.barY(poispdf, {filter: d =&gt; d.x &lt;= params_pois[1], x: \"x\", y: \"pdf\", fill: \"#50C878\", opacity: 1,\n                title: (d) =&gt; `P(X=${d.x}) = ${(d.pdf).toPrecision(4)}`})\n    ]\n  })\n\n\n\n\n\n\n\n\nFigure¬†1: https://observablehq.com/@mattiasvillani/poisson-distribution\n\n\n\n\n\npoispdf = {\n  const x = d3.range(0, params_pois[0] + 4*Math.sqrt(params_pois[0]), 1);\n  const data = x.map(x =&gt; ({x: x, pdf: jstat.poisson.pdf(x, params_pois[0])}));\n  return data\n}\n\n\n\n\n\n\n\npoiscdf = jstat.poisson.cdf(params_pois[1], params_pois[0]);"
  },
  {
    "objectID": "activity/act_reg.html",
    "href": "activity/act_reg.html",
    "title": "Activity: Simple Linear Regression",
    "section": "",
    "text": "You will:\n\nIdentify predictor (height) and response (shoe size).\nExplain what the slope means in context.\nRecognize variability and limitations of small-sample regressions.\nExperience how combining data strengthens inference."
  },
  {
    "objectID": "activity/act_reg.html#objective",
    "href": "activity/act_reg.html#objective",
    "title": "Activity: Simple Linear Regression",
    "section": "",
    "text": "You will:\n\nIdentify predictor (height) and response (shoe size).\nExplain what the slope means in context.\nRecognize variability and limitations of small-sample regressions.\nExperience how combining data strengthens inference."
  },
  {
    "objectID": "activity/act_reg.html#instructions",
    "href": "activity/act_reg.html#instructions",
    "title": "Activity: Simple Linear Regression",
    "section": "Instructions",
    "text": "Instructions\n\nForm a group of 4 or 5. (This is your sample size!)\n\nCollect your data. Each person will share their height and shoe size!\n\nHeight (\\(X\\)): inches (Use feet-to-inch conversion to help you get your height in inches.)\nShoe Size (\\(Y\\)): US\n\n\n\nMake a R data frame. (Replace my numbers with your data!)\n\n\ndf &lt;- data.frame(height = c(69, 74, 77, 60, 62), \n                 size = c(11, 10.5, 12, 9.5, 9))\ndf\n\n  height size\n1     69 11.0\n2     74 10.5\n3     77 12.0\n4     60  9.5\n5     62  9.0\n\n\n\nAnalyze within your group.\n\n\n# (a) Plot your data. Height on the x-axis, Shoe Size on the y-axis.\nplot(x = ______, y = _______)\n\n\n# (b) Fit the simple linear regression model\nmodel &lt;- lm(____ ~ ____)\n\n# (c) Draw the fitted regression line.\nabline(______)\n\n\n# (d) Obtain the estimated slope. What does it mean in context? Is the effect statistically significant?"
  },
  {
    "objectID": "activity/act_reg.html#discussion",
    "href": "activity/act_reg.html#discussion",
    "title": "Activity: Simple Linear Regression",
    "section": "Discussion",
    "text": "Discussion\n\nHow does your group‚Äôs pattern compare to the overall line?\nWhat does the slope mean in context?\nWhy might individual data vary around the line?\nWould it make sense to predict shoe size for someone far outside our data range (e.g., 7‚Äô3‚Äù tall)? Why or why not?"
  },
  {
    "objectID": "activity/act_sampling_clt.html",
    "href": "activity/act_sampling_clt.html",
    "title": "Activity: Sampling Distribution and Central Limit Theorem",
    "section": "",
    "text": "You will experience\n\nhow sample means vary,\nhow a sampling distribution of sample mean is created\nhow they approximate a normal distribution as sample size increases."
  },
  {
    "objectID": "activity/act_sampling_clt.html#objective",
    "href": "activity/act_sampling_clt.html#objective",
    "title": "Activity: Sampling Distribution and Central Limit Theorem",
    "section": "",
    "text": "You will experience\n\nhow sample means vary,\nhow a sampling distribution of sample mean is created\nhow they approximate a normal distribution as sample size increases."
  },
  {
    "objectID": "activity/act_sampling_clt.html#instructions",
    "href": "activity/act_sampling_clt.html#instructions",
    "title": "Activity: Sampling Distribution and Central Limit Theorem",
    "section": "Instructions",
    "text": "Instructions\n\nGo to Dr.¬†Yu‚Äôs app https://cheyu.shinyapps.io/rolling-die/\n\n\n\n\n\n\n\n\n\n\n\nRoll your die 5 times, records your outcomes in Enter your die rolls separated by commas or spaces:, then hit Submit Rolls.\nCheck the histogram of sample means. Does it look like a normal distribution?\nChange 5 to 30 in Select number of rolls:\nRoll your die 30 times, records your outcomes in Enter your die rolls separated by commas or spaces:, then hit Submit Rolls.\nHow does the histogram look compared to the 5-roll sample means?"
  },
  {
    "objectID": "activity/act_sampling_clt.html#discussion",
    "href": "activity/act_sampling_clt.html#discussion",
    "title": "Activity: Sampling Distribution and Central Limit Theorem",
    "section": "Discussion",
    "text": "Discussion\n\nWhat happened to the variability of the sample means as sample size increased?\nHow did the shape of the distribution change?\nHow does this illustrate the Central Limit Theorem?"
  },
  {
    "objectID": "hw/hw01.html",
    "href": "hw/hw01.html",
    "title": "Homework 1",
    "section": "",
    "text": "Important\n\n\n\nDue Friday, Sep 5, 11:59 PM\n\n\n\nHomework 1 covers Week 1 course materials.\nPlease submit your work in one PDF file to D2L &gt; Assessments &gt; Dropbox. Multiple files or a file that is not in pdf format is not accepted.\nIn your homework, please number questions in order.\n\n\nQuestions started with (MSSC) are required for MSSC 5720 students, and optional for MATH 4720 students.\n\nHomework Questions\n\n\nData Type. Identify each of the following as numerical or categorical data.\n\nThe names of the pharmaceutical companies that manufacture aspirin tablets\nThe colors of pills\nThe weights of aspirin tablets\n\n\n\n\n  # (a) Categorical\n  # (b) Categorical\n  # (c) Numerical\n\n\n\n\nLevel of Measurements. Identify the level of measurement used in each of the following.\n\nThe weights of people in a sample of Marquette engineering students.\nA physician‚Äôs descriptions of ‚Äúabstains from alcohol, light drinker, moderate drinker, heavy drinker.‚Äù\nTree classifications of ‚Äúoak, maple, elm.‚Äù\nBob measures time in days, with 0 corresponding to his birth date. The day before his birth is -1, the day after his birth is +1, and so on. Bib has converted the dates of major historical events to his numbering system. What is the level of measurement of these numbers?\n\n\n\n\n  # (a) Ratio\n  # (b) Ordinal\n  # (c) Nominal\n  # (d) Interval\n\n\n\n\nDiscrete vs Continuous. Determine whether the data are discrete or continuous.\n\nThe length of stay (in days) for each baby in a sample of babies born in Wisconsin.\nSeveral subjects are randomly selected and their heights are recorded.\nFrom a data set, we see that a female had an arm circumference of 32.49 cm.\nA sample of married couples is randomly selected and the number of children in each family is recorded.\n\n\n\n\n  # (a) Discrete\n  # (b) Continuous\n  # (c) Continuous\n  # (d) Discrete\n\n\n\n\nSampling Method. Identify which of these types of sampling is used: random, stratified, or cluster.\n\nDr.¬†Yu surveys his statistics class by identifying groups of males and females, then randomly selecting 5 students from each of those two groups.\nDr.¬†Yu conducts a survey by randomly selecting 3 different classes at Marquette and surveying all of the students as they left those classes.\n532 subjects were randomly assigned to (1) regular exercise or (2) no exercise groups to study the effectiveness of exercise in lowering blood pressure.\n\n\n\n\n  # (a) Stratified\n  # (b) Cluster\n  # (c) Random\n\n\n\n\nStudy Type. Determine whether the study is an experiment or an observational study, then identify a major problem with this study.\n\nIn a survey conducted by USA Today, 1072 Internet users chose to respond to the question:‚ÄúHow often do you seek medical information online?‚Äù 38% of the respondents said ‚Äúfrequently.‚Äù\nThe Physicians‚Äô Health Study involved 22,071 male physicians. Based on random selections, 11,037 of them were treated with aspirin and other other 11,034 were given placebos. The study was stopped early because it became clear that aspirin reduced the risk of myocardial infarctions by a substantial amount.\n\n\n\n\n  # (a) Observational: self-reported voluntary sample may not be representative and may be biased. The question is posted in E-newsletter, so the sample is biased from the begining.\n  # (b) Experiment: Male physicians only. Better to include male and female who are not physicians.\n\n\n\n\n(MSSC) UK baby names. The visualization below shows the number of baby girls born in the United Kingdom (comprised of England & Wales, Northern Ireland, and Scotland) who were given the name ‚ÄúFiona‚Äù over the years.\n\nList the variables you believe were necessary to create this visualization.\nIndicate whether each variable is numerical or categorical. If numerical, identify as continuous or discrete. If categorical, indicate if the variable is ordinal.\n\n\n\n\n\n\n\n\n\n\n\n\n  # (a) Year, number of baby girls named Fiona born in that year, nation. \n  # (b) Year (numerical, discrete), number of baby girls named Fiona born in that year (numerical, discrete), nation (categorical, nominal).\n\n\n\nAI Usage Declaration. Using GenAI is permitted for this course. If you choose to use GenAI to assist with your homework, you must include a brief statement documenting your use. Please provide the following information:\n\n\nWhy/How I Used AI Why do you need to use GenAI? Which tool did you use? Describe your prompts or questions. What and how did you ask the AI to help you?\n\nGenerated Output Include a screenshot or excerpt (copy and paste) of the AI‚Äôs response.\n\nHow I Used the Output Did you revise it? Did you use it directly, or compare it with your answers? What decisions did you make based on the output?"
  },
  {
    "objectID": "hw/hw02.html",
    "href": "hw/hw02.html",
    "title": "Homework 2",
    "section": "",
    "text": "Important\n\n\n\nDue Friday, Sep 19, 11:59 PM\n\n\n\nHomework 2 covers Week 2 and 3 course materials.\nPlease submit your work in one PDF file to D2L &gt; Assessments &gt; Dropbox. Multiple files or a file that is not in pdf format is not accepted.\nAttach your R code to show how you obtain your answer. \nHand drawn tables and graphs receive no credits.\n\n\nQuestions started with (MSSC) are required for MSSC 5720 students, and optional for MATH 4720 students.\n\nHomework Questions\n\n# ==============================================================================\n## Vector\n# ==============================================================================\npoker_vec &lt;- c(140, -50, 20, -120, 240)\nroulette_vec &lt;- c(-24, -50, 100, -350, 10)\ndays_vec &lt;- c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\")\nnames(poker_vec) &lt;- days_vec\nnames(roulette_vec) &lt;- days_vec\n\n\n\nVector. The code above shows a Marquette student poker and roulette winnings from Monday to Friday. Copy and Paste them into your R and complete the problem 1.\n\nAssign to the variable total_daily how much you won or lost on each day in total (poker and roulette combined).\nCalculate the winnings overall total_week. Print it out.\n\n\n\n\n\n# ==============================================================================\n## Factor\n# ==============================================================================\n# Create speed_vector\nspeed_vec &lt;- c(\"medium\", \"slow\", \"slow\", \"medium\", \"fast\")\n\n\n\nFactor.\n\n\nspeed_vec above should be converted to an ordinal factor since its categories have a natural ordering. Create an ordered factor vector speed_fac by completing the code below. Set the argument ordered to TRUE, and set the argument levels to c(\"slow\", \"medium\", \"fast\"). Print speed_fac.\n\n\n\n\n\n# ==============================================================================\n## Data frame\n# ==============================================================================\nname &lt;- c(\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \n          \"Uranus\", \"Neptune\")\ntype &lt;- c(\"Terrestrial planet\", \"Terrestrial planet\", \"Terrestrial planet\", \n          \"Terrestrial planet\", \"Gas giant\", \"Gas giant\", \n          \"Gas giant\", \"Gas giant\")\ndiameter &lt;- c(0.382, 0.949, 1, 0.532, 11.209, 9.449, 4.007, 3.883)\nrotation &lt;- c(58.64, -243.02, 1, 1.03, 0.41, 0.43, -0.72, 0.67)\nrings &lt;- c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE)\n\n\n\nData Frame. You want to construct a data frame that describes the main characteristics of eight planets in our solar system. You feel confident enough to create the necessary vectors: name, type, diameter, rotation and rings that have already been coded up as above. The first element in each of these vectors correspond to the first observation.\n\nUse the function data.frame() to construct a data frame. Pass the vectors name, type, diameter, rotation and rings as arguments to data.frame(), in this order. Call the resulting data frame planets_df.\nUse str() to investigate the structure of the new planets_df variable. Which are categorical (qualitative) variables and which are numerical (quantitative) variables? For those that are categorical, are they nominal or ordinal? For those numerical variables, are they interval or ratio level? discrete or continuous?\nFrom planets_df, select the diameter of Mercury: this is the value at the first row and the third column. Simply print out the result.\nFrom planets_df, select all data on Mars (the fourth row). Simply print out the result.\nSelect and print out the first 5 values in the diameter column of planets_df.\nUse $ to select the rings variable from planets_df.\nUse (f) to select all columns for planets that have rings.\n\n\n\n\n\n\nData Description and Graphics. We use the data set mtcars to do data summary and graphics. Type ?mtcars for the description of the data set.\n\nUse the function pie() to create a pie chart for the number of cylinders (cyl). Show the plot. What the number of cylinders has the most frequencies in the data?\nUse the function barplot() to create a bar chart for the number of gears (gear). Show the plot. What the number of gears has the most frequencies in the data?\nUse the function hist() to generate a histogram of the gross horsepower (hp). Show the plot. Is it right or left-skewed?\nUse the function boxplot() to generate a boxplot of car weight (wt). Show the plot. Are there any outliers?\nUse the function plot() to create a scatter plot of displacement (disp) vs.¬†miles per gallon (mpg). Show the plot. As the displacement increases, the miles per gallon tends to increase or decrease?\nCompute the mean, median and standard deviation of the miles per gallon (mpg).      \n\n\n\n\nTo save your figures, in RStudio you go to tab Plots &gt; Export &gt; Save as Image &gt; choose Image format (PNG is good), choose where the image is saved (Directory), type the File name, decide Width and Height &gt; Click Save. To download an image file to your local computer, select the file, go to More &gt; Export &gt; Download. You could take screenshots of plots or code to show your work too.\n\n\n\n(MSSC) R List Data Structure. A list in R allows you to gather a variety of objects under one name (that is, the name of the list) in an ordered way. These objects can be matrices, vectors, data frames, even other lists. Alos, it is not required that these objects are related to each other in any way.\n\nUse command list() to create a list named employee of three elements, name, salary, and union with value \"Joe\", 55000 and TRUE respectively. Print employee.\nConstruct a list, named my_list that contains 3 list components: days_vec, planets_df, and employee. Print the structure of my_list.\n\nWhat is the difference between my_list[[1]] and my_list[1]? What is the data type they return?\nObtain the salary in employee from my_list using $.\n\n\n\n\n\n\nAI Usage Declaration. Using GenAI is permitted for this course. If you choose to use GenAI to assist with your homework, you must include a brief statement documenting your use. Please provide the following information:\n\n\nWhy/How I Used AI Why do you need to use GenAI? Which tool did you use? Describe your prompts or questions. What and how did you ask the AI to help you?\n\nGenerated Output Include a screenshot or excerpt (copy and paste) of the AI‚Äôs response.\n\nHow I Used the Output Did you revise it? Did you use it directly, or compare it with your answers? What decisions did you make based on the output?"
  },
  {
    "objectID": "hw/hw07.html",
    "href": "hw/hw07.html",
    "title": "Homework 7",
    "section": "",
    "text": "Important\n\n\n\nDue Friday, Nov 7, 11:59 PM\n\n\n\n\nPlease submit your work in one PDF file to D2L &gt; Assessments &gt; Dropbox. Multiple files or a file that is not in pdf format is not allowed.\nIn your homework, please number questions in order.\n\n\nHandwritten tables and figures receive no credits.\nYou do not need to attach your code of any language you use in the homework. However, if you fail to complete your calculations or produce your table or figure, you receive partial credits if your code is attached.\n\nHomework Questions\n\nThe data about male and female pulse rates are summarized below.\n\nConstruct a 95% CI for \\(\\sigma_{male}\\) of pulse rates for males.\nConstruct a 95% CI for \\(\\sigma_{male}/\\sigma_{female}\\).\nDoes it appear that the population standard deviations for males and females are different? Why or why not?\n\n\n\n\n\n\n\n\n\n\nPulse Rate by Gender\n\n\n\nMale\nFemale\n\n\n\n\nMean (xÃÑ)\n70\n76\n\n\nStandard deviation (s)\n10\n13\n\n\nSample size (n)\n16\n11\n\n\n\n\n\n\n\n\nA production process for filling orange juice containers labeled as 64 ounces is monitored for the actual amount of juice in the container. The process is designed such that the amount of juice in the containers has a normal distribution with a mean of 64.3 ounces and a standard deviation of .15 ounces. The process is monitored by randomly selecting 24 containers every hour and measuring the actual amount of juice in the containers. An increase in the standard deviation beyond .15 ounces with the mean remaining at 64.3 ounces will result in a production run with too many underfilled and overfilled containers. The following data are the actual amounts of juice in a random sample of 24 containers.\n\nIf the amount of juice in the containers has a normal distribution with a mean of 64.3 ounces and a standard deviation of .15 ounces, what proportion of containers filled on the production line will be underfilled (contain less than 64 ounces)?\nConstruct a 95% confidence interval on the process standard deviation.\nDo the test on the claim that the process standard deviation is greater than .15 ounces. Use \\(\\alpha = 0.05\\).\nWhat is the p-value of your test in (c)? (We can use pchisq())\nIf you do (b) and (c) correctly, you should find that (b) and (c) lead to a different conclusion about the claim that \\(\\sigma &gt; 0.15\\). Can you provide the reasoning?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n64.37\n64.26\n64.22\n64.42\n64.13\n64.44\n64.64\n64.19\n63.85\n64.17\n64.21\n64.23\n\n\n64.64\n64.12\n63.98\n64.34\n64.20\n64.31\n64.15\n64.09\n64.33\n64.19\n64.57\n64.19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI Usage Declaration. Using GenAI is permitted for this course. If you choose to use GenAI to assist with your homework, you must include a brief statement documenting your use. Please provide the following information:\n\n\nWhy/How I Used AI Why do you need to use GenAI? Which tool did you use? Describe your prompts or questions. What and how did you ask the AI to help you?\n\nGenerated Output Include a screenshot or excerpt (copy and paste) of the AI‚Äôs response.\n\nHow I Used the Output Did you revise it? Did you use it directly, or compare it with your answers? What decisions did you make based on the output?"
  },
  {
    "objectID": "hw/hw04.html",
    "href": "hw/hw04.html",
    "title": "Homework 4",
    "section": "",
    "text": "Important\n\n\n\nDue Friday, Oct 3, 11:59 PM\n\n\n\nHomework 4 covers Week 5 and 6.\nPlease submit your work in one PDF file to D2L &gt; Assessments &gt; Dropbox. Multiple files or a file that is not in pdf format is not allowed.\n\n\nHandwritten tables and figures receive no credits.\nYou do not need to attach your code of any language you use in the homework. However, if you fail to complete your calculations or produce your table or figure, you receive partial credits if your code is attached.\nQuestions started with (MSSC) are required for MSSC 5720 students, and optional for MATH 4720 students.\n\nHomework Questions\n\nWhat percent of a standard normal distribution \\(N(\\mu=0, \\sigma=1)\\) is found in each region? Draw a normal graph may help.\n\n\\(-0.4 &lt; Z &lt; 1.5\\)\n\\(|Z| &gt; 2\\)\n\n\n\n\n\nThe average daily high temperature in June in Milwaukee is 77\\(^{\\circ}\\) F with a standard deviation of 5\\(^{\\circ}\\) F. Suppose that the temperatures in June closely follow a normal distribution.\n\nWhat is the probability of observing an 83\\(^{\\circ}\\) F temperature or higher in Milwaukee during a randomly chosen day in June?\nHow cool are the coldest 10% of the days (days with lowest average high temperature) during June in Milwaukee?\n\n\n\n\n\nData collected by the Substance Abuse and Mental Health Services Administration (SAMSHA) suggests that 70% of 18-20 year olds consumed alcoholic beverages in any given year.\n\nSuppose a random sample of ten 18-20 year olds is taken. When does it make sense to use binomial distribution for calculating the probability that exactly six consumed alcoholic beverages? \n\nWhat is the probability that exactly four out of ten 18-20 year olds have consumed an alcoholic beverage?\nWhat is the probability that at most 2 out of 5 randomly sampled 18-20 year olds have consumed alcoholic beverages?\n\n\n\n\n\nA Starbucks in Chicago serves an average of 75 customers per hour during the morning rush.\n\nWhich distribution have we studied that is most appropriate for calculating the probability of a given number of customers arriving within one hour during this time of day?\nCalculate the probability that this Starbucks serves 70 customers in one hour during this time of day.\n\n\n\n\n\n\n(MSSC) In observing patients administered a new drug product in a properly conducted clinical trial, the number of persons experiencing a particular side effect might be quite small. Suppose \\(\\pi\\) (the probability a person experiences a side effect to the drug) is \\(.001\\) and \\(1,000\\) patients in the clinical trial received the drug.\n\nUse binomial distribution to Compute the probability that none of a random sample of \\(n = 1,000\\) patients administered the drug experiences a particular side effect.\nUse Poisson distribution to Compute the probability that none of a random sample of \\(n = 1,000\\) patients administered the drug experiences a particular side effect. [Hint:] The parameter \\(\\lambda = n\\pi\\).\nWhen \\(n\\) is large and \\(\\pi\\) is small, Poisson\\((\\lambda)\\) looks pretty similar to binomial\\((n, \\pi)\\). Plot the two distributions for \\(x = 0, 1, \\dots, 8\\). Are they similar?\n\n\n\n\n\n\nAI Usage Declaration. Using GenAI is permitted for this course. If you choose to use GenAI to assist with your homework, you must include a brief statement documenting your use. Please provide the following information:\n\n\nWhy/How I Used AI Why do you need to use GenAI? Which tool did you use? Describe your prompts or questions. What and how did you ask the AI to help you?\n\nGenerated Output Include a screenshot or excerpt (copy and paste) of the AI‚Äôs response.\n\nHow I Used the Output Did you revise it? Did you use it directly, or compare it with your answers? What decisions did you make based on the output?"
  },
  {
    "objectID": "present-work.html",
    "href": "present-work.html",
    "title": "Midterm Project I Proposal and Presentation",
    "section": "",
    "text": "The presentation order is determined by the time you sent me your member info (c(\"John\", \"Jeremy\", \"Praful\") is the first to let me know, followed by c(\"Sajjad\", \"Tanjina\", \"Dewan\"), etc) and random sampling as follows.\n\nteam_lst &lt;- list(c(\"John\", \"Jeremy\", \"Praful\"), \n                 c(\"Sajjad\", \"Tanjina\", \"Dewan\"),\n                 c(\"Ethan\", \"Navid\", \"Sylvester\"), \n                 c(\"Violet\", \"Vanessa\", \"Michele\"), \n                 c(\"Rakesh\", \"Daniel\", \"Jeremy\"), \n                 c(\"Sai\", \"Rohith\", \"Shristi\"))\nset.seed(6250)\nteam_lst[sample(1:6, 6)]\n\n[[1]]\n[1] \"Sajjad\"  \"Tanjina\" \"Dewan\"  \n\n[[2]]\n[1] \"John\"   \"Jeremy\" \"Praful\"\n\n[[3]]\n[1] \"Violet\"  \"Vanessa\" \"Michele\"\n\n[[4]]\n[1] \"Rakesh\" \"Daniel\" \"Jeremy\"\n\n[[5]]\n[1] \"Sai\"     \"Rohith\"  \"Shristi\"\n\n[[6]]\n[1] \"Ethan\"     \"Navid\"     \"Sylvester\""
  },
  {
    "objectID": "present-work.html#presentation-order",
    "href": "present-work.html#presentation-order",
    "title": "Midterm Project I Proposal and Presentation",
    "section": "",
    "text": "The presentation order is determined by the time you sent me your member info (c(\"John\", \"Jeremy\", \"Praful\") is the first to let me know, followed by c(\"Sajjad\", \"Tanjina\", \"Dewan\"), etc) and random sampling as follows.\n\nteam_lst &lt;- list(c(\"John\", \"Jeremy\", \"Praful\"), \n                 c(\"Sajjad\", \"Tanjina\", \"Dewan\"),\n                 c(\"Ethan\", \"Navid\", \"Sylvester\"), \n                 c(\"Violet\", \"Vanessa\", \"Michele\"), \n                 c(\"Rakesh\", \"Daniel\", \"Jeremy\"), \n                 c(\"Sai\", \"Rohith\", \"Shristi\"))\nset.seed(6250)\nteam_lst[sample(1:6, 6)]\n\n[[1]]\n[1] \"Sajjad\"  \"Tanjina\" \"Dewan\"  \n\n[[2]]\n[1] \"John\"   \"Jeremy\" \"Praful\"\n\n[[3]]\n[1] \"Violet\"  \"Vanessa\" \"Michele\"\n\n[[4]]\n[1] \"Rakesh\" \"Daniel\" \"Jeremy\"\n\n[[5]]\n[1] \"Sai\"     \"Rohith\"  \"Shristi\"\n\n[[6]]\n[1] \"Ethan\"     \"Navid\"     \"Sylvester\""
  },
  {
    "objectID": "present-work.html#project-materials",
    "href": "present-work.html#project-materials",
    "title": "Midterm Project I Proposal and Presentation",
    "section": "Project Materials",
    "text": "Project Materials\n\nGroup 1 (Sajjad, Tanjina, Dewan) Predicting Life Expectancy: proposal\nGroup 2 (John, Jeremy, Praful) Poisson Regression: proposal\nGroup 3 (Violet, Vanessa, Michele) Is Ridge Regression or LASSO a better model to predict the price of houses in Ames, Iowa: proposal\nGroup 4 (Rakesh, Daniel, Jeremy) Predicting Abalone Age Using Regression Models: proposal\nGroup 5 (Sai, Rohith, Shristi) Analyzing the Impact of Housing Features on Sale Prices: A Regression-Based Study on the Ames Housing Dataset : proposal\nGroup 6 (Ethan, Navid, Sylvester) Predicting Car Sticker Price Regression: proposal"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (‚ÄúPublic License‚Äù). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 ‚Äì Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter‚Äôs License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 ‚Äì Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor ‚Äì Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor ‚Äì Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter‚Äôs License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 ‚Äì License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter‚Äôs License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter‚Äôs License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter‚Äôs License You apply.\n\n\n\nSection 4 ‚Äì Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 ‚Äì Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 ‚Äì Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 ‚Äì Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 ‚Äì Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the ‚ÄúLicensor.‚Äù The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark ‚ÄúCreative Commons‚Äù or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "present-description.html",
    "href": "present-description.html",
    "title": "Midterm Presentation I - Regression",
    "section": "",
    "text": "Team up! You will be working as a group of 3. One of you, please email me your team member list by Friday, 2/21 11:59 PM\nProposal. Please send me a one-page PDF describing what you are going to do for your project (no word limit) with your project title by Friday, 2/28 11:59 PM.\nPresentation. You will be presenting your project on Thursday, 3/6 in class.\nMaterials. Please share your entire work (slides, code, data, etc) by Friday, 3/7 11:59 PM."
  },
  {
    "objectID": "present-description.html#team-up",
    "href": "present-description.html#team-up",
    "title": "Midterm Presentation I - Regression",
    "section": "Team up!",
    "text": "Team up!\n\nEach one of you loses 5 points of your project grade if you don‚Äôt meet the requirement or miss the deadline.\nYou will be randomly assigned to a group if you do not belong to any group before the deadline."
  },
  {
    "objectID": "present-description.html#proposal",
    "href": "present-description.html#proposal",
    "title": "Midterm Presentation I - Regression",
    "section": "Proposal",
    "text": "Proposal\n\nEach one of you loses 5 points of your project grade if you don‚Äôt meet the requirement or miss the deadline.\n\nYour proposal (in PDF) should include three parts:\n\nProject title\n\nThe goal of your project. For example, what is the research question you‚Äôd like to answer? What machine learning method/model/algorithm you‚Äôd like to introduce? What data you‚Äôd like to use for analysis or demonstration? etc.\n\n\nAlthough it is risky, you can change your project topic after you submit your proposal if you decide to do something else."
  },
  {
    "objectID": "present-description.html#presentation",
    "href": "present-description.html#presentation",
    "title": "Midterm Presentation I - Regression",
    "section": "Presentation",
    "text": "Presentation\n\nEach group presentation should be between 10 and 11 minute long, followed by 1 to 2 minute Q&A. If your presentation is too short or too long, every one of you loses 5 points of your project grade.\nEvery group member has to present some part of the group work. The one who does not present receives no point.\nQuestions are encouraged during Q&A. Everyone is welcome to ask any questions about the projects."
  },
  {
    "objectID": "present-description.html#materials",
    "href": "present-description.html#materials",
    "title": "Midterm Presentation I - Regression",
    "section": "Materials",
    "text": "Materials\n\nEach one of you loses 5 points of your project grade if you don‚Äôt meet the requirement or miss the deadline.\nYou need to share your entire work, including slides, code, and data if applicable.\nYour code should be able to reproduce all the numerical results, outputs, tables, and figures shown in the slides, including the source of the raw data (where you find and load the data) if the project is about data analysis."
  },
  {
    "objectID": "present-description.html#data-analytics",
    "href": "present-description.html#data-analytics",
    "title": "Midterm Presentation I - Regression",
    "section": "Data Analytics",
    "text": "Data Analytics\nFor your DA project, you need to\n\nDescribe the selected data set.\nExplain and show why the chosen model(s) is appropriate for answering your research questions and better than others.\nInterpret your analysis result.\n\nBelow are some data repositories you can start with, but you are encouraged to explore more and find your favorite one.\n\nTidyTuesday\nKaggle\nAwesome Public Datasets\nHarvard Dataverse\nUCI Machine Learning Repository\nFiveThirtyEight"
  },
  {
    "objectID": "present-description.html#modelalgorithm",
    "href": "present-description.html#modelalgorithm",
    "title": "Midterm Presentation I - Regression",
    "section": "Model/Algorithm",
    "text": "Model/Algorithm\nFor your MA project, you need to\n\nDescribe the intuition and idea of the method. What are the pros and cons of the method?\nProvide the mathematical expression of the model/algorithm. Explain the model and its properties, and how we do supervised learning with the model/algorithm.\nCompare the chosen method with other methods learned in class. Determine which method performs better under what conditions.\nDemo how to implement the method using a programming language for supervised learning."
  },
  {
    "objectID": "present-description.html#group-performance-evaluation",
    "href": "present-description.html#group-performance-evaluation",
    "title": "Midterm Presentation I - Regression",
    "section": "Group Performance Evaluation",
    "text": "Group Performance Evaluation\n\nYou will need to evaluate all group projects except the one you work on.\n\nYou evaluate group performance based on the rubric attached. Four evaluation criteria are considered:\n\nProject Content and Organization (8 pts)\nPresentation Material (Slides) Quality (4 pts)\nOral Presentation Skill and Delivery (4 pts)\nInteractions and Q&A (4 pts)\n\n\nThe total points of a project presentation is 20 points.\nEvaluation sheets will be provided on the presentation day.\n\nHow do you get the full points for each category? Check the rubric below. \n\n\nContent and Organization (DA)\n\nBeautiful visualization helps find out relationship of variables and specification of models\nAll questions are answered accurately by the models\nDiscuss how and why the models are chosen\nApply sophisticated models and detailed analysis\nAll ideas are presented in logical order\n\n\n\nContent and Organization (MA)\n\nExplain the method clearly and accuratly\nShow the pros and cons of the method, and compare with the methods learned in class.\nShow how the method can be implemednted for supervised learning.\nAll ideas are presented in logical order\n\n\n\nPresentation Material Quality\n\nPresentation material show code and output beautifully\nPresentation material clearly aid the speaker in telling a coherent story\nAll tables and graphics are informative and related to the topic and make it easier to understand\nAttractive design, layout, and neatness.\n\n\n\nOral Presentation Skill\n\nGood volume and energy\nProper pace and diction\nAvoidance of distracting gestures\n\n\n\nInteractions and Q&A\n\nGood eye contact with audience\nExcellent listening skills\nAnswers audience questions with authority and accuracy\n\n\nAfter you evaluate all group project presentations, you rank them from 1st to last based on their earned points.\nNo two groups receive the same ranking. If you give two or more groups some points, you still need to give them a different ranking, deciding which team deserves a higher ranking according to your preference."
  },
  {
    "objectID": "hw/hw05.html",
    "href": "hw/hw05.html",
    "title": "Homework 5",
    "section": "",
    "text": "Important\n\n\n\nDue Friday, Oct 24, 11:59 PM\n\n\n\nHomework 5 covers Week 7 and 8.\nPlease submit your work in one PDF file to D2L &gt; Assessments &gt; Dropbox. Multiple files or a file that is not in pdf format is not allowed.\n\n\nYou do not need to attach your code of any language you use in the homework. However, if you fail to complete your calculations or produce your table or figure, you receive partial credits if your code is attached.\nQuestions started with (MSSC) are required for MSSC 5720 students, and optional for MATH 4720 students.\n\nHomework Questions\n\nAssume that females have pulse rates that are normally distributed with a mean of 74.0 beats per minute and a standard deviation of 12.5 beats per minute.\n\nIf 1 adult female is randomly selected, find the probability that her pulse rate is less than 80 beats per minute.\nIf 16 adult female are randomly selected, find the probability that their mean pulse rate is less than 80 beats per minute.\nWhy can the normal distribution be used in (b), even though the sample size does not exceed 30?\n\n\n\n\n\nAn independent random sample is selected from an approximately normal population with unknown standard deviation. Find the degrees of freedom and the critical \\(t\\)-value for the given sample size and confidence level.\n\n\n\\(n = 6\\), confidence level 90%\n\n\\(n = 21\\), confidence level 98%\n\n\n\n\n\nHere are summary statistics for randomly selected weights of newborn girls: \\(n =205\\), \\(\\bar{x} = 30.4\\)hg (1hg = 100 grams), \\(s = 7.1\\)hg.\n\nCompute a 95% confidence interval for \\(\\mu\\), the mean weight of newborn girls.\nAre the result in (a) very different from the 95% confidence interval if \\(\\sigma = 7.1\\)? Why?\n\n\n\n\n\nA 95% confidence interval for a population mean \\(\\mu\\) is given as \\((18.98, 21.02)\\). This confidence interval is based on a simple random sample of 36 observations. Calculate the sample mean and standard deviation. Assume that all conditions necessary for inference are satisfied. Use the \\(t\\)-distribution in any calculations.\n\n\n\nA market researcher wants to evaluate car insurance savings at a competing company. Based on past studies he is assuming that the standard deviation \\(\\sigma\\) of savings is $100. He wants to collect data such that he can get a margin of error of no more than $10 at a 95% confidence level. How large of a sample should he collect?\n\n\n\n\n(MSSC) Nonparametric methods for estimating a population mean do not assume any specific distributional form (like normality) for the population. Instead, they rely on the data‚Äôs rank or resampling properties. Describe the idea of boostrapping, and how we can use boostrapping to construct a confidence interval for \\(\\mu\\).\n\n\n\n\nAI Usage Declaration. Using GenAI is permitted for this course. If you choose to use GenAI to assist with your homework, you must include a brief statement documenting your use. Please provide the following information:\n\n\nWhy/How I Used AI Why do you need to use GenAI? Which tool did you use? Describe your prompts or questions. What and how did you ask the AI to help you?\n\nGenerated Output Include a screenshot or excerpt (copy and paste) of the AI‚Äôs response.\n\nHow I Used the Output Did you revise it? Did you use it directly, or compare it with your answers? What decisions did you make based on the output?"
  },
  {
    "objectID": "hw/hw06.html",
    "href": "hw/hw06.html",
    "title": "Homework 6",
    "section": "",
    "text": "Important\n\n\n\nDue Friday, Oct 31, 11:59 PM\n\n\n\nHomework 6 covers Week 9 and 10.\nPlease submit your work in one PDF file to D2L &gt; Assessments &gt; Dropbox. Multiple files or a file that is not in pdf format is not allowed.\nIn your homework, please number questions in order.\n\n\nHandwritten tables and figures receive no credits.\nYou do not need to attach your code of any language you use in the homework. However, if you fail to complete your calculations or produce your table or figure, you receive partial credits if your code is attached.\n\nHomework Questions\n\nYou are given the following hypotheses: \\[\\begin{align*}\nH_0&: \\mu = 60 \\\\\nH_A&: \\mu \\neq 60\n\\end{align*}\\] We know that the sample standard deviation is 8 and the sample size is 20. For what sample mean would the p-value be equal to 0.05? Assume that all conditions necessary for inference are satisfied.\n\n\n\nHere are summary statistics for randomly selected weights of newborn girls: \\(n =205\\), \\(\\bar{x} = 30.4\\)hg (1hg = 100 grams), \\(s = 7.1\\)hg.\n\nWith significance level 0.01, use the critical value method to test the claim that the population mean of birth weights of females is greater than 30hg.\nDo the same test in (a), but use the p-value method. Are the conclusions in (a) and (b) the same?\n\n\n\n\n\nHere are summary statistics for randomly selected weights of newborn girls: \\(n =205\\), \\(\\bar{x} = 30.4\\)hg (1hg = 100 grams), \\(s = 7.1\\)hg.\n\nCompute a 95% confidence interval for \\(\\mu\\), the mean weight of newborn girls.\nAre the result in (a) very different from the 95% confidence interval if \\(\\sigma = 7.1\\)? Why?\n\n\n\n\n\nA study was conducted to assess the effects that occur when children are expected to cocaine before birth. Children were tested at age 4 for object assembly skill, which was described as ‚Äúa task requiring visual-spatial skills related to mathematical competence.‚Äù The 190 children born to cocaine users had a mean of 7.3 and a standard deviation of 3.0. The 186 children not exposed to cocaine had a mean score of 8.2 and a standard deviation of 3.0.\n\nWith \\(\\alpha = 0.05\\), use the critical-value method and p-value method to perform a 2-sample t-test on the claim that prenatal cocaine exposure is associated with lower scores of 4-year-old children on the test of object assembly.\nTest the claim in part (a) by using a confidence interval.\n\n\n\n\n\nListed below are heights (in.) of mothers and their first daughters.\n\nUse \\(\\alpha = 0.05\\) to test the claim that there is no difference in heights between mothers and their first daughters.\nTest the claim in part (a) by using a confidence interval.\n\n\n\n\n\n\n\n\n\n\nHeight of Mother and Daughter (inches)\n\n\nHeight of Mother\nHeight of Daughter\n\n\n\n\n68.0\n68.5\n\n\n60.0\n60.0\n\n\n61.0\n63.5\n\n\n63.5\n67.5\n\n\n69.0\n68.0\n\n\n64.0\n65.5\n\n\n69.0\n69.0\n\n\n64.0\n68.0\n\n\n63.5\n64.5\n\n\n66.0\n63.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI Usage Declaration. Using GenAI is permitted for this course. If you choose to use GenAI to assist with your homework, you must include a brief statement documenting your use. Please provide the following information:\n\n\nWhy/How I Used AI Why do you need to use GenAI? Which tool did you use? Describe your prompts or questions. What and how did you ask the AI to help you?\n\nGenerated Output Include a screenshot or excerpt (copy and paste) of the AI‚Äôs response.\n\nHow I Used the Output Did you revise it? Did you use it directly, or compare it with your answers? What decisions did you make based on the output?"
  },
  {
    "objectID": "hw/hw03.html",
    "href": "hw/hw03.html",
    "title": "Homework 3",
    "section": "",
    "text": "Important\n\n\n\nDue Friday, Sep 26, 11:59 PM\n\n\n\nHomework 3 covers Week 4.\nPlease submit your work in one PDF file to D2L &gt; Assessments &gt; Dropbox. Multiple files or a file that is not in pdf format is not allowed.\n\n\nQuestions started with (MSSC) are required for MSSC 5720 students, and optional for MATH 4720 students.\n\nHomework Questions\n\n\nProbability Rules. A Pew Research survey asked 2,373 randomly sampled registered voters their political affiliation (Republican, Democrat, or Independent) and whether or not they identify as swing voters. 35% of respondents identified as Independent, 23% identified as swing voters, and 11% identified as both.\n\n\nWhat percent of voters are Independent but not swing voters?\nWhat percent of voters are Independent or swing voters?\nWhat percent of voters are neither Independent nor swing voters?\nIs the event that someone is a swing voter independent of the event that someone is a political Independent?\n\n\n\n\nConditional Probability and Independence. A Pew Research poll asked 1,306 Americans ``From what you‚Äôve read and heard, is there solid evidence that the average temperature on earth has been getting warmer over the past few decades, or not?‚Äú. The table below shows the distribution of responses by party and ideology, where the counts have been replaced with relative frequencies.\n\n\nTable 1. Party ideology and responses\n\n\n\n\n\n\n\n\n\n\n\n\nResponse\n\n\n\n\n\n\n\nEarth warming\nNot warming\nDon‚Äôt Know\nTotal\n\n\n\nConservative Republican\n0.11\n0.20\n0.02\n0.33\n\n\nParty and Ideology\nMod/Lib Republican\n0.06\n0.06\n0.01\n0.13\n\n\n\nMod/Cons Democrat\n0.25\n0.07\n0.02\n0.34\n\n\n\nLiberal Democrat\n0.18\n0.01\n0.01\n0.20\n\n\n\nTotal\n0.60\n0.34\n0.06\n1.00\n\n\n\n\nAre believing that the earth is warming and being a liberal Democrat mutually exclusive?\nWhat is the probability that a randomly chosen respondent believes the earth is warming or is a liberal Democrat?\nWhat is the probability that a randomly chosen respondent believes the earth is warming given that he is a liberal Democrat?\nWhat is the probability that a randomly chosen respondent believes the earth is warming given that he is a conservative Republican?\nDoes it appear that whether or not a respondent believes the earth is warming is independent of their party and ideology? Explain your reasoning. \n\n\n\n\n\nBayes Formula. After an MATH 4720/MSSC 5720 course, 80% of students can successfully construct box plots using R. Of those who can construct box plots, 86% passed, while only 65% of those students who could not construct box plots passed. Calculate the probability that a student is able to construct a box plot if it is known that she passed.\n\n\n\n\n(MSSC) Lupus is a medical phenomenon where antibodies that are supposed to attack foreign cells to prevent infections instead see plasma proteins as foreign bodies, leading to a high risk of blood clotting. It is believed that 2% of the population suffer from this disease. The test is 98% accurate if a person actually has the disease. The test is 74% accurate if a person does not have the disease. There is a line from the Fox television show House that is often used after a patient tests positive for lupus: ‚ÄúIt‚Äôs never lupus.‚Äù Do you think there is truth to this statement? Use appropriate probabilities to support your answer.\n\n\n\n\nAI Usage Declaration. Using GenAI is permitted for this course. If you choose to use GenAI to assist with your homework, you must include a brief statement documenting your use. Please provide the following information:\n\n\nWhy/How I Used AI Why do you need to use GenAI? Which tool did you use? Describe your prompts or questions. What and how did you ask the AI to help you?\n\nGenerated Output Include a screenshot or excerpt (copy and paste) of the AI‚Äôs response.\n\nHow I Used the Output Did you revise it? Did you use it directly, or compare it with your answers? What decisions did you make based on the output?"
  },
  {
    "objectID": "hw/hw06-extra.html",
    "href": "hw/hw06-extra.html",
    "title": "Happy Halloween!üéÉüëª",
    "section": "",
    "text": "Important\n\n\n\nDue Friday, Oct 31, 11:59 PM\n\n\n\nPlease include this work in your Homework 6 submission. Submissions as a separate file or standalone document will not be accepted for extra credit.\n\n\nExtra Credit Questions\n\nDownload the R script pumpkin.R, and run the code to generate your statistical pumpkin! Include the resulting image in your Homework 6.\nLook through the code and identify any probability distributions used to create the pumpkin image."
  },
  {
    "objectID": "hw/hw08.html",
    "href": "hw/hw08.html",
    "title": "Homework 8",
    "section": "",
    "text": "Important\n\n\n\nDue Friday, Dec 5, 11:59 PM\n\n\n\n\nPlease submit your work in one PDF file to D2L &gt; Assessments &gt; Dropbox. Multiple files or a file that is not in pdf format is not allowed.\nIn your homework, please number questions in order.\n\n\nHandwritten tables and figures receive no credits.\nYou do not need to attach your code of any language you use in the homework. However, if you fail to complete your calculations or produce your table or figure, you receive partial credits if your code is attached.\nQuestions started with (MSSC) are required for MSSC 5720 students, and optional for MATH 4720 students.\n\nHomework Questions\n\nMaru and Lulu are both collecting data on number of rainy days in a year and the total rainfall for the year. Maru records rainfall in inches and Lulu in centimeters. How will their correlation coefficients compare?\n\n\n\nSuppose we fit a regression line to predict the shelf life of an apple based on its weight. For a particular apple, we predict the shelf life to be 4.6 days. The apple‚Äôs residual is -0.6 days. Did we over or under estimate the shelf-life of the apple? Explain your reasoning.\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\nTar\n25.0\n27.0\n20.0\n24.0\n20.0\n20\n21.0\n24.0\n\n\nNicotine\n1.5\n1.7\n1.1\n1.6\n1.1\n1\n1.2\n1.4\n\n\n\n\n\n\n\nConstruct a scatterplot using tar for the \\(x\\) axis and nicotine for the \\(y\\) axis. Does the scatterplot suggest a linear relationship between the two variables? Are they positively or negatively related?\nLet \\(y\\) be the amount of nicotine and let \\(x\\) be the amount of tar. Fit a simple linear regression to the data and identify the sample regression equation.\nWhat percentage of the variation in nicotine can be explained by the linear correlation between nicotine and tar?\nThe Raleigh brand king size cigarette is not included in the table, and it has 23 mg of tar. What is the best predicted amount of nicotine? How does the predicted amount compare to the actual amount of 1.3 mg of nicotine? What is the value of residual?\nPerform the test \\(H_0: \\beta_1 = 0\\) vs.¬†\\(H_1: \\beta_1 \\ne 0\\).\nProvide 95% confidence interval for \\(\\beta_1\\).\nGenerate the ANOVA table for the linear regression.\n\n\n\n\n\n(MSSC) We have been doing a variety of hypothesis tests, two-sample pooled \\(t\\) tests, \\(F\\)-test for comparing two variances for example. However, the null hypothesis significance testing (NHST) paradigm and the p-value usage have been much criticized and shown to be problematic and often misused in data analysis. In fact, in his research, Dr.¬†Yu never does NHST or uses p-value taught in MATH 4720. Please read the articles\n\nRonald L. Wasserstein and Nicole A. Lazar (2016), ‚ÄúThe ASA Statement on p-Values: Context, Process, and Purpose (page 131 ‚Äì 132)‚Äù\nV. Amrhein, S. Greenland and B. McShane (2019), ‚ÄúRetire statistical significance‚Äù, Nature, 567, 305-307.\nR. Nuzzo (2014), ‚ÄúScientific method: Statistical errors‚Äù, Nature, 506, 150‚Äì152.\n\nWrite a one-page summary discussing the problems of NHST paradigm and p-value. Welcome to share what does not make sense to you about the hypothesis testing (or confidence interval) that was proposed about 100 years ago.\n\n\n\n\n\nAI Usage Declaration. Using GenAI is permitted for this course. If you choose to use GenAI to assist with your homework, you must include a brief statement documenting your use. Please provide the following information:\n\n\nWhy/How I Used AI Why do you need to use GenAI? Which tool did you use? Describe your prompts or questions. What and how did you ask the AI to help you?\n\nGenerated Output Include a screenshot or excerpt (copy and paste) of the AI‚Äôs response.\n\nHow I Used the Output Did you revise it? Did you use it directly, or compare it with your answers? What decisions did you make based on the output?"
  },
  {
    "objectID": "present-description-2.html",
    "href": "present-description-2.html",
    "title": "Midterm Presentation II - Classification",
    "section": "",
    "text": "Proposal. Please send me a one-page PDF describing what you are going to do for your project (no word limit) with your project title by Tuesday, 4/8 11:59 PM.\nPresentation. You will be presenting your project on Tuesday, 4/15 in class.\nMaterials. Please share your entire work (slides, code, data, etc) by Friday, 4/18 11:59 PM."
  },
  {
    "objectID": "present-description-2.html#proposal",
    "href": "present-description-2.html#proposal",
    "title": "Midterm Presentation II - Classification",
    "section": "Proposal",
    "text": "Proposal\n\nEach one of you loses 5 points of your project grade if you don‚Äôt meet the requirement or miss the deadline.\n\nYour proposal (in PDF) should include three parts:\n\nProject title\n\nThe goal of your project. For example, what is the research question you‚Äôd like to answer? What machine learning method/model/algorithm you‚Äôd like to introduce? What data you‚Äôd like to use for analysis or demonstration? etc.\n\n\nAlthough it is risky, you can change your project topic after you submit your proposal if you decide to do something else."
  },
  {
    "objectID": "present-description-2.html#presentation",
    "href": "present-description-2.html#presentation",
    "title": "Midterm Presentation II - Classification",
    "section": "Presentation",
    "text": "Presentation\n\nEach group presentation should be between 10 and 11 minute long, followed by 1 to 2 minute Q&A. If your presentation is too short or too long, every one of you loses 5 points of your project grade.\nEvery group member has to present some part of the group work. The one who does not present receives no point.\nQuestions are REQUIRED during Q&A. Each group is required to ask as least one question. More questions are welcome. If you, as a group, don‚Äôt ask a question when you should, every one of you loses 5 points of your project grade.\n\n\n\n\nTeam Presenting\nTeam Asking Questions\n\n\n\nTeam 1\nTeam 6\n\n\nTeam 2\nTeam 1\n\n\nTeam 3\nTeam 2\n\n\nTeam 4\nTeam 3\n\n\nTeam 5\nTeam 4\n\n\nTeam 6\nTeam 5"
  },
  {
    "objectID": "present-description-2.html#materials",
    "href": "present-description-2.html#materials",
    "title": "Midterm Presentation II - Classification",
    "section": "Materials",
    "text": "Materials\n\nEach one of you loses 5 points of your project grade if you don‚Äôt meet the requirement or miss the deadline.\nYou need to share your entire work, including slides, code, and data if applicable.\nYour code should be able to reproduce all the numerical results, outputs, tables, and figures shown in the slides, including the source of the raw data (where you find and load the data) if the project is about data analysis."
  },
  {
    "objectID": "present-description-2.html#data-analytics",
    "href": "present-description-2.html#data-analytics",
    "title": "Midterm Presentation II - Classification",
    "section": "Data Analytics",
    "text": "Data Analytics\nFor your DA project, you need to\n\nDescribe the selected data set.\nExplain and show why the chosen model(s) is appropriate for answering your research questions and better than others.\nInterpret your analysis result.\n\nBelow are some data repositories you can start with, but you are encouraged to explore more and find your favorite one.\n\nTidyTuesday\nKaggle\nAwesome Public Datasets\nHarvard Dataverse\nUCI Machine Learning Repository\nFiveThirtyEight"
  },
  {
    "objectID": "present-description-2.html#modelalgorithm",
    "href": "present-description-2.html#modelalgorithm",
    "title": "Midterm Presentation II - Classification",
    "section": "Model/Algorithm",
    "text": "Model/Algorithm\nFor your MA project, you need to\n\nDescribe the intuition and idea of the method. What are the pros and cons of the method?\nProvide the mathematical expression of the model/algorithm. Explain the model and its properties, and how we do supervised learning with the model/algorithm.\nCompare the chosen method with other methods learned in class. Determine which method performs better under what conditions.\nDemo how to implement the method using a programming language for supervised learning."
  },
  {
    "objectID": "present-description-2.html#group-performance-evaluation",
    "href": "present-description-2.html#group-performance-evaluation",
    "title": "Midterm Presentation II - Classification",
    "section": "Group Performance Evaluation",
    "text": "Group Performance Evaluation\n\nYou will need to evaluate all group projects except the one you work on.\n\nYou evaluate group performance based on the rubric attached. Four evaluation criteria are considered:\n\nProject Content and Organization (8 pts)\nPresentation Material (Slides) Quality (4 pts)\nOral Presentation Skill and Delivery (4 pts)\nInteractions and Q&A (4 pts)\n\n\nThe total points of a project presentation is 20 points.\nEvaluation sheets will be provided on the presentation day.\n\nHow do you get the full points for each category? Check the rubric below. \n\n\nContent and Organization (DA)\n\nBeautiful visualization helps find out relationship of variables and specification of models\nAll questions are answered accurately by the models\nDiscuss how and why the models are chosen\nApply sophisticated models and detailed analysis\nAll ideas are presented in logical order\n\n\n\nContent and Organization (MA)\n\nExplain the method clearly and accuratly\nShow the pros and cons of the method, and compare with the methods learned in class.\nShow how the method can be implemednted for supervised learning.\nAll ideas are presented in logical order\n\n\n\nPresentation Material Quality\n\nPresentation material show code and output beautifully\nPresentation material clearly aid the speaker in telling a coherent story\nAll tables and graphics are informative and related to the topic and make it easier to understand\nAttractive design, layout, and neatness.\n\n\n\nOral Presentation Skill\n\nGood volume and energy\nProper pace and diction\nAvoidance of distracting gestures\n\n\n\nInteractions and Q&A\n\nGood eye contact with audience\nExcellent listening skills\nAnswers audience questions with authority and accuracy\n\n\nAfter you evaluate all group project presentations, you rank them from 1st to last based on their earned points.\nNo two groups receive the same ranking. If you give two or more groups some points, you still need to give them a different ranking, deciding which team deserves a higher ranking according to your preference."
  },
  {
    "objectID": "activity/act_distribution.html",
    "href": "activity/act_distribution.html",
    "title": "Activity: Match the Scenario to the Distribution",
    "section": "",
    "text": "You will practice identifying which type of probability distribution (binomial, Poisson, or normal) can approximately fit or describe a given real-world context, and explain why."
  },
  {
    "objectID": "activity/act_distribution.html#objective",
    "href": "activity/act_distribution.html#objective",
    "title": "Activity: Match the Scenario to the Distribution",
    "section": "",
    "text": "You will practice identifying which type of probability distribution (binomial, Poisson, or normal) can approximately fit or describe a given real-world context, and explain why."
  },
  {
    "objectID": "activity/act_distribution.html#instructions",
    "href": "activity/act_distribution.html#instructions",
    "title": "Activity: Match the Scenario to the Distribution",
    "section": "Instructions",
    "text": "Instructions\n\nForm a group of three or four.\nScan the QR code, or go to https://forms.office.com/r/bprPDtLtbQ to fill out your names.\nIn the form, read each scenario, choose the most appropriate distribution, or all three distributions may not be appropriate to model the scenario.\nThe first group with the most correct answers will get a hex sticker, and share their reasoning!"
  },
  {
    "objectID": "activity/act_distribution.html#scenarios",
    "href": "activity/act_distribution.html#scenarios",
    "title": "Activity: Match the Scenario to the Distribution",
    "section": "Scenarios",
    "text": "Scenarios\n\nA quality inspector checks 20 light bulbs, each having a 5 percent chance of being defective. What distribution models the number of defective bulbs?\nThe number of emails arriving in Dr.¬†Yu‚Äôs inbox between 2 and 3 pm.\nThe distribution of households income in 2010 in the USA.\n\n\n\n\n\n\n\n\n\n\nThe number of students who show up to class without doing their homework, out of 30 students.\nTime between buses arriving at a stop The Commons.\nThe number of cars passing through a toll booth in a 10 minute interval.\nThe heights of adult women in the United States.\n\n\n\n\n\n\n\n\n\n\nStudents‚Äô letter grades in Dr.¬†Yu‚Äôs MATH 4720 class (A, B, C, D, F).\nThe distribution of blood pressure of adults in Milwaukee.\n\n\n\n\n\n\n\n\n\n\nSurvival time of cancer patients after chemotherapy."
  },
  {
    "objectID": "activity/act_distribution.html#quick-reference-key-features",
    "href": "activity/act_distribution.html#quick-reference-key-features",
    "title": "Activity: Match the Scenario to the Distribution",
    "section": "Quick Reference: Key Features",
    "text": "Quick Reference: Key Features\n\n\n\n\n\n\n\nDistribution\nWhen to use\nCommon parameters\n\n\n\nBinomial\nFixed number of trials, two outcomes per trial, constant probability, independent trials\n\n\\((n)\\) number of trials, \\((\\pi)\\) probability of success\n\n\nPoisson\nCounting events in a fixed interval of time or space, events occur independently at a constant average rate\n\n\\((\\lambda)\\) average rate per interval\n\n\nNormal\nContinuous measurements that are symmetric around a center with bell shape\n\n\\((\\mu)\\) mean, \\((\\sigma)\\) standard deviation\n\n\n\n\n  \n    ‚àí\n    +\n \n 10:00"
  },
  {
    "objectID": "activity/act_ci.html",
    "href": "activity/act_ci.html",
    "title": "Activity: How Much Do Students Sleep?",
    "section": "",
    "text": "You will create your own confidence interval for estimating on average how much college students sleep."
  },
  {
    "objectID": "activity/act_ci.html#objective",
    "href": "activity/act_ci.html#objective",
    "title": "Activity: How Much Do Students Sleep?",
    "section": "",
    "text": "You will create your own confidence interval for estimating on average how much college students sleep."
  },
  {
    "objectID": "activity/act_ci.html#instructions",
    "href": "activity/act_ci.html#instructions",
    "title": "Activity: How Much Do Students Sleep?",
    "section": "Instructions",
    "text": "Instructions\n\nForm a group of three or four. (This is your sample size!)\nRecord how many hours of sleep each one of you got last night. No need to be integers. Sleeping 7.6 hours totally works! So please use decimal hours as your data. Don‚Äôt know decimal hours for 7 hours 20 minutes? Use the app below for the answer!\n\n\n\n\nConstruct your 95% confidence interval for mean sleeping hours using the formula\n\n\\[ \\left[ \\bar{x} - t_{0.025, n-1} \\frac{s}{\\sqrt{n}}, \\, \\bar{x} + t_{0.025, n-1} \\frac{s}{\\sqrt{n}} \\right]\\]\nwhere \\(\\bar{x}\\) is the sample mean, \\(s\\) is the sample standard deviation, \\(n\\) is the sample size, and \\(t_{0.025, n-1}\\) is the \\(t\\) critical value such that \\(P(T &gt; t_{0.025, n-1}) = 2.5\\%\\).\n\nLet‚Äôs create a larger sample by considering all of your sleeping hours. Construct its 95% confidence interval. Note that you need to obtain new \\(\\bar{x}\\), \\(s\\), \\(n\\), and \\(t_{0.025, n-1}\\)."
  },
  {
    "objectID": "activity/act_ci.html#discussion",
    "href": "activity/act_ci.html#discussion",
    "title": "Activity: How Much Do Students Sleep?",
    "section": "Discussion",
    "text": "Discussion\n\nWhat does this interval mean in words?\nDoes the CI tell us about individual sleep hours or about the average student‚Äôs sleep?\nIf we repeated this process with new samples, would every interval capture the true population mean?\n\n\n  \n    ‚àí\n    +\n \n 10:00"
  },
  {
    "objectID": "ojs/infer-var-chisq-ojs.html",
    "href": "ojs/infer-var-chisq-ojs.html",
    "title": "Chi-squared distribution",
    "section": "",
    "text": "viewof params = Inputs.form([\n      Inputs.range([1, 100], {value: 4, step: 1, label: tex`\\nu`}),\n      Inputs.range([0.01, 50], {value: 2, step: 0.01, label: \"quantile:\"})\n    ])\n\n\n\n\n\n\n\ndist_quantile = tex`\\text{If } X \\sim \\chi^2_{${params[0]}} \\text{ then } \\\\[1em]`\n\n\n\n\n\n\n\nmoments = tex`\n\\begin{aligned}\n&E( X) =\\nu = ${params[0].toPrecision(3)} \\\\[1em]\n&Var( X) =2\\nu= ${(2*params[0]).toPrecision(3)} \\\\[1em]\n&P( X\\leq ${params[1]}) =${chisquarecdf.toPrecision(4)}\n\\end{aligned}\n`\n\n\n\n\n\n\n\n\n\n\n\n\nplt_pdf = Plot.plot({\n    width: 800, // or a dynamic value based on `width` variable\n    height: 600,\n    style: {fontSize: \"15px\"},\n    color: {\n      legend: true\n    },\n    x: {\n      label: \"x\",\n      axis: true\n    },\n    y: {\n      label: \"f(x)\",\n      //axis: false,\n      domain: [0,jstat.chisquare.pdf(Math.max(0,params[0]-2), params[0])]\n    },\n    marks: [\n      Plot.ruleY([0]),\n      Plot.ruleX([0]),\n      Plot.line(chisquarepdf, {x: \"x\", y: \"pdf\", stroke : \"steelblue\", strokeWidth: 2}),\n      Plot.areaY(chisquarepdf, {filter: d =&gt; d.x &lt;= params[1], x: \"x\", y: \"pdf\", fill: \"steelblue\", opacity: 0.2})\n    ]\n  })\n\n\n\n\n\n\n\n\nFigure¬†1: https://observablehq.com/@mattiasvillani/chi2-distribution\n\n\n\n\n\nchisquarepdf = {\n  const x = d3.range(0.01, params[0]+5*Math.sqrt(2*params[0]), 0.01);\n  const pdf = x.map(x =&gt; ({x: x, pdf: jstat.chisquare.pdf(x, params[0])}));\n  return pdf\n}\n\n\n\n\n\n\n\nchisquarecdf = jstat.chisquare.cdf(params[1], params[0]);\n\n\n\n\n\n\n\njstat = require('jstat')"
  },
  {
    "objectID": "ojs/data-extremes-ojs.html",
    "href": "ojs/data-extremes-ojs.html",
    "title": "Extreme values",
    "section": "",
    "text": "viewof N = Inputs.range([2, 100], {value: 10, step: 1, label: \"number of data points\"})\n\n\n\n\n\n\n\nviewof maxscalefact = Inputs.range([1, 5], {value: 1, label: \"Multiply the largest data point with\", step: 1})\n\n\n\n\n\n\n\njstat = require('jstat')\n\n\n\n\n\n\n\npop = d3.sort(d3.range(N).map(d =&gt; Math.round(jstat.normal.sample( 50, 10 ).toFixed(2))))\n\n\n\n\n\n\n\ndatascaled = {\n  var tmp;\n  tmp = pop.map(d =&gt; ({data: d, type: \"data points\", width: 1, symbol: \"circle\", radius: 1}))\n  tmp[tmp.length - 1] = {data: d3.max(pop)*maxscalefact, type: \"max value\", width: 3, symbol: \"circle\", radius: 1}\n  const meanval = d3.mean(tmp.map(d =&gt; d.data))\n  const medianval = d3.median(tmp.map(d =&gt; d.data))\n  tmp.push(({data: meanval, type: \"mean\", width: 3, symbol: \"diamond\", radius: 3}))\n  tmp.push(({data: medianval, type: \"median\", width: 3, symbol: \"square\", radius: 3}))\n  return tmp\n}\n\n\n\n\n\n\n\nPlot.plot({\n  width: 1800, // or a dynamic value based on `width` variable\n  height: 1200,\n  color: {legend: true},\n  x: {\n    domain: [0, 500]\n  },\n  height: 100,\n  marks: [\n    Plot.dotX(datascaled, {filter: d =&gt; d.type == \"data points\" || d.type == \"max value\", x: \"data\", stroke: \"type\", strokeWidth: \"width\", symbol: \"symbol\", r: \"radius\"}),\n    Plot.tickX(datascaled, {filter: d =&gt; d.type == \"mean\" || d.type == \"median\", x: \"data\", stroke: \"type\", strokeWidth: \"width\", symbol: \"symbol\"}),\n  ]\n})\n\n\n\n\n\n\n\n\nFigure¬†1: https://observablehq.com/@mattiasvillani/how-the-mean-and-median-are-affected-by-extreme-observation?collection=@mattiasvillani/statistics"
  },
  {
    "objectID": "ojs/model-reg-ojs.html",
    "href": "ojs/model-reg-ojs.html",
    "title": "Linear regression Model Fitting",
    "section": "",
    "text": "viewof n = Inputs.range([10, 1000], {value: 10, step: 1, label: tex`\\text{sample size, }n`});\nviewof beta0 = Inputs.range([-2, 2], {value: -1, step: 0.1, label: tex`\\text{intercept, }\\beta_0`})\nviewof beta1 = Inputs.range([-2, 2], {value: 1, step: 0.1, label: tex`\\text{slope, }\\beta_1`})\nviewof sigmaepsilon = Inputs.range([0, 1], {value: 0.5, step: 0.1, label: tex`\\text{sd of error, }\\sigma`})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof sampleno = Scrubber(d3.ticks(1, 1000, 1000), {\n  autoplay: false,\n  loop: false,\n  delay: 900,\n  initial: 0,\n  format: x =&gt; `dataset no: ${x.toFixed(0)}`\n})\n\n\n\n\n\n\n\nestimates = tex`\\text{Estimated intercept: } b_0 = ${data_estimates[1][sampleno-1].intercept.toPrecision(4)} \\\\\n\\\\\n\\text{Estimated slope: } b_1 =${data_estimates[1][sampleno-1].lutning.toPrecision(4)}`\n\n\n\n\n\n\n\n\n\n\n\n\nplt = Plot.plot({\n  width: 800, // or a dynamic value based on `width` variable\n  height: 600,\n  style: {fontSize: \"16px\"},\n  caption: html`&lt;span style=\"color:orange\"&gt;Population Model&lt;/span&gt; &lt;br&gt; &lt;span style=\"color:steelblue\"&gt;Estimated regression line&lt;/span&gt;`,\n  color: {legend: true},\n  x: {domain: [-1, 1]},\n  //y: {domain: [-3,3]},\n  y: {domain: [beta0 + (-1)*beta1 - 3*sigmaepsilon, beta0 + 1*beta1 + 3*sigmaepsilon]},\n  marks: [\n    Plot.ruleY([beta0 + (-1)*beta1 - 3*sigmaepsilon]),\n    Plot.ruleX([-1]),\n    Plot.line(\n      [{x: -1, y: beta0 + (-1)*beta1}, {x:  1, y: beta0 + (1)*beta1}], \n      {x: \"x\", y: \"y\", stroke: \"orange\", strokeWidth: 5}\n    ),\n    Plot.dot(data_estimates[0][sampleno-1], {x: \"x\", y: \"y\", strokeOpacity: 0.5, r: 10, fill: colors[0]}),\n    Plot.line(\n      [{x: -1, y: data_estimates[1][sampleno-1].intercept + (-1)*data_estimates[1][sampleno-1].lutning}, \n       {x:  1, y: data_estimates[1][sampleno-1].intercept + (1)*data_estimates[1][sampleno-1].lutning}\n      ], \n      {x: \"x\", y: \"y\", stroke: \"steelblue\", strokeWidth: 5}\n    ),\n  ],\n})\n\n\n\n\n\n\n\n\nFigure¬†1: https://observablehq.com/@mattiasvillani/samplingfordelningen-linear-regression\n\n\n\n\n\nimport {lm} from \"@chrispahm/linear-models-in-observable-notebooks\"\nimport {summary} from \"@chrispahm/linear-models-in-observable-notebooks\"\nimport {Histogram} from \"@d3/histogram\"\nimport {Scrubber} from \"@mbostock/scrubber\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njstat = require('jstat')\n\n\n\n\n\n\n\nmath = require('mathjs')\n\n\n\n\n\n\n\nfunction simulate_and_estimate(n, beta0, beta1, sigmaepsilon, nrep){\n  const x = d3.range(-1, 1, 2/n);\n  const xbar = d3.mean(x)\n  const SSx = d3.sum(math.dotPow(math.subtract(x, xbar), 2))\n  var data = [];\n  var coef = [];\n  for (let j = 1; j &lt;= nrep; j++){\n    var dataset = x.map(x =&gt; ({sample_id: j, x: x, y: beta0 + beta1*x + jstat.normal.sample(0,sigmaepsilon)}))\n    data.push(dataset);\n    let y = dataset.map(d =&gt; d.y)\n    let ybar = d3.mean(y)\n    let SSxy = d3.sum(math.dotMultiply(math.subtract(x, xbar), math.subtract(y, ybar))  )\n    let b1 = SSxy/SSx\n    let b0 = ybar - b1*xbar\n    //let linear_model = lm('y ~ x', dataset);\n    coef.push(\n      {\n        sample_id: j, \n        //intercept: linear_model.coefficients[0],\n        //lutning: linear_model.coefficients[1],\n        intercept: b0,\n        lutning: b1\n      }\n    )\n  }\n  return [data, coef]\n}\n\n\n\n\n\n\n\ndata_estimates = simulate_and_estimate(n, beta0, beta1, sigmaepsilon, 1000)\n\n\n\n\n\n\n\ncolors = d3.schemePaired"
  },
  {
    "objectID": "ojs/prob-clt-ojs.html",
    "href": "ojs/prob-clt-ojs.html",
    "title": "MATH 4720 (MSSC 5720) - Spring 2026",
    "section": "",
    "text": "viewof dist_type = Inputs.select(['beta', 'bimodal', 'binomial', 'cauchy', 'chi2', 'lognormal', 'poisson', 'studentt', 'uniform',], {value: \"uniform\", label: \"data distribution:\"})\n\n\n\n\n\n\n\n// Returns a normal deviate (mu=0, sigma=1).\nfunction randn() {\n  let u, v, x, y, q;\n  do {\n    u = Math.random();\n    v = 1.7156 * (Math.random() - 0.5);\n    x = u - 0.449871;\n    y = Math.abs(v) + 0.386595;\n    q = x * x + y * (0.19600 * y - 0.25472 * x);\n  } while (q &gt; 0.27597 && (q &gt; 0.27846 || v * v &gt; -4 * Math.log(u) * u * u));\n  return v / u;\n}\n\n\n\n\n\n\n\nviewof params_clt = {\n  var form;\n    if (dist_type == 'binomial'){\n    form = Inputs.form([\n      Inputs.range([1, 30], {value: 10, step: 1, label: tex`\\text{# of trials }n`}),\n      Inputs.range([Number.EPSILON, 1 - Number.EPSILON], {value: 0.3, step: 0.01, label: tex`\\text{success prob }\\pi`})\n    ])\n  }\n  if (dist_type == 'poisson'){\n    form = Inputs.form([\n      Inputs.range([0, 10], {value: 1, step: 0.1, label: tex`\\text{mean }\\lambda`})\n    ])\n  }\n  if (dist_type == 'beta'){\n    form = Inputs.form([\n      Inputs.range([0.1, 20], {value: 3, step: 0.1, label: tex`\\text{shape }\\alpha`}),\n      Inputs.range([0.1, 20], {value: 5, step: 0.1, label: tex`\\text{shape }\\beta`})\n    ])\n  }\n  if (dist_type == 'cauchy'){\n    form = Inputs.form([\n      Inputs.range([-5, 5], {value: 0, step: 0.1, label: tex`\\text{location }m`}),\n      Inputs.range([0.1, 5], {value: 1, step: 0.1, label: tex`\\text{scale }\\gamma`}),\n    ])\n  }\n  else if (dist_type == 'uniform'){\n    form = Inputs.form([\n      Inputs.range([-5, 5], {value: 0, step: 0.1, label: tex`\\text{lower limit }a`}),\n      Inputs.range([-5, 5], {value: 1, step: 0.1, label: tex`\\text{upper limit }b`})\n    ])\n  }\n  else if (dist_type == 'studentt'){\n    form = Inputs.form([\n      Inputs.range([-5, 5], {value: 0, step: 0.1, label: tex`\\text{location }\\mu`}),\n      Inputs.range([0.1, 5], {value: 1, step: 0.1, label: tex`\\text{scale }\\sigma`}),\n      Inputs.range([1, 50], {value: 4, step: 1, label: tex`\\text{df }\\nu`})\n    ])\n  }\n  else if (dist_type == 'chi2'){\n    form = Inputs.form([\n      Inputs.range([1, 50], {value: 3, step: 1, label: tex`\\text{df }\\nu`})\n    ])\n  }\n  else if (dist_type == 'lognormal'){\n    form = Inputs.form([\n      Inputs.range([-5,5], {value: 0, step: 0.1, label: tex`\\text{location }\\mu`}),\n      Inputs.range([0.1,5], {value: 1, step: 0.1, label: tex`\\text{scale }\\sigma`})\n    ])\n  }\n  else if (dist_type == 'bimodal'){\n    form = Inputs.form([\n      Inputs.range([-5,5], {value: -2, step: 0.1, label: tex`\\text{location }\\mu_1`}),\n      Inputs.range([-5,5], {value: 2, step: 0.1, label: tex`\\text{location }\\mu_2`}),\n      Inputs.range([0.1,5], {value: 1, step: 0.1, label: tex`\\text{scale }\\sigma_1`}),\n      Inputs.range([0.1,5], {value: 1, step: 0.1, label: tex`\\text{scale }\\sigma_2`}),\n      Inputs.range([0,1], {value: 0.5, step: 0.01, label: tex`\\text{weight }w_1`})\n    ])\n  }\n  return form;\n}\n\n\n\n\n\n\n\nviewof sample_size = Inputs.range([1, 1000], {value: 2, step: 1, label: tex`\\text{sample size }n:`})\n\n\n\n\n\n\n\njStat = require('jstat')\n\n\n\n\n\n\n\nnRep = 5000\n\n\n\n\n\n\n\nfunction rand_bimodal(mu1,mu2,sigma1,sigma2,vikt){\n  const comp = Math.random() &lt; vikt\n  return comp ?  jStat.normal.sample(mu1,sigma1) : jStat.normal.sample(mu2,sigma2)\n}\n\n\n\n\n\n\n\nfunction rand_lognormal(mu, sigma){\n  return jStat.lognormal.sample(mu, sigma)\n}\n\n\n\n\n\n\n\nfunction rand_chi2(df){\n  return jStat.chisquare.sample(df)\n}\n\n\n\n\n\n\n\nfunction rand_studentt(location, scale, df){\n  return location + scale * jStat.studentt.sample(df)\n}\n\n\n\n\n\n\n\n// Returns a gamma deviate by the method of Marsaglia and Tsang.\nfunction randg(shape) {\n  let oalph = shape, a1, a2, u, v, x, mat;\n  if (!shape) shape = 1;\n  if (shape &lt; 1) shape += 1;\n  a1 = shape - 1 / 3;\n  a2 = 1 / Math.sqrt(9 * a1);\n  do {\n    do {\n      x = randn();\n      v = 1 + a2 * x;\n    } while (v &lt;= 0);\n    v = v * v * v;\n    u = Math.random();\n  } while (\n    u &gt; 1 - 0.331 * Math.pow(x, 4) &&\n    Math.log(u) &gt; 0.5 * x * x + a1 * (1 - v + Math.log(v))\n  );\n  if (shape === oalph) return a1 * v; // alpha &gt; 1\n  do u = Math.random(); while (u === 0); // alpha &lt; 1\n  return Math.pow(u, 1 / oalph) * a1 * v;\n}\n\n\n\n\n\n\n\nfunction randb(alpha, beta) {\n  const u = randg(alpha);\n  return u / (u + randg(beta));\n}\n\n\n\n\n\n\n\nfunction binomialSample(n,p){\n  return d3.sum(d3.range(n).map(d =&gt; Math.random()&lt;p))\n}\n\n\n\n\n\n\n\ndata = {\n  const size = 1000;\n  if (dist_type == 'binomial'){\n    const n = params_clt[0];\n    const p = params_clt[1];\n    return d3.range(size).map(d =&gt; ({x: binomialSample(n, p)})) \n  }\n  if (dist_type == 'poisson'){\n    const lambda = params_clt[0];\n    return d3.range(size).map(d =&gt; ({x: jStat.poisson.sample(lambda)}))\n  }\n  if (dist_type == 'beta'){\n    const alpha = params_clt[0];\n    const beta = params_clt[1];\n    return d3.range(size).map(d =&gt; ({x: randb(alpha, beta)}))\n  }\n  if (dist_type == 'cauchy'){\n    const location = params_clt[0];\n    const scale = params_clt[1];\n    const df = 1;\n    return d3.range(size).map(d =&gt; ({x: rand_studentt(location, scale, df)}))\n  }\n  else if (dist_type == 'uniform'){\n    const min = params_clt[0];\n    const max = params_clt[1];\n    return d3.range(size).map(d =&gt; ({x: Math.random() * (max-min) + min}))\n  }\n  else if (dist_type == 'studentt'){\n    const location = params_clt[0];\n    const scale = params_clt[1];\n    const df = params_clt[2];\n    return d3.range(size).map(d =&gt; ({x: rand_studentt(location, scale, df)}))\n  }\n  else if (dist_type == 'chi2'){\n    const df = params_clt[0];\n    return d3.range(size).map(d =&gt; ({x: rand_chi2(df)}))\n  }\n  else if (dist_type == 'lognormal'){\n    const mu = params_clt[0];\n    const sigma = params_clt[1];\n    return d3.range(size).map(d =&gt; ({x: rand_lognormal(mu, sigma)}))\n  }\n  else if (dist_type == 'bimodal'){\n    const mu1 = params_clt[0];\n    const mu2 = params_clt[1];\n    const sigma1 = params_clt[2];\n    const sigma2 = params_clt[3];\n    const vikt = params_clt[4];\n    return d3.range(size).map(d =&gt; ({x: rand_bimodal(mu1, mu2, sigma1, sigma1, vikt)}))\n  }\n  \n}\n\n\n\n\n\n\n\npltdata = {\n  if (discrete){\n    return Plot.plot({\n      caption:html`&lt;span style=\"color:#6C8EBF\"&gt;a single dataset simulated from the data distribution.&lt;/span&gt; `,\n      marks: [\n        Plot.barY(data, Plot.groupX({y: \"count\"}, {x: \"x\", fill: \"colors[0]\"})),\n        Plot.ruleY([0])\n      ]\n    }) \n  }\n  else {\n    \n    return Plot.plot({\n      caption:html`&lt;span style=\"color:#6C8EBF\"&gt;a single dataset simulated from the data distribution.&lt;/span&gt; `,\n      marks: [\n        Plot.rectY(data.filter(d =&gt; Math.abs(d.x) &lt;=50), Plot.binX({y: \"count\"}, {x: \"x\", fill: colors[0]})),\n        Plot.ruleY([0])\n      ]\n    })\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmeans = sample_finite_pop(data, sample_size, nRep)\n\n\n\n\n\n\n\n\n\n\n\npltmeans = Plot.plot({\n  caption:html`&lt;span style=\"color:#bb989a\"&gt;distribution of the sample mean from samples with n = ${sample_size} observations.(https://observablehq.com/@mattiasvillani/central-limit-theorem?collection=@mattiasvillani/probability)&lt;/span&gt; `,\n  x: {\n    label: \"sample means\"\n  },\n  y:{\n    axis: false\n  },\n  marks: [\n    Plot.rectY(means, Plot.binX({y: \"count\"}, {x: \"mean\", fill: colors[8]})),\n    //Plot.areaY(means, Plot.binX({y: \"count\", filter: null}, {x: \"mean\", fillOpacity: 0.2})),\n    //Plot.lineY(means, Plot.binX({y: \"count\", filter: null}, {x: \"mean\"})),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\nimport {colors} from \"@mattiasvillani/statistics-tools\"\n\n\n\n\n\n\n\nfunction sample_finite_pop(data, n, nRep){\n  // Sample nRep samples of size n from the data and return the mean in each sample\n  var samplemeans = [];\n  for (let i = 0; i &lt; nRep; ++i) {\n    samplemeans.push({sample: i+1, mean: d3.mean(ss.sample(data, n).map(d =&gt; d.x))})\n  }\n  return samplemeans;\n}\n\n\n\n\n\n\n\nss = require('simple-statistics')\n\n\n\n\n\n\n\ndiscrete = ([\"poisson\", \"binomial\"].includes(dist_type))\n\n\n\n\n\n\n\nplt = html`&lt;html&gt;\n &lt;head&gt;\n &lt;/head&gt;\n &lt;body&gt;\n    &lt;div class=\"container\" style=\"display: flex; height: 400px;\"&gt;\n        &lt;div style=\"width: 44%;\"&gt;\n            ${pltdata}\n        &lt;/div&gt;\n        &lt;div style=\"width: 2%;\"&gt;\n           \n        &lt;/div&gt;\n        &lt;div style=\"width: 44%;\"&gt;\n            ${pltmeans}\n        &lt;/div&gt;\n &lt;/body&gt;\n&lt;/html&gt;`"
  },
  {
    "objectID": "ojs/data-hist-ojs.html",
    "href": "ojs/data-hist-ojs.html",
    "title": "Number of bins of histogram",
    "section": "",
    "text": "viewof bin_input = Inputs.form([\n  Inputs.select(['manual choice','sturges','scott'], {value: \"manuellt val\", label: \"method for bins:\"}),\n  Inputs.range([2, 25], {value: 10, step: 1, label: \"number of bins\"})])\n\n\n\n\n\n\n\n\nplt = {\n  var plt;\n  if (bin_input[0] == 'manual choice'){\n  plt = Plot.plot({\n    width: 1600, // or a dynamic value based on `width` variable\n    height: 900,\n    caption: 'Histogram of the age distribution on the Titanic',\n    facet: {\n      data: titanic,\n      x: \"Sex\"\n    },\n    marks: [\n      Plot.rectY(titanic, Plot.binX({y: \"count\"}, {x: \"Age\", thresholds: bin_input[1], fill: \"Sex\"})),\n      Plot.ruleY([0])\n    ]\n  })\n  }else{\n  plt = Plot.plot({\n    caption: 'Histogram of the age distribution on the Titanic',\n    facet: {\n      data: titanic,\n      x: \"Sex\"\n    },\n    marks: [\n      Plot.rectY(titanic, Plot.binX({y: \"count\"}, {x: \"Age\", thresholds: bin_input[0], fill: \"Sex\"})),\n      Plot.ruleY([0])\n    ]\n  })\n  }\n  return plt;\n}\n\n\n\n\n\n\n\n\nFigure¬†1: https://observablehq.com/@mattiasvillani/histogram-the-choice-of-bins\n\n\n\n\n\nCh3data = FileAttachment(\"./OnlyTitanic.xlsx\").xlsx(\"Titanic\")\n\n\n\n\n\n\n\ntitanic = Ch3data.sheet(0, { headers: true })"
  },
  {
    "objectID": "ojs/infer-ci-t-ojs.html",
    "href": "ojs/infer-ci-t-ojs.html",
    "title": "Student-t Distribution",
    "section": "",
    "text": "jstat = require('jstat')\n\n\n\n\n\n\n\nnormcdf = jstat.normal.cdf(myinputs[1], 0, 1);\nstudentcdf = jstat.studentt.cdf(myinputs[1], myinputs[0]);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof myinputs = Inputs.form([\n      Inputs.range([1, 100], {value: 4, step: 1, label: tex`\\text{DF }\\nu:`}),\n      Inputs.range([-8, 8], {value: -1.96, step: 0.01, label: \"Quantile:\"})\n    ])\n\n\n\n\n\n\n\ntailprobs = tex`\\text{Normal distribution: } P(X\\leq ${myinputs[1]})=${normcdf.toPrecision(4)} \\\\ \\text{Student-}t \\text{ distribution: } P(X\\leq ${myinputs[1]})=${studentcdf.toPrecision(4)}`\n\n\n\n\n\n\n\ndata_t = {\n  const x = d3.range(-8, 8, 0.01);\n  const normpdf = x.map(x =&gt; ({x: x, pdf: jstat.normal.pdf(x, 0, 1), dist: \"normal\"}));\n  const studentpdf = x.map(x =&gt; ({x: x, pdf: jstat.studentt.pdf(x, myinputs[0]), dist: \"student-t\"}));\n  const data = normpdf.concat(studentpdf)\n  return data\n}\n\n\n\n\n\n\n\nplt_t = Plot.plot({\n    style: {fontSize: \"16px\"},\n    color: {\n      legend: true,\n      style: {fontSize: \"16px\"}\n    },\n    x: {\n      label: \"x\",\n      axis: true\n    },\n    y: {\n      axis: false,\n      domain: [0,0.4]\n    },\n    marks: [\n      Plot.ruleY([0]),\n      Plot.line(data_t, {x: \"x\", y: \"pdf\", stroke : \"dist\", strokeWidth: 2}),\n      Plot.areaY(data_t, {filter: d =&gt; d.x &lt;= myinputs[1] && d.dist == \"normal\", x: \"x\", y: \"pdf\", fill: \"steelblue\", opacity: 0.2}),\n      Plot.areaY(data_t, {filter: d =&gt; d.x &lt;= myinputs[1] && d.dist == \"student-t\", x: \"x\", y: \"pdf\", fill: \"orange\", opacity: 0.2}),\n    ]\n  })\n\n\n\n\n\n\n\n\nFigure¬†1: https://observablehq.com/@mattiasvillani/student-t\n\n\n\n\n\n\n\n\n\n\nviewof myinputs_zoom = Inputs.form([\n      Inputs.range([1, 100], {value: 10, step: 1, label: tex`\\text{DF }\\nu:`}),\n      Inputs.range([-8, -3], {value: -5, step: 0.01, label: \"Quantile:\"})\n    ])\n\n\n\n\n\n\n\ndata_zoom = {\n  const x = d3.range(-10, -3, 0.01);\n  const normpdf = x.map(x =&gt; ({x: x, pdf: jstat.normal.pdf(x, 0, 1), dist: \"normal\"}));\n  const studentpdf = x.map(x =&gt; ({x: x, pdf: jstat.studentt.pdf(x, myinputs_zoom[0]), dist: \"student-t\"}));\n  const data = normpdf.concat(studentpdf)\n  return data\n}\n\n\n\n\n\n\n\nPlot.plot({\n    caption: html`Zoom in the left tail`,\n    style: {fontSize: \"16px\"},\n    color: {\n      legend: true,\n      style: {fontSize: \"16px\"}\n    },\n    x: {\n      label: \"x\",\n      axis: true,\n      ticks: [-8,-7,-6,-5,-4,-3, myinputs_zoom[1]],\n      domain: [-8,-3]\n    },\n    y: {\n      axis: true,\n      domain: [0,0.01]\n    },\n    marks: [\n      Plot.ruleY([0]),     \n      Plot.ruleX([-10]), \n      Plot.ruleX([myinputs_zoom[1]], {stroke: \"gray\", strokeOpacity: 0.4}),\n      Plot.line(data_zoom, {filter: d =&gt; d.x &lt;= -3, x: \"x\", y: \"pdf\", stroke : \"dist\", strokeWidth: 2}),\n      //Plot.areaY(data, {filter: d =&gt; d.x &lt;= myinputs[1] && d.dist == \"normal\", x: \"x\", y: \"pdf\", fill: \"steelblue\", opacity: 0.2}),\n      //Plot.areaY(data, {filter: d =&gt; d.x &lt;= myinputs[1] && d.dist == \"student-t\", x: \"x\", y: \"pdf\", fill: \"orange\", opacity: 0.2}),\n    ]\n  })\n\n\n\n\n\n\n\n\nFigure¬†2: https://observablehq.com/@mattiasvillani/student-t"
  },
  {
    "objectID": "ojs/infer-ht-ojs.html",
    "href": "ojs/infer-ht-ojs.html",
    "title": "Hypothesis Testing for Population Mean",
    "section": "",
    "text": "hypotext = \n{\n  var hyposign;\n  if (alternative == 'equal to'){\n    hyposign = \"\\\\neq\"\n  }else if (alternative == 'less than'){\n    hyposign = \"&lt;\"\n  }else{\n    hyposign = \"&gt;\"\n  }\n   return tex`\\textbf{Hypotheses }\\\\\n              H_0: \\mu = \\mu_0 = ${mu0}\\\\\n              H_A: \\mu ${hyposign} \\mu_0 = ${mu0}`\n}\n\n\n\n\n\n\n\ntobstext = \n{\n   return tex`\\textbf{Test statistic }\\\\\n    t_{obs} = \\frac{\\bar x -\\mu_0}{\\frac{s}{\\sqrt{n}}} = \\frac{${xbar.toFixed(3)} - ${mu0.toFixed(3)}}{\\frac{${s.toFixed(3)}}{\\sqrt{${n}}}} = ${tobs.toFixed(3)}`\n}\n\n\n\n\n\n\n\ntcrittext = \n{\n   return tex`\\textbf{Critical value }\\\\\nt_{crit} = t_{${alphaside}, \\,${n-1}} = ${tcrit.toPrecision(4)}`\n}\n\n\n\n\n\n\n\npvaluetext = {\n  var rejecttext;\n  var sign;\n  if (pvaluestudentt &lt; alpha){\n    rejecttext = \" \\\\textbf{we reject} \";\n    sign = \"&lt;\";\n  }else{\n    rejecttext = \"we \\\\textbf{do not reject} \";\n    sign = \"\\\\geq\";\n  }\n   return tex`\\textbf{p-value}\\\\\np\\text{-value}=${pvaluestudentt.toPrecision(4)}`\n}\n\n\n\n\n\n\n\npconclusion = {\n  var rejecttext;\n  var sign;\n  if (pvaluestudentt &lt; alpha){\n    rejecttext = \"We\\\\textcolor{red}{ \\\\textbf{reject}}\\ \";\n    sign = \"&lt;\";\n  }else{\n    rejecttext = \"We\\\\textcolor{steelblue}{ \\\\textbf{do not reject}} \";\n    sign = \"\\\\geq\";\n  }\n   return tex`\\text{${rejecttext}} H_0 \\text{ at } ${(alpha*100).toFixed(1)}\\% \\text{ significance level.}  `\n}\n\n\n\n\n\n\n\nviewof inputs = Inputs.form([\n      Inputs.range([2, 1000], {value: 5, step: 1, label: tex`\\text{sample }n`}), \n      Inputs.range([-10, 10], {value: 1.7, step: 0.01, label: tex`\\text{sample } \\bar x`}),\n      Inputs.range([Number.EPSILON, 10], {value: 0.5, step: 0.1, label: tex`\\text{sample }s`}),\n      Inputs.range([0.001, 1], {value: 0.05, step: 0.001, label: tex`\\text{sig. lvl }\\alpha`}),\n      Inputs.range([-10, 10], {value: 1, step: 0.1, label: tex`\\text{null }\\mu_0`}),\n      Inputs.select([\"equal to\",\"greater than\", \"less than\"], {label: tex`\\text{alternative}`})\n    ])\n\n\n\n\n\n\n\nviewof toggles = Inputs.form([\n  Inputs.toggle({value: true, label: \"show critical regions\"}),\n  Inputs.toggle({value: false, label: \"show p-values\"})\n  ])\n\n\n\n\n\n\n\njstat = require('jstat')\n\n\n\n\n\n\n\ntextfontsize = 14\n\n\n\n\n\n\n\nshowcrit = toggles[0]\n\n\n\n\n\n\n\nshowpvalue = toggles[1]\n\n\n\n\n\n\n\nn = inputs[0]\nxbar = inputs[1]\ns = inputs[2]\nalpha = inputs[3]\nmu0 = inputs[4]\nalternative = inputs[5]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nalphaside = {\n  if (alternative == 'equal to'){\n    return alpha/2\n  }else{\n    return alpha\n  }\n}\n\n\n\n\n\n\n\ntobs = (xbar-mu0)/(s/Math.sqrt(n))\n\n\n\n\n\n\n\ntcrit = jstat.studentt.inv(1-alphaside, n-1)\n\n\n\n\n\n\n\nzcrit = jstat.normal.inv(1-alphaside, 0, 1)\n\n\n\n\n\n\n\npvaluestudentt = {\n  if (alternative == 'equal to'){\n      return 2*jstat.studentt.cdf(-Math.abs(tobs), n-1)\n  }else if(alternative == 'less than'){\n    return jstat.studentt.cdf(tobs, n-1)\n  }else{\n    return 1-jstat.studentt.cdf(tobs, n-1)\n  }\n}\n\n\n\n\n\n\n\npdfdata = {\n  const x = d3.range(-10, 10, 0.01);\n  var data = x.map(x =&gt; ({x: x, pdf: jstat.normal.pdf(x, 0, 1), dist: \"normal\", type: \"pdf\"}));\n  const studentpdf = x.map(x =&gt; ({x: x, pdf: jstat.studentt.pdf(x, n-1), dist: \"student-t\", type: \"pdf\"}));\n  data = data.concat(studentpdf)\n  data = data.concat(tcritdata)\n   data = data.concat(tobsdata)\n  return data\n}\n\n\n\n\n\n\n\n\nFigure¬†1: Source: https://observablehq.com/@mattiasvillani/hypothesis-test-mean\n\n\n\n\n\nmax_y = 1.05*jstat.normal.pdf(0, 0, 1)\n\n\n\n\n\n\n\ntwosided = alternative == 'equal to'\n\n\n\n\n\n\n\nside = {\n  if (alternative == 'less than'){\n    return \"left\"\n  }else if(alternative == 'greater than'){\n    return \"right\"\n  }else{\n    return \"both\"\n  }\n}\n\n\n\n\n\n\n\ntcritdata = {\n  if (twosided){\n    return [{x: tcrit, y: max_y/4, dist: \"student-t\", type: \"crit\"}, {x: -tcrit, y: max_y/4, dist: \"student-t\", type: \"crit\"}]\n  }else if (alternative == 'less than'){\n    return [{x: -Math.abs(tcrit), y: max_y/4, dist: \"student-t\", type: \"crit\"}]\n  }else\n  {\n    return [{x: Math.abs(tcrit), y: max_y/4, dist: \"student-t\", type: \"crit\"}]\n  }\n}\n\n\n\n\n\n\n\ntobsdata = {\n  if (twosided){\n    return [{x: tobs, y: 0, dist: \"student-t\", type: \"obs\"}, {x: -tobs, y: 0, dist: \"student-t\", type: \"obs\"}]\n  }else if (alternative == 'less than'){\n    return [{x: -Math.abs(tobs), y: 0, dist: \"student-t\", type: \"obs\"}]\n  }else\n  {\n    return [{x: Math.abs(tobs), y: 0, dist: \"student-t\", type: \"obs\"}]\n  }\n}\n\n\n\n\n\n\n\nconclusioncolor = {\n  if(pvaluestudentt &lt; alpha){\n    return \"red\"\n  }else{\n    return \"steelblue\"\n  }\n}\n\n\n\n\n\n\n\nnudge = 1.1\nbignudge = 1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt = Plot.plot({\n    width: 800, // or a dynamic value based on `width` variable\n    height: 600,\n    style: {fontSize: \"16px\"},\n    color: {\n      legend: false\n    },\n    x: {\n      label: \"t-statistic\",\n      axis: true,\n      domain: [-10,10]\n    },\n    y: {\n      axis: false,\n      domain: [0,max_y]\n    },\n    marks: [\n      Plot.ruleY([0]),\n      Plot.line(pdfdata, {filter: d =&gt; d.dist == \"student-t\" && d.type == \"pdf\", x: \"x\", y: \"pdf\", stroke : \"dist\", strokeWidth: 2}),\n      Plot.areaY(pdfdata, {filter: d =&gt; showpvalue && side == \"left\" && d.x &lt;= tobs && \n          d.dist == \"student-t\" && d.type == \"pdf\", x: \"x\", y: \"pdf\", fill: \"orange\", opacity: 0.5}),\n      Plot.areaY(pdfdata, {filter: d =&gt; showpvalue && side == \"right\" && d.x &gt;= tobs && \n        d.dist == \"student-t\" && d.type == \"pdf\", x: \"x\", y: \"pdf\", fill: \"orange\", opacity: 0.5}),\n      Plot.areaY(pdfdata, {filter: d =&gt; showpvalue && side == \"both\" && d.x &lt;= -Math.abs(tobs) && \n          d.dist == \"student-t\" && d.type == \"pdf\", x: \"x\", y: \"pdf\", fill: \"orange\", opacity: 0.5}),\n      Plot.areaY(pdfdata, {filter: d =&gt; showpvalue && side == \"both\" && d.x &gt;= Math.abs(tobs) && \n        d.dist == \"student-t\" && d.type == \"pdf\", x: \"x\", y: \"pdf\", fill: \"orange\", opacity: 0.5}),\n      \n      Plot.dot([{x: tobs, y: 0}], {x: \"x\", y: \"y\", fill: conclusioncolor, symbol: \"circle\", r: 5}),\n      Plot.arrow([{x1: tobs, y1: max_y/12, x2:tobs, y2: 0.01}], {x1: \"x1\", y1: \"y1\", x2: \"x2\", y2: \"y2\", stroke: conclusioncolor}),\n      Plot.text([{x1: tobs, y1: bignudge*max_y/12, texter: \"t(obs)\"}], {x: \"x1\", y: \"y1\", text: \"texter\", fontSize: textfontsize}),\n      \n      Plot.ruleX([{x: tcrit, y: max_y/4}], {filter: d =&gt; showcrit && side == \"both\", x: \"x\", y: \"y\", fill: \"green\"}),\n      Plot.ruleX([{x: -tcrit, y: max_y/4}], {filter: d =&gt; showcrit && side == \"both\", x: \"x\", y: \"y\", fill: \"green\"}),\n      Plot.text([{x1: tcrit, y1: nudge*max_y/4, texter: \"t(crit)\"}], \n                {filter: d =&gt; showcrit && side == \"both\",x: \"x1\", y: \"y1\", text: \"texter\", fontSize: textfontsize}),\n      Plot.text([{x1: -tcrit, y1: nudge*max_y/4, texter: \"-t(crit)\"}], \n                {filter: d =&gt; showcrit && side == \"both\",x: \"x1\", y: \"y1\", text: \"texter\", fontSize: textfontsize}),\n\n      Plot.ruleX([{x: tcrit, y: max_y/4}], {filter: d =&gt; showcrit && side == \"right\", x: \"x\", y: \"y\", fill: \"green\"}),\n      Plot.text([{x1: tcrit, y1: nudge*max_y/4, texter: \"t(crit)\"}], \n                {filter: d =&gt; showcrit && side == \"right\",x: \"x1\", y: \"y1\", text: \"texter\", fontSize: textfontsize}),\n      \n      Plot.ruleX([{x: -tcrit, y: max_y/4}], {filter: d =&gt; showcrit && side == \"left\", x: \"x\", y: \"y\", fill: \"green\"}),\n      Plot.text([{x1: -tcrit, y1: nudge*max_y/4, texter: \"-t(crit)\"}], \n                {filter: d =&gt; showcrit && side == \"left\",x: \"x1\", y: \"y1\", text: \"texter\", fontSize: textfontsize}),\n\n      Plot.arrow([{x1: tcrit*bignudge, y1: max_y/6, x2:10, y2: max_y/6}], {filter: d =&gt; showcrit && side == \"right\", \n                                                                  x1: \"x1\", y1: \"y1\", x2: \"x2\", y2: \"y2\", stroke: \"red\"}),\n      Plot.arrow([{x1: -tcrit*bignudge, y1: max_y/6, x2:-10, y2: max_y/6}], {filter: d =&gt; showcrit && side == \"left\", \n                                                                    x1: \"x1\", y1: \"y1\", x2: \"x2\", y2: \"y2\", stroke: \"red\"}),\n      Plot.arrow([{x1: tcrit*bignudge, y1: max_y/6, x2:10, y2: max_y/6}], {filter: d =&gt; showcrit && side == \"both\", \n                                                                  x1: \"x1\", y1: \"y1\", x2: \"x2\", y2: \"y2\", stroke: \"red\"}),\n      Plot.arrow([{x1: -tcrit*bignudge, y1: max_y/6, x2:-10, y2: max_y/6}], {filter: d =&gt; showcrit && side == \"both\", \n                                                                    x1: \"x1\", y1: \"y1\", x2: \"x2\", y2: \"y2\", stroke: \"red\"}),\n      Plot.text([{x1: (-10 - tcrit*bignudge)/2, y1: max_y/4, texter: \"Reject H‚ÇÄ\"}], \n                {filter: d =&gt; showcrit && side == \"left\", x: \"x1\", y: \"y1\", text: \"texter\", fontSize: textfontsize}),\n      Plot.text([{x1: (10 + tcrit*bignudge)/2, y1: max_y/4, texter: \"Reject H‚ÇÄ\"}], \n                {filter: d =&gt; showcrit && side == \"right\", x: \"x1\", y: \"y1\", text: \"texter\", fontSize: textfontsize}),\n      Plot.text([{x1: (-10 - tcrit*bignudge)/2, y1: max_y/4, texter: \"Reject H‚ÇÄ\"}], \n                {filter: d =&gt; showcrit && side == \"both\", x: \"x1\", y: \"y1\", text: \"texter\", fontSize: textfontsize}),\n      Plot.text([{x1: (10 + tcrit*bignudge)/2, y1: max_y/4, texter: \"Reject H‚ÇÄ\"}], \n                {filter: d =&gt; showcrit && side == \"both\", x: \"x1\", y: \"y1\", text: \"texter\", fontSize: textfontsize})\n    ]\n  })\n\n\n\n\n\n\n\n\nFigure¬†2: https://observablehq.com/@mattiasvillani/hypothesis-test-mean?collection=@mattiasvillani/statistics"
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "MATH 4720/5720 Fall 2025",
    "section": "",
    "text": "This course introduces the ideas of data summary and visualization, probability, and statistics with applications to natural and social sciences and daily life. Topics include but not limited to: random variables, discrete and continuous probability distributions, sampling distributions, confidence intervals, hypothesis testing, analysis of variance, and linear regression. This course builds a foundation for statistical inference, machine learning, and data modeling.\nThe course will assume facility with using the internet and a personal computer. A portion of the course involves  programming using RStudio or Posit Cloud, but prior coding experience is not required.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "slides/13-anova.html#comparing-more-than-two-population-means",
    "href": "slides/13-anova.html#comparing-more-than-two-population-means",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Comparing More Than Two Population Means",
    "text": "Comparing More Than Two Population Means\nIn many research settings, we‚Äôd like to compare 3 or more population means.\n\n\n\n 4 types of devices used to determine the pH of soil samples. \n Determine whether there are differences in the mean readings of those 4 devices. \n\n\n\n\n\n\n\n\n\n\n\n\n Do different treatments (None, Fertilizer, Irrigation, Fertilizer and Irrigation) affect the mean weights of poplar trees? \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Chapter 6 we learned methods for comparing 2 population means."
  },
  {
    "objectID": "slides/13-anova.html#one-way-analysis-of-variance",
    "href": "slides/13-anova.html#one-way-analysis-of-variance",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "One-Way Analysis of Variance",
    "text": "One-Way Analysis of Variance\n\nA factor is a property or characteristic (categorical variable) that allows us to distinguish the different populations from one another.\nType of devices and treatment of trees are factors.\nOne-way ANOVA examines the effect of a categorical variable on the mean of a numerical variable (response).\nWe use analysis of  variance  to test the equality of 3 or more population  means. ü§î\nThe method is one-way because we use one single property (categorical variable) for categorizing the populations.\n\n\n\nA treatment (or factor) can be a category or level of a categorical variable."
  },
  {
    "objectID": "slides/13-anova.html#requirements-of-one-way-anova",
    "href": "slides/13-anova.html#requirements-of-one-way-anova",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Requirements of One-Way ANOVA",
    "text": "Requirements of One-Way ANOVA\n\nThe populations of each category are normally distributed.\nThe populations have the same variance \\(\\sigma^2\\) (two-sample pooled \\(t\\)-test).\nThe samples are random samples.\nThe samples are independent of each other. (not matched or paired in any way)\n\n\n\nThe method can still work fairly well when the variances are not equal, unless they are differ by large amounts."
  },
  {
    "objectID": "slides/13-anova.html#rationale-for-anova",
    "href": "slides/13-anova.html#rationale-for-anova",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Rationale for ANOVA",
    "text": "Rationale for ANOVA\nData 1 and Data 2 have the same group sample means \\(\\bar{y}_1\\), \\(\\bar{y}_2\\) and \\(\\bar{y}_3\\) denoted as red dots.\n\n\n\n\n\n\n\n\n\nWhich data you are more confident to say the population means \\(\\mu_1\\), \\(\\mu_2\\) and \\(\\mu_3\\) are not all the same?\n\n\n\nThe boxplots clearly show this difference.\nThe 2 data sets have the same group means, but the variance within groups for data 2 is much greater than the variance within groups for data 1.\nThe difference in sample means in Data 1 is more likely due to the true difference in population means.\nvariation between samples is measured by the pairwise distance among the sample means.\nvariation within samples is measured by the how far away the data points away from each other in each sample group."
  },
  {
    "objectID": "slides/13-anova.html#variation-between-samples-variation-within-samples",
    "href": "slides/13-anova.html#variation-between-samples-variation-within-samples",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Variation Between Samples & Variation Within Samples",
    "text": "Variation Between Samples & Variation Within Samples\n\nData 1: Variability between samples is large in comparison to the variation within samples.\nData 2: Variation between samples is small relatively to the variation within samples.\n\n\nMore confident to conclude there is a difference in population means when variation between samples is relatively larger than variation within samples.\n\n\n\n\n\n\n\n\n\n\n=&gt; population means are different.\n=&gt; less likely and confident to conclude there is a difference in population means.\n\nWhen variation within samples relatively larger than the variation between samples, it‚Äôs more difficult to distinguish or disentangle one sample from another."
  },
  {
    "objectID": "slides/13-anova.html#anova-table",
    "href": "slides/13-anova.html#anova-table",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "ANOVA Table",
    "text": "ANOVA Table"
  },
  {
    "objectID": "slides/13-anova.html#anova-data",
    "href": "slides/13-anova.html#anova-data",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "ANOVA Data",
    "text": "ANOVA Data\n\nThere are 5 populations.\nWithin each population, 4 data points are collected.\nData \\(y_{ij}\\) is the the \\(j\\)-th data point in the \\(i\\)-th group.\n\n\n\n\n\n\n\n\n\nPopulation\nData\nSample Mean\nPopulation Mean\n\n\n\n1\n\n\\(y_{11}\\) \\(\\quad\\) \\(y_{12}\\) \\(\\quad\\) \\(y_{13}\\) \\(\\quad\\) \\(y_{14}\\)\n\n\\(\\bar{y}_{1}\\)\n\\(\\mu_{1}\\)\n\n\n2\n\n\\(y_{21}\\) \\(\\quad\\) \\(y_{22}\\) \\(\\quad\\) \\(y_{23}\\) \\(\\quad\\) \\(y_{24}\\)\n\n\\(\\bar{y}_{2}\\)\n\\(\\mu_{2}\\)\n\n\n3\n\n\\(y_{31}\\) \\(\\quad\\) \\(y_{32}\\) \\(\\quad\\) \\(y_{33}\\) \\(\\quad\\) \\(y_{34}\\)\n\n\\(\\bar{y}_{3}\\)\n\\(\\mu_{3}\\)\n\n\n4\n\n\\(y_{41}\\) \\(\\quad\\) \\(y_{42}\\) \\(\\quad\\) \\(y_{43}\\) \\(\\quad\\) \\(y_{44}\\)\n\n\\(\\bar{y}_{4}\\)\n\\(\\mu_{4}\\)\n\n\n5\n\n\\(y_{51}\\) \\(\\quad\\) \\(y_{52}\\) \\(\\quad\\) \\(y_{53}\\) \\(\\quad\\) \\(y_{54}\\)\n\n\\(\\bar{y}_{5}\\)\n\\(\\mu_{5}\\)\n\n\n\n. . .\n\nThis is NOT a tidy data matrix. We may need to save the data in another format before we do ANOVA."
  },
  {
    "objectID": "slides/13-anova.html#procedure-of-anova",
    "href": "slides/13-anova.html#procedure-of-anova",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Procedure of ANOVA",
    "text": "Procedure of ANOVA\n\n \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\\\  &H_1: \\text{Population means are not all equal} \\end{align}\\) \n\n\nStatistician Ronald Fisher found a way to define a variable that follows the \\(F\\) distribution:\n\n\\[\\frac{\\text{variance between samples}}{\\text{variance within samples}} \\sim F_{df_B,\\, df_W}\\]\n\nIf variance between samples is larger than variance within samples, i.e., \\(F_{test}\\) is much greater than 1, as Data 1, we reject \\(H_0\\).\n\n\nKey: Define variance between samples and variance within samples so that the ratio is \\(F\\) distributed.\n\n\nHow do we convert the rationale into a formal testing procedure for the test"
  },
  {
    "objectID": "slides/13-anova.html#variance-within-samples",
    "href": "slides/13-anova.html#variance-within-samples",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Variance Within Samples",
    "text": "Variance Within Samples\n\nBack to two-sample pooled \\(t\\)-test with equal variance \\(\\sigma^2\\):\n\n\\[s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}\\]\n\nWhat if general \\(k\\) samples?\n\n. . .\n\nANOVA assumes the populations have the same variance \\(\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_k^2 = \\sigma^2\\).\n\n\\[\\boxed{s_W^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2 + \\cdots + (n_k-1)s_k^2}{n_1 + n_2 + \\cdots + n_k - k}}\\] where \\(s_i^2\\), \\(i = 1, \\dots ,k\\), is the sample variance of group \\(i\\).\n\n\n\\(s_W^2\\) represents a combined estimate of the common variance \\(\\sigma^2\\). It measures variability of the observations within the \\(k\\) populations.\n\n\n\\(s_i^2 = \\frac{\\sum_{j=1}^{n_i}\\left(y_{ij} - \\bar{y}_{i\\cdot}\\right)^2}{n_i-1}\\)"
  },
  {
    "objectID": "slides/13-anova.html#variance-between-samples",
    "href": "slides/13-anova.html#variance-between-samples",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Variance Between Samples",
    "text": "Variance Between Samples\n\\[\\boxed{s^2_{B} = \\frac{\\sum_{i=1}^k n_i (\\bar{y}_{i} - \\bar{y}_{})^2}{k-1}}\\]\n\n\\(\\bar{y}_{i}\\) is the \\(i\\)-th sample mean.\n\\(\\bar{y}_{}\\) is the grand sample mean with all data points in all groups combined.\n\n. . .\n\n\\(s^2_{B}\\) is also an estimate of \\(\\sigma^2\\) and measures variability among sample means for the \\(k\\) groups.\n\nIf \\(H_0\\) is true \\((\\mu_1 = \\cdots = \\mu_k = \\mu)\\), any variation in the sample means is due to chance and randomness, and shouldn‚Äôt be too large.\n\n\n\\(\\bar{y}_{1}, \\cdots, \\bar{y}_{k}\\) should be close each other, and they are close to \\(\\bar{y}_{}\\)."
  },
  {
    "objectID": "slides/13-anova.html#s_b2-and-s_w2-as-sum-of-squaresdegrees-of-freedom",
    "href": "slides/13-anova.html#s_b2-and-s_w2-as-sum-of-squaresdegrees-of-freedom",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "\n\\(s_B^2\\) and \\(s_W^2\\) as Sum of Squares/Degrees of Freedom",
    "text": "\\(s_B^2\\) and \\(s_W^2\\) as Sum of Squares/Degrees of Freedom\n\nVariance is defined as \\(\\frac{\\text{Sum of Squares}}{\\text{Degrees of Freedom}}\\), which is also called \\(\\text{Mean Square (MS)}\\)\n\\(s_B^2 = \\frac{\\sum_{i=1}^k n_i (\\bar{y}_{i} - \\bar{y}_{})^2}{k-1} = \\frac{\\text{Sum of Squares Between Samples (SSB)}}{df_B} = MSB\\)\n\\(s_W^2 = \\frac{\\sum_{i=1}^{k} (n_i - 1)s_i^2}{n_1 + n_2 + \\cdots + n_k - k} = \\frac{\\text{Sum of Squares Within Samples (SSW)}}{df_W} = MSW\\) \\((N = n_1 + \\cdots + n_k)\\)"
  },
  {
    "objectID": "slides/13-anova.html#sum-of-squares-identity",
    "href": "slides/13-anova.html#sum-of-squares-identity",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Sum of Squares Identity",
    "text": "Sum of Squares Identity\n\\[\\text{Total Sum of Squares (SST)} = \\sum_{j=1}^{n_i}\\sum_{i=1}^{k} \\left(y_{ij} - \\bar{y}_{}\\right)^2 = SSB + SSW\\]\n\\[df_{T} = df_{B} + df_{W} \\implies N - 1 = (k-1) + (N - k)\\]"
  },
  {
    "objectID": "slides/13-anova.html#section-1",
    "href": "slides/13-anova.html#section-1",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "",
    "text": "Total Sum of Squares (SST) measures total variation around \\(\\bar{y}_{}\\) in all of the sample data combined (ignoring the groups):\n\n\\[\\scriptsize{\\color{blue}{SST = \\sum_{j=1}^{n_i}\\sum_{i=1}^{k} \\left(y_{ij} - \\bar{y}_{}\\right)^2}}\\]\n\n\n\n\nSum of Squares Between Samples (SSB) measures the variation between sample means:\n\n\\[\\scriptsize{ \\color{blue}{SSB = \\sum_{i=1}^{k}n_i \\left(\\bar{y}_{i} - \\bar{y}_{}\\right)^2}}\\] \n\n\n\nSum of Squares Within Samples (SSW) measures the variation of an value \\(y_{ij}\\) about its sample mean \\(\\bar{y}_{i}\\):\n\n\\[\\scriptsize{ \\color{blue}{SSW = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} \\left(y_{ij} - \\bar{y}_{i}\\right)^2 = \\sum_{i=1}^{k} (n_i - 1)s_i^2}}\\]\n\n\n\nSST: sum of squared deviation from each observation to the overall grand mean\nSSB: sum of squared deviation from each group mean to the overall grand mean\nWhat happens when all group means are equal?\nSSW: sum of squared deviation from each group value to its group mean"
  },
  {
    "objectID": "slides/13-anova.html#sum-of-squares-identity-1",
    "href": "slides/13-anova.html#sum-of-squares-identity-1",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Sum of Squares Identity",
    "text": "Sum of Squares Identity\n\n\n\n\\(SST = SSB + SSW\\)\n\\(df_{T} = df_{B} + df_{W} \\implies N - 1 = (k-1) + (N - k)\\) \\((N = n_1 + \\cdots + n_k)\\)"
  },
  {
    "objectID": "slides/13-anova.html#sum-of-squares-identity-2",
    "href": "slides/13-anova.html#sum-of-squares-identity-2",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Sum of Squares Identity",
    "text": "Sum of Squares Identity\n\n\\(\\text{Mean Square (MS)} = \\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\)\n\\(MSB = \\frac{SSB}{k-1} = s^2_{B}\\)\n\\(MSW = \\frac{SSW}{N-k} = s^2_{W}\\)"
  },
  {
    "objectID": "slides/13-anova.html#sum-of-squares-identity-3",
    "href": "slides/13-anova.html#sum-of-squares-identity-3",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Sum of Squares Identity",
    "text": "Sum of Squares Identity\n\n\\(F_{test} = \\frac{MSB}{MSW}\\)\nUnder \\(H_0\\), \\(\\frac{S^2_{B}}{S_W^2} \\sim F_{k-1, \\, N-k}\\)\n\nReject \\(H_0\\) if\n\n\\(F_{test} &gt; F_{\\alpha, \\, k - 1,\\, N-k}\\)\n\n\\(p\\)-value \\(P(F_{k - 1,\\, N-k} &gt; F_{test}) &lt; \\alpha\\)"
  },
  {
    "objectID": "slides/13-anova.html#example",
    "href": "slides/13-anova.html#example",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Example",
    "text": "Example\n\nA hypothesis is that a nutrient ‚ÄúIsoflavones‚Äù varies among three types of food: (1) cereals and snacks, (2) energy bars, and (3) veggie burgers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA sample of 5 each is taken and the amount of isoflavones is measured.\nIs there a sufficient evidence to conclude that the mean isoflavone levels vary among these food items? \\(\\alpha = 0.05\\).\n\n\nAsk for \\(H_0\\)"
  },
  {
    "objectID": "slides/13-anova.html#example---data",
    "href": "slides/13-anova.html#example---data",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Example - Data",
    "text": "Example - Data\n\n\n\n\n\n\ndata\n\n   1  2  3\n1  3 19 25\n2 17 10 15\n3 12  9 12\n4 10  7  9\n5  4  5  8\n\n\nHere columns represent food items and rows are samples.\n\nSo tell me what is the value of \\(y_{23}\\)!\n\n\n\nWe prefer data format like\n\ndata_anova\n\n    y    food\n1   3 cereals\n2  17 cereals\n3  12 cereals\n4  10 cereals\n5   4 cereals\n6  19  energy\n7  10  energy\n8   9  energy\n9   7  energy\n10  5  energy\n11 25  veggie\n12 15  veggie\n13 12  veggie\n14  9  veggie\n15  8  veggie\n\n\n\n\n\n\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n# A tibble: 15 √ó 2\n   food        y\n   &lt;chr&gt;   &lt;dbl&gt;\n 1 cereals     3\n 2 cereals    17\n 3 cereals    12\n 4 cereals    10\n 5 cereals     4\n 6 energy     19\n 7 energy     10\n 8 energy      9\n 9 energy      7\n10 energy      5\n11 veggie     25\n12 veggie     15\n13 veggie     12\n14 veggie      9\n15 veggie      8"
  },
  {
    "objectID": "slides/13-anova.html#example---boxplot",
    "href": "slides/13-anova.html#example---boxplot",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Example - Boxplot",
    "text": "Example - Boxplot"
  },
  {
    "objectID": "slides/13-anova.html#example---test-assumptions",
    "href": "slides/13-anova.html#example---test-assumptions",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Example - Test Assumptions",
    "text": "Example - Test Assumptions\n\nAssumptions:\n\n\n\\(\\sigma_1 = \\sigma_2 = \\sigma_3\\) (I tested it)\nData are generated from a normal distribution for each type of food."
  },
  {
    "objectID": "slides/13-anova.html#example---anova-testing",
    "href": "slides/13-anova.html#example---anova-testing",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Example - ANOVA Testing",
    "text": "Example - ANOVA Testing\n\n\n\n\n\n \\(\\begin{align}&H_0: \\mu_1 = \\mu_2 = \\mu_3\\\\&H_1: \\mu_is \\text{ not all equal} \\end{align}\\) \n\n. . .\n\nüòé Do all calculations and generate an ANOVA table using just one line of code! ü§ü ‚úåÔ∏è"
  },
  {
    "objectID": "slides/13-anova.html#example---anova-table",
    "href": "slides/13-anova.html#example---anova-table",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Example - ANOVA Table",
    "text": "Example - ANOVA Table\n\nanova(lm(y ~ food, data = data_anova))\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nfood       2   60.4   30.20   0.828   0.46\nResiduals 12  437.6   36.47               \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    One-way analysis of means\n\ndata:  y and food\nF = 0.8282, num df = 2, denom df = 12, p-value = 0.46"
  },
  {
    "objectID": "slides/13-anova-slides.html#comparing-more-than-two-population-means",
    "href": "slides/13-anova-slides.html#comparing-more-than-two-population-means",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Comparing More Than Two Population Means",
    "text": "Comparing More Than Two Population Means\nIn many research settings, we‚Äôd like to compare 3 or more population means.\n\n\n\n 4 types of devices used to determine the pH of soil samples. \n Determine whether there are differences in the mean readings of those 4 devices. \n\n\n\n\n\n\n\n\n\n\n\n\n Do different treatments (None, Fertilizer, Irrigation, Fertilizer and Irrigation) affect the mean weights of poplar trees? \n\n\n\n\n\n\n\n\n\n\n\n\nIn Chapter 6 we learned methods for comparing 2 population means."
  },
  {
    "objectID": "slides/13-anova-slides.html#one-way-analysis-of-variance",
    "href": "slides/13-anova-slides.html#one-way-analysis-of-variance",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "One-Way Analysis of Variance",
    "text": "One-Way Analysis of Variance\n\nA factor is a property or characteristic (categorical variable) that allows us to distinguish the different populations from one another.\nType of devices and treatment of trees are factors.\nOne-way ANOVA examines the effect of a categorical variable on the mean of a numerical variable (response).\nWe use analysis of  variance  to test the equality of 3 or more population  means. ü§î\nThe method is one-way because we use one single property (categorical variable) for categorizing the populations.\n\n\nA treatment (or factor) can be a category or level of a categorical variable."
  },
  {
    "objectID": "slides/13-anova-slides.html#requirements-of-one-way-anova",
    "href": "slides/13-anova-slides.html#requirements-of-one-way-anova",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Requirements of One-Way ANOVA",
    "text": "Requirements of One-Way ANOVA\n\nThe populations of each category are normally distributed.\nThe populations have the same variance \\(\\sigma^2\\) (two-sample pooled \\(t\\)-test).\nThe samples are random samples.\nThe samples are independent of each other. (not matched or paired in any way)\n\n\nThe method can still work fairly well when the variances are not equal, unless they are differ by large amounts."
  },
  {
    "objectID": "slides/13-anova-slides.html#rationale-for-anova",
    "href": "slides/13-anova-slides.html#rationale-for-anova",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Rationale for ANOVA",
    "text": "Rationale for ANOVA\nData 1 and Data 2 have the same group sample means \\(\\bar{y}_1\\), \\(\\bar{y}_2\\) and \\(\\bar{y}_3\\) denoted as red dots.\n\n\nWhich data you are more confident to say the population means \\(\\mu_1\\), \\(\\mu_2\\) and \\(\\mu_3\\) are not all the same?\n\n\nThe boxplots clearly show this difference.\nThe 2 data sets have the same group means, but the variance within groups for data 2 is much greater than the variance within groups for data 1.\nThe difference in sample means in Data 1 is more likely due to the true difference in population means.\nvariation between samples is measured by the pairwise distance among the sample means.\nvariation within samples is measured by the how far away the data points away from each other in each sample group."
  },
  {
    "objectID": "slides/13-anova-slides.html#variation-between-samples-variation-within-samples",
    "href": "slides/13-anova-slides.html#variation-between-samples-variation-within-samples",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Variation Between Samples & Variation Within Samples",
    "text": "Variation Between Samples & Variation Within Samples\n\nData 1: Variability between samples is large in comparison to the variation within samples.\nData 2: Variation between samples is small relatively to the variation within samples.\n\n\nMore confident to conclude there is a difference in population means when variation between samples is relatively larger than variation within samples.\n\n\n=&gt; population means are different.\n=&gt; less likely and confident to conclude there is a difference in population means.\n\nWhen variation within samples relatively larger than the variation between samples, it‚Äôs more difficult to distinguish or disentangle one sample from another."
  },
  {
    "objectID": "slides/13-anova-slides.html#anova-table",
    "href": "slides/13-anova-slides.html#anova-table",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "ANOVA Table",
    "text": "ANOVA Table"
  },
  {
    "objectID": "slides/13-anova-slides.html#anova-data",
    "href": "slides/13-anova-slides.html#anova-data",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "ANOVA Data",
    "text": "ANOVA Data\n\nThere are 5 populations.\nWithin each population, 4 data points are collected.\nData \\(y_{ij}\\) is the the \\(j\\)-th data point in the \\(i\\)-th group.\n\n\n\n\n\n\n\n\n\nPopulation\nData\nSample Mean\nPopulation Mean\n\n\n\n1\n\n\\(y_{11}\\) \\(\\quad\\) \\(y_{12}\\) \\(\\quad\\) \\(y_{13}\\) \\(\\quad\\) \\(y_{14}\\)\n\n\\(\\bar{y}_{1}\\)\n\\(\\mu_{1}\\)\n\n\n2\n\n\\(y_{21}\\) \\(\\quad\\) \\(y_{22}\\) \\(\\quad\\) \\(y_{23}\\) \\(\\quad\\) \\(y_{24}\\)\n\n\\(\\bar{y}_{2}\\)\n\\(\\mu_{2}\\)\n\n\n3\n\n\\(y_{31}\\) \\(\\quad\\) \\(y_{32}\\) \\(\\quad\\) \\(y_{33}\\) \\(\\quad\\) \\(y_{34}\\)\n\n\\(\\bar{y}_{3}\\)\n\\(\\mu_{3}\\)\n\n\n4\n\n\\(y_{41}\\) \\(\\quad\\) \\(y_{42}\\) \\(\\quad\\) \\(y_{43}\\) \\(\\quad\\) \\(y_{44}\\)\n\n\\(\\bar{y}_{4}\\)\n\\(\\mu_{4}\\)\n\n\n5\n\n\\(y_{51}\\) \\(\\quad\\) \\(y_{52}\\) \\(\\quad\\) \\(y_{53}\\) \\(\\quad\\) \\(y_{54}\\)\n\n\\(\\bar{y}_{5}\\)\n\\(\\mu_{5}\\)\n\n\n\n\n\nThis is NOT a tidy data matrix. We may need to save the data in another format before we do ANOVA."
  },
  {
    "objectID": "slides/13-anova-slides.html#procedure-of-anova",
    "href": "slides/13-anova-slides.html#procedure-of-anova",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Procedure of ANOVA",
    "text": "Procedure of ANOVA\n\n \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\\\  &H_1: \\text{Population means are not all equal} \\end{align}\\) \n\n\nStatistician Ronald Fisher found a way to define a variable that follows the \\(F\\) distribution:\n\n\\[\\frac{\\text{variance between samples}}{\\text{variance within samples}} \\sim F_{df_B,\\, df_W}\\]\n\nIf variance between samples is larger than variance within samples, i.e., \\(F_{test}\\) is much greater than 1, as Data 1, we reject \\(H_0\\).\n\n\nKey: Define variance between samples and variance within samples so that the ratio is \\(F\\) distributed.\n\nHow do we convert the rationale into a formal testing procedure for the test"
  },
  {
    "objectID": "slides/13-anova-slides.html#variance-within-samples",
    "href": "slides/13-anova-slides.html#variance-within-samples",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Variance Within Samples",
    "text": "Variance Within Samples\n\nBack to two-sample pooled \\(t\\)-test with equal variance \\(\\sigma^2\\):\n\n\\[s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}\\]\n\nWhat if general \\(k\\) samples?\n\n\n\nANOVA assumes the populations have the same variance \\(\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_k^2 = \\sigma^2\\).\n\n\\[\\boxed{s_W^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2 + \\cdots + (n_k-1)s_k^2}{n_1 + n_2 + \\cdots + n_k - k}}\\] where \\(s_i^2\\), \\(i = 1, \\dots ,k\\), is the sample variance of group \\(i\\).\n\n\n\\(s_W^2\\) represents a combined estimate of the common variance \\(\\sigma^2\\). It measures variability of the observations within the \\(k\\) populations.\n\n\\(s_i^2 = \\frac{\\sum_{j=1}^{n_i}\\left(y_{ij} - \\bar{y}_{i\\cdot}\\right)^2}{n_i-1}\\)"
  },
  {
    "objectID": "slides/13-anova-slides.html#variance-between-samples",
    "href": "slides/13-anova-slides.html#variance-between-samples",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Variance Between Samples",
    "text": "Variance Between Samples\n\\[\\boxed{s^2_{B} = \\frac{\\sum_{i=1}^k n_i (\\bar{y}_{i} - \\bar{y}_{})^2}{k-1}}\\]\n\n\\(\\bar{y}_{i}\\) is the \\(i\\)-th sample mean.\n\\(\\bar{y}_{}\\) is the grand sample mean with all data points in all groups combined.\n\n\n\n\\(s^2_{B}\\) is also an estimate of \\(\\sigma^2\\) and measures variability among sample means for the \\(k\\) groups.\n\nIf \\(H_0\\) is true \\((\\mu_1 = \\cdots = \\mu_k = \\mu)\\), any variation in the sample means is due to chance and randomness, and shouldn‚Äôt be too large.\n\n\n\\(\\bar{y}_{1}, \\cdots, \\bar{y}_{k}\\) should be close each other, and they are close to \\(\\bar{y}_{}\\)."
  },
  {
    "objectID": "slides/13-anova-slides.html#s_b2-and-s_w2-as-sum-of-squaresdegrees-of-freedom",
    "href": "slides/13-anova-slides.html#s_b2-and-s_w2-as-sum-of-squaresdegrees-of-freedom",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "\n\\(s_B^2\\) and \\(s_W^2\\) as Sum of Squares/Degrees of Freedom",
    "text": "\\(s_B^2\\) and \\(s_W^2\\) as Sum of Squares/Degrees of Freedom\n\nVariance is defined as \\(\\frac{\\text{Sum of Squares}}{\\text{Degrees of Freedom}}\\), which is also called \\(\\text{Mean Square (MS)}\\)\n\\(s_B^2 = \\frac{\\sum_{i=1}^k n_i (\\bar{y}_{i} - \\bar{y}_{})^2}{k-1} = \\frac{\\text{Sum of Squares Between Samples (SSB)}}{df_B} = MSB\\)\n\\(s_W^2 = \\frac{\\sum_{i=1}^{k} (n_i - 1)s_i^2}{n_1 + n_2 + \\cdots + n_k - k} = \\frac{\\text{Sum of Squares Within Samples (SSW)}}{df_W} = MSW\\) \\((N = n_1 + \\cdots + n_k)\\)"
  },
  {
    "objectID": "slides/13-anova-slides.html#sum-of-squares-identity",
    "href": "slides/13-anova-slides.html#sum-of-squares-identity",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Sum of Squares Identity",
    "text": "Sum of Squares Identity\n\\[\\text{Total Sum of Squares (SST)} = \\sum_{j=1}^{n_i}\\sum_{i=1}^{k} \\left(y_{ij} - \\bar{y}_{}\\right)^2 = SSB + SSW\\]\n\\[df_{T} = df_{B} + df_{W} \\implies N - 1 = (k-1) + (N - k)\\]"
  },
  {
    "objectID": "slides/13-anova-slides.html#sum-of-squares-identity-3",
    "href": "slides/13-anova-slides.html#sum-of-squares-identity-3",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Sum of Squares Identity",
    "text": "Sum of Squares Identity\n\n\\(F_{test} = \\frac{MSB}{MSW}\\)\nUnder \\(H_0\\), \\(\\frac{S^2_{B}}{S_W^2} \\sim F_{k-1, \\, N-k}\\)\n\nReject \\(H_0\\) if\n\n\\(F_{test} &gt; F_{\\alpha, \\, k - 1,\\, N-k}\\)\n\n\\(p\\)-value \\(P(F_{k - 1,\\, N-k} &gt; F_{test}) &lt; \\alpha\\)"
  },
  {
    "objectID": "slides/13-anova-slides.html#example",
    "href": "slides/13-anova-slides.html#example",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Example",
    "text": "Example\n\nA hypothesis is that a nutrient ‚ÄúIsoflavones‚Äù varies among three types of food: (1) cereals and snacks, (2) energy bars, and (3) veggie burgers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA sample of 5 each is taken and the amount of isoflavones is measured.\nIs there a sufficient evidence to conclude that the mean isoflavone levels vary among these food items? \\(\\alpha = 0.05\\).\n\nAsk for \\(H_0\\)"
  },
  {
    "objectID": "slides/13-anova-slides.html#example---data",
    "href": "slides/13-anova-slides.html#example---data",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Example - Data",
    "text": "Example - Data\n\n\n\n\n\n\ndata\n\n   1  2  3\n1  3 19 25\n2 17 10 15\n3 12  9 12\n4 10  7  9\n5  4  5  8\n\n\nHere columns represent food items and rows are samples.\n\nSo tell me what is the value of \\(y_{23}\\)!\n\n\n\nWe prefer data format like\n\ndata_anova\n\n    y    food\n1   3 cereals\n2  17 cereals\n3  12 cereals\n4  10 cereals\n5   4 cereals\n6  19  energy\n7  10  energy\n8   9  energy\n9   7  energy\n10  5  energy\n11 25  veggie\n12 15  veggie\n13 12  veggie\n14  9  veggie\n15  8  veggie\n\n\n\n\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n# A tibble: 15 √ó 2\n   food        y\n   &lt;chr&gt;   &lt;dbl&gt;\n 1 cereals     3\n 2 cereals    17\n 3 cereals    12\n 4 cereals    10\n 5 cereals     4\n 6 energy     19\n 7 energy     10\n 8 energy      9\n 9 energy      7\n10 energy      5\n11 veggie     25\n12 veggie     15\n13 veggie     12\n14 veggie      9\n15 veggie      8"
  },
  {
    "objectID": "slides/13-anova-slides.html#example---boxplot",
    "href": "slides/13-anova-slides.html#example---boxplot",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Example - Boxplot",
    "text": "Example - Boxplot"
  },
  {
    "objectID": "slides/13-anova-slides.html#example---test-assumptions",
    "href": "slides/13-anova-slides.html#example---test-assumptions",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Example - Test Assumptions",
    "text": "Example - Test Assumptions\n\nAssumptions:\n\n\n\\(\\sigma_1 = \\sigma_2 = \\sigma_3\\) (I tested it)\nData are generated from a normal distribution for each type of food."
  },
  {
    "objectID": "slides/13-anova-slides.html#example---anova-testing",
    "href": "slides/13-anova-slides.html#example---anova-testing",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Example - ANOVA Testing",
    "text": "Example - ANOVA Testing\n\n\n\n\n\n \\(\\begin{align}&H_0: \\mu_1 = \\mu_2 = \\mu_3\\\\&H_1: \\mu_is \\text{ not all equal} \\end{align}\\) \n\n\n\nüòé Do all calculations and generate an ANOVA table using just one line of code! ü§ü ‚úåÔ∏è"
  },
  {
    "objectID": "slides/13-anova-slides.html#example---anova-table",
    "href": "slides/13-anova-slides.html#example---anova-table",
    "title": "Analysis of Variance (ANOVA) \n",
    "section": "Example - ANOVA Table",
    "text": "Example - ANOVA Table\n\nanova(lm(y ~ food, data = data_anova))\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nfood       2   60.4   30.20   0.828   0.46\nResiduals 12  437.6   36.47               \n\n\n\n\n\n\n    One-way analysis of means\n\ndata:  y and food\nF = 0.8282, num df = 2, denom df = 12, p-value = 0.46"
  },
  {
    "objectID": "slides/10-ht.html#what-is-a-hypothesis-testing",
    "href": "slides/10-ht.html#what-is-a-hypothesis-testing",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "What is a Hypothesis Testing?",
    "text": "What is a Hypothesis Testing?\n\n\nA hypothesis is a claim or statement about a property of a population, often the value of a population parameter.\n\n The mean body temperature of humans is less than \\(98.6^{\\circ}\\) F, or \\(\\mu &lt; 98.6\\). \n Marquette students‚Äô IQ scores has standard deviation equal to 15, or \\(\\sigma = 15\\). \n\n\n\n. . .\n\n\nNull hypothesis \\((H_0)\\): a statement that the value of a parameter is\n\n\nequal to some claim value\nthe negation of the alternative hypothesis\noften represents a skeptical perspective to be tested\n\n\n\nAlternative hypothesis \\((H_1\\) or \\(H_a)\\): a claim that the parameter is less than, greater than or not equal to some value.\n\nusually our research hypothesis of some new scientific theory or finding\n\n\n\n\n\n\nThe null hypothesis (H0) often represents either a skeptical perspective or a claim to be tested.\nThe alternative hypothesis (HA) is an alternative claim and is often represented by a range of possible parameter values.\nThe null hypothesis often represents a skeptical position or a perspective of ‚Äúno difference‚Äù.\nThe alternative hypothesis generally represents a new or stronger perspective."
  },
  {
    "objectID": "slides/10-ht.html#null-and-alternative-hypothesis",
    "href": "slides/10-ht.html#null-and-alternative-hypothesis",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Null and Alternative Hypothesis",
    "text": "Null and Alternative Hypothesis\n\nA \\(H_0\\) claim or \\(H_1\\) claim?\n\n\n The percentage of Marquette female students loving Japanese food is equal to 80%.\n On average, Marquette students consume less than 3 drinks per week. \n\n. . .\n\nA hypothesis testing 1 is a procedure to decide whether to reject \\(H_0\\) or not by how much evidence against \\(H_0\\).\n\n\nA hypothesis testing is a method for calculating the probability of making a specific observation under a working hypothesis, called the null hypothesis."
  },
  {
    "objectID": "slides/10-ht.html#hypothesis-testing-example",
    "href": "slides/10-ht.html#hypothesis-testing-example",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Hypothesis Testing Example",
    "text": "Hypothesis Testing Example\n\n\nA person is charged with a crime.\n\nA jury decide whether the person is guilty or not.\nThe accuse is assumed to be innocent until the jury declares otherwise.\nOnly if overwhelming evidence of the person‚Äôs guilt can be shown is the jury expected to declare the person guilty, otherwise the person is considered not guilty.\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\nWhat should be \\(H_0\\) and \\(H_a\\)?\n\n\n\nA person comes into court charged with a crime.\nA jury decide whether the person is guilty or not.\nEven though the person is charged with the crime, at the beginning of the trial (and until the jury declares otherwise) the accuse is assumed to be innocent.\nOnly if overwhelming evidence of the person‚Äôs guilt can be shown is the jury expected to declare the person guilty, otherwise the person is considered innocent.\n\n.question[ What should be \\(H_0\\) and \\(H_a\\)?]\n\n. . .\n\n\\(H_0:\\) The person is  not guilty  üôÇ\n\\(H_1:\\) The person is  guilty  üòü\nEvidence:  Photos, videos, witness, fingerprint, DNA \nDecision Rule:  Jury‚Äôs voting \nConclusion: Verdict  ‚Äúguilty‚Äù  or  ‚ÄúNOT enough evidence to convict‚Äù"
  },
  {
    "objectID": "slides/10-ht.html#how-to-formally-do-a-statistical-hypothesis-testing",
    "href": "slides/10-ht.html#how-to-formally-do-a-statistical-hypothesis-testing",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "How to Formally Do a Statistical Hypothesis Testing",
    "text": "How to Formally Do a Statistical Hypothesis Testing\n\n\nStep 0: Check Method Assumptions\n\n\n\nStep 1: Set the \\(H_0\\) and \\(H_a\\) in Symbolic Form from a Claim\n\n\n\nStep 2: Set the Significance Level \\(\\alpha\\)\n\n\nStep 3: Calculate the Test Statistic (Evidence) \n\n\n\n(Decision Rule I: Critical Value Method)\n\n Step 4-c: Find the Critical Value \n Step 5-c: Draw a Conclusion Using Critical Value Method \n\n\n(Decision Rule II: P-Value Method)\n\n Step 4-p: Find the P-Value \n Step 5-p: Draw a Conclusion Using P-Value Method \n\n\n\n\nStep 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim\n\n. . .\n\nüòé No worries, we will learn the steps step by step!"
  },
  {
    "objectID": "slides/10-ht.html#the-new-treatment-is-effective",
    "href": "slides/10-ht.html#the-new-treatment-is-effective",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\nA population of hypertension group is normal and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population, and \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\).\nGoal: Determine whether a new treatment is effective in reducing BP."
  },
  {
    "objectID": "slides/10-ht.html#step-0-check-method-assumptions",
    "href": "slides/10-ht.html#step-0-check-method-assumptions",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 0: Check Method Assumptions",
    "text": "Step 0: Check Method Assumptions\nThe testing methods are based on normality or approximate normality by CLT.\n\nRandom sample\nNormally distributed and/or \\(n &gt; 30\\)"
  },
  {
    "objectID": "slides/10-ht.html#the-new-treatment-is-effective-1",
    "href": "slides/10-ht.html#the-new-treatment-is-effective-1",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\n A population of hypertension group is normal  and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population, and \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\).\nGoal: Determine whether a new treatment is effective in reducing BP."
  },
  {
    "objectID": "slides/10-ht.html#step-1-set-the-h_0-and-h_1-from-a-claim",
    "href": "slides/10-ht.html#step-1-set-the-h_0-and-h_1-from-a-claim",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim",
    "text": "Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim\n\nüë©‚Äçüè´ The mean IQ score of statistics professors is higher than 120.\n\n. . .\n\n \\(\\begin{align}&H_0: \\mu \\le 120 \\\\ &H_1: \\mu &gt; 120 \\end{align}\\) \nüíµ The mean starting salary for Marquette graduates who didn‚Äôt take MATH 4720 is less than $60,000.\n\n. . .\n\n \\(\\begin{align} &H_0: \\mu \\ge 60000 \\\\ &H_1: \\mu &lt; 60000 \\end{align}\\) \nüì∫ The mean time between uses of a TV remote control by males during commercials equals 5 sec.\n\n. . .\n\n \\(\\begin{align} &H_0: \\mu = 5 \\\\ &H_1: \\mu \\ne 5 \\end{align}\\) \n\n\n( \\(H_0\\) claim)"
  },
  {
    "objectID": "slides/10-ht.html#the-new-treatment-is-effective-2",
    "href": "slides/10-ht.html#the-new-treatment-is-effective-2",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\nA population of hypertension group is normal and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population, and \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\).\nGoal: Determine whether a new treatment is  effective in reducing BP .\n\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\nThe claim that the new treatment is effective in reducing BP means the mean BP is less than 150. ( \\(H_1\\) claim )\n\n. . .\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\)"
  },
  {
    "objectID": "slides/10-ht.html#step-2-set-the-significance-level-alpha",
    "href": "slides/10-ht.html#step-2-set-the-significance-level-alpha",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 2: Set the Significance Level \\(\\alpha\\)\n",
    "text": "Step 2: Set the Significance Level \\(\\alpha\\)\n\n\nThe significant level \\(\\alpha\\) determines how rare or unlikely our evidence must be in order to represent sufficient evidence against \\(H_0\\).\nAn \\(\\alpha\\) level of 0.05 implies that evidence occurring with probability lower than 5% will be considered sufficient evidence against \\(H_0\\) (Reject \\(H_0\\)).\n\\(\\alpha = P(\\text{Reject } H_0 \\mid H_0 \\text{ is true})\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nRare Event Rule: If, under a given assumption, the probability of a particular observed event is exceptional small, we conclude that the assumption is probably not correct.\n\n\n\n\n\n\n\\(\\alpha\\) is like a threshold. If our collected evidence has a smaller probability than \\(\\alpha\\) happening, then the current assumption or claim is probably not correct.\nIf the true mean IQ of statistics professors is 120, \\(\\alpha \\%\\) of them will have IQ greater than 125.\nThe level \\(\\alpha\\) is related to \\(\\alpha\\) used in confidence intervals for defining a ‚Äúcritical value‚Äù.\n\n\\(\\alpha = 0.05\\) means that we incorrectly reject \\(H_0\\) 5 out of every 100 times we collect a sample and run the test."
  },
  {
    "objectID": "slides/10-ht.html#the-new-treatment-is-effective-3",
    "href": "slides/10-ht.html#the-new-treatment-is-effective-3",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\n A population of hypertension group is normal  and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population, and \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\).\nGoal: Determine whether a new treatment is effective in reducing BP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\) \n\n. . .\n\nSet \\(\\alpha= 0.05\\). We are asking: Is there a sufficient evidence at \\(\\alpha= 0.05\\) that the new treatment is effective?"
  },
  {
    "objectID": "slides/10-ht.html#step-3-calculate-the-test-statistic",
    "href": "slides/10-ht.html#step-3-calculate-the-test-statistic",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 3: Calculate the Test Statistic",
    "text": "Step 3: Calculate the Test Statistic\n\nA test statistic is a statistic value used in making a decision about the \\(H_0\\).\n\n\nSuppose  \\(H_0: \\mu = \\mu_0 \\quad H_1: \\mu &lt; \\mu_0\\) \nWhen computing a test statistic, we assume \\(H_0\\) is true.\nWhen \\(\\sigma\\) is known, the test statistic for testings about \\(\\mu\\) is\n\n\\[\\small \\boxed{ z_{test} = \\frac{\\overline{x} - \\color{blue}{\\mu_0}}{\\sigma/\\sqrt{n}} }\\]\n\nGuess what test statistic we use when \\(\\sigma\\) is unknown!\n\n. . .\n\nWhen \\(\\sigma\\) is unknown, the test statistic for testings about \\(\\mu\\) is\n\n\\[\\small \\boxed{ t_{test} = \\frac{\\overline{x} - \\color{blue}{\\mu_0}}{s/\\sqrt{n}} }\\]"
  },
  {
    "objectID": "slides/10-ht.html#the-new-treatment-is-effective-4",
    "href": "slides/10-ht.html#the-new-treatment-is-effective-4",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\n A population of hypertension group is normal  and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population,  \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\) .\nGoal: Determine whether a new treatment is effective in reducing BP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\) \n\n. . .\n\n The test statistic is \\(\\small t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}} =  \\frac{147.2 - 150}{5.5/\\sqrt{25}} = -2.55\\)"
  },
  {
    "objectID": "slides/10-ht.html#step-4-c-find-the-critical-value",
    "href": "slides/10-ht.html#step-4-c-find-the-critical-value",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 4-c: Find the Critical Value",
    "text": "Step 4-c: Find the Critical Value\n\nThe critical value(s) separates the rejection region or critical region (where we reject \\(H_0\\)) from the values of the test statistic that do not lead to rejection of \\(H_0\\).\n\nThey depends on whether the test is right-tailed, left-tailed or two-tailed.\n\n\n\n\n\n\n\n\n\n\n\n\nIn the absence of prior evidence that people typically wish to be lighter (or heavier), it is reasonable to begin with an alternative hypothesis that allows for differences in either direction. A one-sided alternative could be used if, for example, an investigator felt there was prior evidence that people typically wish to weigh less than they currently do:"
  },
  {
    "objectID": "slides/10-ht.html#step-4-c-find-the-critical-value-1",
    "href": "slides/10-ht.html#step-4-c-find-the-critical-value-1",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 4-c: Find the Critical Value",
    "text": "Step 4-c: Find the Critical Value\n\nüëâ \\(z_{\\alpha}\\) is such that \\(P(Z &gt; z_{\\alpha}) = \\alpha\\) and \\(Z \\sim N(0, 1)\\).\nüëâ \\(t_{\\alpha, n-1}\\) is such that \\(P(T &gt; t_{\\alpha, n-1}) = \\alpha\\) and \\(T \\sim t_{n-1}\\).\n\n\n\n\n\n\n\n\n\nCondition ¬† ¬†\n\nRight-tailed \\((H_1: \\mu &gt; \\mu_0)\\)\n\n\nLeft-tailed \\((H_1: \\mu &lt; \\mu_0)\\)\n\n\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\n\\(\\sigma\\) known\n\\(z_{\\alpha}\\)\n\\(-z_{\\alpha}\\)\n\n\\(-z_{\\alpha/2}\\) and \\(z_{\\alpha/2}\\)\n\n\n\n\n\\(\\sigma\\) unknown\n\\(t_{\\alpha, n-1}\\)\n\\(-t_{\\alpha, n-1}\\)\n\n\\(-t_{\\alpha/2, n-1}\\) and \\(t_{\\alpha/2, n-1}\\)\n\n\n\n\n\n\\(z_{0.025} =\\) 1.96, \\(z_{0.05} =\\) 1.64\n\\(z_{\\alpha}\\) and \\(t_{\\alpha, n-1}\\) are always positive."
  },
  {
    "objectID": "slides/10-ht.html#the-new-treatment-is-effective-5",
    "href": "slides/10-ht.html#the-new-treatment-is-effective-5",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\n A population of hypertension group is normal  and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population,  \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\) .\nGoal: Determine whether a new treatment is effective in reducing BP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\) \n The test statistic is \\(\\small t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}} =  \\frac{147.2 - 150}{5.5/\\sqrt{25}} = -2.55\\)\n\n. . .\n\n The critical value is \\(\\small -t_{0.05, 25-1} = -t_{0.05, 24} = -1.711\\)"
  },
  {
    "objectID": "slides/10-ht.html#step-5-c-draw-a-conclusion-using-critical-value",
    "href": "slides/10-ht.html#step-5-c-draw-a-conclusion-using-critical-value",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 5-c: Draw a Conclusion Using Critical Value",
    "text": "Step 5-c: Draw a Conclusion Using Critical Value\nIf the test statistic is\n\nin the rejection region, we reject \\(H_0\\).\nnot in the rejection region, we do not or fail to reject \\(H_0\\).\n\nReject \\(H_0\\) if\n\n\n\n\n\n\n\n\n\nCondition ¬† ¬†\n\nRight-tailed \\((H_1: \\mu &gt; \\mu_0)\\)\n\n\nLeft-tailed \\((H_1: \\mu &lt; \\mu_0)\\)\n\n\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\n\\(\\sigma\\) known\n\\(z_{test} &gt; z_{\\alpha}\\)\n\\(z_{test} &lt; -z_{\\alpha}\\)\n\\(\\mid z_{test}\\mid \\, &gt; z_{\\alpha/2}\\)\n\n\n\n\\(\\sigma\\) unknown\n\\(t_{test} &gt; t_{\\alpha, n-1}\\)\n\\(t_{test} &lt; -t_{\\alpha, n-1}\\)\n\\(\\mid t_{test}\\mid \\, &gt; t_{\\alpha/2, n-1}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(|z_{test}| &gt; z_{\\alpha/2}\\)\\(|t_{test}| &gt; t_{\\alpha/2, n-1}\\)"
  },
  {
    "objectID": "slides/10-ht.html#the-new-treatment-is-effective-6",
    "href": "slides/10-ht.html#the-new-treatment-is-effective-6",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\n A population of hypertension group is normal  and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population,  \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\) .\nGoal: Determine whether a new treatment is effective in reducing BP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\) \n The test statistic is \\(\\small t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}} =  \\frac{147.2 - 150}{5.5/\\sqrt{25}} = -2.55\\)\n The critical value is \\(\\small -t_{0.05, 25-1} = -t_{0.05, 24} = -1.711\\) \n\n. . .\n\n We reject \\(H_0\\) if \\(t_{test} &lt; -t_{\\alpha, n-1}\\). Since \\(\\small t_{test} = -2.55 &lt; -1.711 = -t_{\\alpha, n-1}\\), we reject \\(H_0\\)."
  },
  {
    "objectID": "slides/10-ht.html#step-4-p-find-the-p-value",
    "href": "slides/10-ht.html#step-4-p-find-the-p-value",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 4-p: Find the P-Value",
    "text": "Step 4-p: Find the P-Value\n\nThe \\(p\\)-value measures the strength of the evidence against \\(H_0\\) provided by the data.\nThe smaller the \\(p\\)-value, the greater the evidence against \\(H_0\\).\n\nThe \\(p\\)-value is the probability of getting a test statistic value that is at least as extreme as the one obtained from the data, assuming that \\(H_0\\) is true. \\((\\mu = \\mu_0)\\)\n\nFor example, \\(p\\)-value \\(= P(Z \\ge z_{test} \\mid H_0)\\) for a right-tailed test.\n\n\n\n\n\nWhat is the chance of having evidence that is more unlikely to happen than our current collected evidence?"
  },
  {
    "objectID": "slides/10-ht.html#p-value-illustration",
    "href": "slides/10-ht.html#p-value-illustration",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "P-Value Illustration",
    "text": "P-Value Illustration"
  },
  {
    "objectID": "slides/10-ht.html#the-new-treatment-is-effective-7",
    "href": "slides/10-ht.html#the-new-treatment-is-effective-7",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\n A population of hypertension group is normal  and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population,  \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\) .\nGoal: Determine whether a new treatment is effective in reducing BP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\) \nThis is a left-tailed test, so the \\(p\\)-value is \\(P(T &lt; t_{test})=P(T &lt; -2.55) =\\) 0.01 \n\n\n\nStep 5-p:  We reject \\(H_0\\) if \\(p\\)-value &lt; \\(\\alpha\\). Since \\(p\\)-value \\(= 0.0088 &lt; 0.05 = \\alpha\\), we reject \\(H_0\\).\n\nStep 6:  There is sufficient evidence to support the claim that the new treatment is effective."
  },
  {
    "objectID": "slides/10-ht.html#step-5-p-draw-a-conclusion-using-p-value-method",
    "href": "slides/10-ht.html#step-5-p-draw-a-conclusion-using-p-value-method",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 5-p: Draw a Conclusion Using P-Value Method",
    "text": "Step 5-p: Draw a Conclusion Using P-Value Method\n\nIf \\(p\\)-value \\(\\le \\alpha\\) , reject \\(H_0\\).\nIf \\(p\\)-value \\(&gt; \\alpha\\), do not reject \\(H_0\\).\n\n\n\n\n\n\n\n\n\nCondition ¬† ¬†\n\nRight-tailed \\((H_1: \\mu &gt; \\mu_0)\\)\n\n\nLeft-tailed \\((H_1: \\mu &lt; \\mu_0)\\)\n\n\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\n\\(\\sigma\\) known\n\\(P(Z &gt; z_{test} \\mid H_0)\\)\n\\(P(Z &lt; z_{test} \\mid H_0)\\)\n\\(2P(Z &gt; \\,\\mid z_{test} \\mid \\, \\mid H_0)\\)\n\n\n\n\\(\\sigma\\) unknown\n\\(P(T &gt; t_{test} \\mid H_0)\\)\n\\(P(T &lt; t_{test} \\mid H_0)\\)\n\\(2P(T &gt; \\, \\mid t_{test} \\mid  \\, \\mid H_0)\\)\n\n\n\n\n\\(2P(Z &gt; z_{test} \\mid H_0)\\) if \\(z_{test}\\) on the right \\(2P(Z &lt; z_{test} \\mid H_0)\\) if \\(z_{test}\\) on the left \\(2P(T &gt; t_{test} \\mid H_0)\\) if \\(t_{test}\\) on the right \\(2P(T &lt; t_{test} \\mid H_0)\\) if \\(t_{test}\\) on the left"
  },
  {
    "objectID": "slides/10-ht.html#the-new-treatment-is-effective-8",
    "href": "slides/10-ht.html#the-new-treatment-is-effective-8",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\n A population of hypertension group is normal  and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population,  \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\) .\nGoal: Determine whether a new treatment is effective in reducing BP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\) \nThis is a left-tailed test, so the \\(p\\)-value is \\(P(T &lt; t_{test})=P(T &lt; -2.55) =\\) 0.01 \n\n. . .\n\n We reject \\(H_0\\) if \\(p\\)-value &lt; \\(\\alpha\\). Since \\(p\\)-value \\(= 0.01 &lt; 0.05 = \\alpha\\), we reject \\(H_0\\).\n\n\n\nStep 6:  There is sufficient evidence to support the claim that the new treatment is effective."
  },
  {
    "objectID": "slides/10-ht.html#both-methods-lead-to-the-same-conclusion",
    "href": "slides/10-ht.html#both-methods-lead-to-the-same-conclusion",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Both Methods Lead to the Same Conclusion",
    "text": "Both Methods Lead to the Same Conclusion"
  },
  {
    "objectID": "slides/10-ht.html#step-6-restate-the-conclusion-in-nontechnical-terms-and-address-the-original-claim",
    "href": "slides/10-ht.html#step-6-restate-the-conclusion-in-nontechnical-terms-and-address-the-original-claim",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim",
    "text": "Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim\n\n\n\n\n\nhttps://www.drdawnwright.com/category/statistics/"
  },
  {
    "objectID": "slides/10-ht.html#section-2",
    "href": "slides/10-ht.html#section-2",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "",
    "text": "https://www.pinterest.com/pin/287878601159173631/\n\n\n\n\n\nNote! Why can‚Äôt we say we ‚Äúaccept the null‚Äù? The reason is that we are assuming the null hypothesis is true and trying to see if there is evidence against it. Therefore, the conclusion should be in terms of rejecting the null."
  },
  {
    "objectID": "slides/10-ht.html#the-new-treatment-is-effective-9",
    "href": "slides/10-ht.html#the-new-treatment-is-effective-9",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\n A population of hypertension group is normal  and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population,  \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\) .\nGoal: Determine whether a new treatment is effective in reducing BP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\) \n There is sufficient evidence to support the claim that the new treatment is effective."
  },
  {
    "objectID": "slides/10-ht.html#example-calculation-in-r",
    "href": "slides/10-ht.html#example-calculation-in-r",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Example Calculation in R",
    "text": "Example Calculation in R\n\n## create objects for any information we have\nalpha &lt;- 0.05; mu_0 &lt;- 150; \nx_bar &lt;- 147.2; s &lt;- 5.5; n &lt;- 25\n\n## Test statistic\n(t_test &lt;- (x_bar - mu_0) / (s / sqrt(n))) \n\n[1] -2.545\n\n## Critical value\n(t_cri &lt;- qt(alpha, df = n - 1, lower.tail = TRUE)) \n\n[1] -1.711\n\n## p-value\n(p_val &lt;- pt(t_test, df = n - 1, lower.tail = TRUE)) \n\n[1] 0.008878\n\n\n\n\n\n[1] \"Reject H0 by crit. value\"\n\n\n[1] \"Reject H0 by p-value\""
  },
  {
    "objectID": "slides/10-ht.html#example-2-two-tailed-z-test",
    "href": "slides/10-ht.html#example-2-two-tailed-z-test",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Example 2: Two-tailed z-test",
    "text": "Example 2: Two-tailed z-test\n\n\nThe milk price of a gallon of 2% milk is normally distributed with standard deviation of $0.10.\n\nLast week the mean milk price was 2.78. This week, based on a sample of size 25, the sample mean milk price \\(\\overline{x} = 2.80\\).\nUnder \\(\\alpha = 0.05\\), determine if this week the mean price is different.\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\nStep 1: This is a \\(H_1\\) claim  \\(\\small \\begin{align}&H_0: \\mu = 2.78 \\\\ &H_1: \\mu \\ne 2.78 \\end{align}\\) \n\n\n. . .\n\nStep 2:  \\(\\small \\alpha = 0.05\\) \n\n\n. . .\n\nStep 3:  \\(\\small z_{test} = \\frac{\\overline{x} - \\mu_0}{\\sigma/\\sqrt{n}} =  \\frac{2.8 - 2.78}{0.1/\\sqrt{25}} = 1.00\\) \n\n\n. . .\n\nStep 4-c:  \\(\\small z_{0.05/2} = 1.96\\). \n\n\n. . .\n\nStep 5-c: This is a two-tailed test and we reject \\(H_0\\) if \\(|z_{test}| &gt; z_{\\alpha/2}\\). Since \\(\\small |z_{test}| = 1 &lt; 1.96 = z_{\\alpha/2}\\), we DO NOT reject \\(H_0\\)."
  },
  {
    "objectID": "slides/10-ht.html#example-2-two-tailed-z-test-contd",
    "href": "slides/10-ht.html#example-2-two-tailed-z-test-contd",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Example 2: two-tailed z-test Cont‚Äôd",
    "text": "Example 2: two-tailed z-test Cont‚Äôd\n\nStep 4-p: This is a two-tailed test, and the test statistic is on the right \\((&gt; 0)\\), so the \\(p\\)-value is \\(2P(Z &gt; z_{test})=\\) 0.317 \n\n\n. . .\n\nStep 5-p:  We reject \\(H_0\\) if \\(p\\)-value &lt; \\(\\alpha\\). Since \\(p\\)-value \\(= 0.317 &gt; 0.05 = \\alpha\\), we DO NOT reject \\(H_0\\).\n\n\n. . .\n\nStep 6:  There is insufficient evidence to support the claim that this week the mean milk price is different from the price last week."
  },
  {
    "objectID": "slides/10-ht.html#example-2-calculation-in-r",
    "href": "slides/10-ht.html#example-2-calculation-in-r",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Example 2 Calculation in R",
    "text": "Example 2 Calculation in R\n\n## create objects to be used\nalpha &lt;- 0.05; mu_0 &lt;- 2.78; \nx_bar &lt;- 2.8; sigma &lt;- 0.1; n &lt;- 25\n\n## Test statistic\n(z_test &lt;- (x_bar - mu_0) / (sigma / sqrt(n))) \n\n[1] 1\n\n## Critical value\n(z_crit &lt;- qnorm(alpha/2, lower.tail = FALSE)) \n\n[1] 1.96\n\n## p-value\n(p_val &lt;- 2 * pnorm(z_test, lower.tail = FALSE)) \n\n[1] 0.3173\n\n\n\n\n\n[1] \"Do not reject H0 by critical value method\"\n\n\n[1] \"Do not reject H0 by p-value method\""
  },
  {
    "objectID": "slides/10-ht.html#testing-summary",
    "href": "slides/10-ht.html#testing-summary",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Testing Summary",
    "text": "Testing Summary\n\n\n\n\n\n\n\n\nNumerical Data, \\(\\sigma\\) known\nNumerical Data, \\(\\sigma\\)  unknown \n\n\n\n\nParameter of Interest\nPopulation Mean \\(\\mu\\)\n\nPopulation Mean \\(\\mu\\)\n\n\n\nTest Type\nOne sample \\(\\color{blue}{z}\\) test \\(H_0: \\mu = \\mu_0\\)\n\nOne sample \\(\\color{blue}{t}\\) test \\(H_0: \\mu = \\mu_0\\)\n\n\n\nConfidence Interval\n\\(\\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\)\n\\(\\bar{x} \\pm t_{\\alpha/2, n-1} \\frac{\\color{blue}{s}}{\\sqrt{n}}\\)\n\n\nTest Stat under \\(H_0\\)\n\\(z_{test} = \\frac{\\overline{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\)\n\\(t_{test} = \\frac{\\overline{x} - \\mu_0}{\\frac{\\color{blue}{s}}{\\sqrt{n}}}\\)\n\n\n\\(p\\)-value under \\(H_0\\)\n\n\\(H_1: \\mu &lt; \\mu_0\\) \\(p\\)-value \\(=P(Z \\le z_{test})\\)\n\n\n\\(H_1: \\mu &lt; \\mu_0\\) \\(p\\)-value \\(=P(T_{n-1} \\le t_{test})\\)\n\n\n\n\n\n\\(H_1: \\mu &gt; \\mu_0\\) \\(p\\)-value \\(=P(Z \\ge z_{test})\\)\n\n\n\\(H_1: \\mu &lt; \\mu_0\\) \\(p\\)-value \\(=P(T_{n-1} \\ge t_{test})\\)\n\n\n\n\n\n\\(H_1: \\mu \\ne \\mu_0\\) \\(p\\)-value \\(=2P(Z \\ge \\, \\mid z_{test}\\mid)\\)\n\n\n\\(H_1: \\mu \\ne \\mu_0\\) \\(p\\)-value \\(=2P(T_{n-1} \\ge  \\, \\mid t_{test} \\mid)\\)"
  },
  {
    "objectID": "slides/10-ht.html#type-i-and-type-ii-errors",
    "href": "slides/10-ht.html#type-i-and-type-ii-errors",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\n\n\nDecision\n\n\\(H_0\\) is true\n\n\\(H_0\\) is false\n\n\n\nReject \\(H_0\\)\n\nType I error\nCorrect decision\n\n\nDo not reject \\(H_0\\)\n\nCorrect decision\nType II error\n\n\n\n. . .\n\nBack to the crime example: \\(H_0:\\) The person is  not guilty  v.s. \\(H_1:\\) The person is  guilty \n\n\n\n\n\n\n\n\nDecision\nTruth is the person innocent\nTruth is the person guilty\n\n\n\nJury decides the person guilty\nType I error\nCorrect decision\n\n\nJury decides the person innocent\nCorrect decision\nType II error\n\n\n\n\n\nIn practice, we will not know for certain if we made the correct decision or if we made one of these two errors."
  },
  {
    "objectID": "slides/10-ht.html#type-i-and-type-ii-errors-1",
    "href": "slides/10-ht.html#type-i-and-type-ii-errors-1",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\n\n\\(\\alpha = P(\\text{type I error}) = P(\\text{rejecting } H_0 \\text{ when } H_0 \\text{ is true})\\)\n\\(\\beta = P(\\text{type II error}) = P(\\text{failing to reject } H_0 \\text{ when } H_0 \\text{ is false})\\)\n\n\n\n\n\n\nhttps://www.statisticssolutions.com/wp-content/uploads/2017/12/rachnovblog.jpg"
  },
  {
    "objectID": "slides/10-ht.html#footnotes",
    "href": "slides/10-ht.html#footnotes",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Footnotes",
    "text": "Footnotes\n\nNull Hypothesis Statistical Testing (NHST), statistical testing or test of significance.‚Ü©Ô∏é"
  },
  {
    "objectID": "slides/10-ht-slides.html#what-is-a-hypothesis-testing",
    "href": "slides/10-ht-slides.html#what-is-a-hypothesis-testing",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "What is a Hypothesis Testing?",
    "text": "What is a Hypothesis Testing?\n\n\nA hypothesis is a claim or statement about a property of a population, often the value of a population parameter.\n\n The mean body temperature of humans is less than \\(98.6^{\\circ}\\) F, or \\(\\mu &lt; 98.6\\). \n Marquette students‚Äô IQ scores has standard deviation equal to 15, or \\(\\sigma = 15\\). \n\n\n\n\n\n\nNull hypothesis \\((H_0)\\): a statement that the value of a parameter is\n\n\nequal to some claim value\nthe negation of the alternative hypothesis\noften represents a skeptical perspective to be tested\n\n\n\nAlternative hypothesis \\((H_1\\) or \\(H_a)\\): a claim that the parameter is less than, greater than or not equal to some value.\n\nusually our research hypothesis of some new scientific theory or finding\n\n\n\n\nThe null hypothesis (H0) often represents either a skeptical perspective or a claim to be tested.\nThe alternative hypothesis (HA) is an alternative claim and is often represented by a range of possible parameter values.\nThe null hypothesis often represents a skeptical position or a perspective of ‚Äúno difference‚Äù.\nThe alternative hypothesis generally represents a new or stronger perspective."
  },
  {
    "objectID": "slides/10-ht-slides.html#null-and-alternative-hypothesis",
    "href": "slides/10-ht-slides.html#null-and-alternative-hypothesis",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Null and Alternative Hypothesis",
    "text": "Null and Alternative Hypothesis\n\nA \\(H_0\\) claim or \\(H_1\\) claim?\n\n\n The percentage of Marquette female students loving Japanese food is equal to 80%.\n On average, Marquette students consume less than 3 drinks per week. \n\n\n\nA hypothesis testing 1 is a procedure to decide whether to reject \\(H_0\\) or not by how much evidence against \\(H_0\\).\n\nA hypothesis testing is a method for calculating the probability of making a specific observation under a working hypothesis, called the null hypothesis.\n\n\nNull Hypothesis Statistical Testing (NHST), statistical testing or test of significance."
  },
  {
    "objectID": "slides/10-ht-slides.html#hypothesis-testing-example",
    "href": "slides/10-ht-slides.html#hypothesis-testing-example",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Hypothesis Testing Example",
    "text": "Hypothesis Testing Example\n\n\nA person is charged with a crime.\n\nA jury decide whether the person is guilty or not.\nThe accuse is assumed to be innocent until the jury declares otherwise.\nOnly if overwhelming evidence of the person‚Äôs guilt can be shown is the jury expected to declare the person guilty, otherwise the person is considered not guilty.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat should be \\(H_0\\) and \\(H_a\\)?\n\n\nA person comes into court charged with a crime.\nA jury decide whether the person is guilty or not.\nEven though the person is charged with the crime, at the beginning of the trial (and until the jury declares otherwise) the accuse is assumed to be innocent.\nOnly if overwhelming evidence of the person‚Äôs guilt can be shown is the jury expected to declare the person guilty, otherwise the person is considered innocent.\n\n.question[ What should be \\(H_0\\) and \\(H_a\\)?]\n\n\n\n\n\\(H_0:\\) The person is  not guilty  üôÇ\n\\(H_1:\\) The person is  guilty  üòü\nEvidence:  Photos, videos, witness, fingerprint, DNA \nDecision Rule:  Jury‚Äôs voting \nConclusion: Verdict  ‚Äúguilty‚Äù  or  ‚ÄúNOT enough evidence to convict‚Äù"
  },
  {
    "objectID": "slides/10-ht-slides.html#how-to-formally-do-a-statistical-hypothesis-testing",
    "href": "slides/10-ht-slides.html#how-to-formally-do-a-statistical-hypothesis-testing",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "How to Formally Do a Statistical Hypothesis Testing",
    "text": "How to Formally Do a Statistical Hypothesis Testing\n\n\nStep 0: Check Method Assumptions\n\n\n\nStep 1: Set the \\(H_0\\) and \\(H_a\\) in Symbolic Form from a Claim\n\n\n\nStep 2: Set the Significance Level \\(\\alpha\\)\n\n\nStep 3: Calculate the Test Statistic (Evidence) \n\n\n\n(Decision Rule I: Critical Value Method)\n\n Step 4-c: Find the Critical Value \n Step 5-c: Draw a Conclusion Using Critical Value Method \n\n\n(Decision Rule II: P-Value Method)\n\n Step 4-p: Find the P-Value \n Step 5-p: Draw a Conclusion Using P-Value Method \n\n\n\nStep 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim\n\n\n\nüòé No worries, we will learn the steps step by step!"
  },
  {
    "objectID": "slides/10-ht-slides.html#the-new-treatment-is-effective",
    "href": "slides/10-ht-slides.html#the-new-treatment-is-effective",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\nA population of hypertension group is normal and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population, and \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\).\nGoal: Determine whether a new treatment is effective in reducing BP."
  },
  {
    "objectID": "slides/10-ht-slides.html#step-0-check-method-assumptions",
    "href": "slides/10-ht-slides.html#step-0-check-method-assumptions",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 0: Check Method Assumptions",
    "text": "Step 0: Check Method Assumptions\nThe testing methods are based on normality or approximate normality by CLT.\n\nRandom sample\nNormally distributed and/or \\(n &gt; 30\\)"
  },
  {
    "objectID": "slides/10-ht-slides.html#the-new-treatment-is-effective-1",
    "href": "slides/10-ht-slides.html#the-new-treatment-is-effective-1",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\n A population of hypertension group is normal  and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population, and \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\).\nGoal: Determine whether a new treatment is effective in reducing BP."
  },
  {
    "objectID": "slides/10-ht-slides.html#step-1-set-the-h_0-and-h_1-from-a-claim",
    "href": "slides/10-ht-slides.html#step-1-set-the-h_0-and-h_1-from-a-claim",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim",
    "text": "Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim\n\nüë©‚Äçüè´ The mean IQ score of statistics professors is higher than 120.\n\n\n\n \\(\\begin{align}&H_0: \\mu \\le 120 \\\\ &H_1: \\mu &gt; 120 \\end{align}\\) \nüíµ The mean starting salary for Marquette graduates who didn‚Äôt take MATH 4720 is less than $60,000.\n\n\n\n\n \\(\\begin{align} &H_0: \\mu \\ge 60000 \\\\ &H_1: \\mu &lt; 60000 \\end{align}\\) \nüì∫ The mean time between uses of a TV remote control by males during commercials equals 5 sec.\n\n\n\n\n \\(\\begin{align} &H_0: \\mu = 5 \\\\ &H_1: \\mu \\ne 5 \\end{align}\\) \n\n( \\(H_0\\) claim)"
  },
  {
    "objectID": "slides/10-ht-slides.html#the-new-treatment-is-effective-2",
    "href": "slides/10-ht-slides.html#the-new-treatment-is-effective-2",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\nA population of hypertension group is normal and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population, and \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\).\nGoal: Determine whether a new treatment is  effective in reducing BP .\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe claim that the new treatment is effective in reducing BP means the mean BP is less than 150. ( \\(H_1\\) claim )\n\n\n\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\)"
  },
  {
    "objectID": "slides/10-ht-slides.html#step-2-set-the-significance-level-alpha",
    "href": "slides/10-ht-slides.html#step-2-set-the-significance-level-alpha",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 2: Set the Significance Level \\(\\alpha\\)\n",
    "text": "Step 2: Set the Significance Level \\(\\alpha\\)\n\n\nThe significant level \\(\\alpha\\) determines how rare or unlikely our evidence must be in order to represent sufficient evidence against \\(H_0\\).\nAn \\(\\alpha\\) level of 0.05 implies that evidence occurring with probability lower than 5% will be considered sufficient evidence against \\(H_0\\) (Reject \\(H_0\\)).\n\\(\\alpha = P(\\text{Reject } H_0 \\mid H_0 \\text{ is true})\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nRare Event Rule: If, under a given assumption, the probability of a particular observed event is exceptional small, we conclude that the assumption is probably not correct.\n\n\n\n\n\\(\\alpha\\) is like a threshold. If our collected evidence has a smaller probability than \\(\\alpha\\) happening, then the current assumption or claim is probably not correct.\nIf the true mean IQ of statistics professors is 120, \\(\\alpha \\%\\) of them will have IQ greater than 125.\nThe level \\(\\alpha\\) is related to \\(\\alpha\\) used in confidence intervals for defining a ‚Äúcritical value‚Äù.\n\n\\(\\alpha = 0.05\\) means that we incorrectly reject \\(H_0\\) 5 out of every 100 times we collect a sample and run the test."
  },
  {
    "objectID": "slides/10-ht-slides.html#the-new-treatment-is-effective-3",
    "href": "slides/10-ht-slides.html#the-new-treatment-is-effective-3",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\n A population of hypertension group is normal  and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population, and \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\).\nGoal: Determine whether a new treatment is effective in reducing BP.\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\) \n\n\n\nSet \\(\\alpha= 0.05\\). We are asking: Is there a sufficient evidence at \\(\\alpha= 0.05\\) that the new treatment is effective?"
  },
  {
    "objectID": "slides/10-ht-slides.html#step-3-calculate-the-test-statistic",
    "href": "slides/10-ht-slides.html#step-3-calculate-the-test-statistic",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 3: Calculate the Test Statistic",
    "text": "Step 3: Calculate the Test Statistic\n\nA test statistic is a statistic value used in making a decision about the \\(H_0\\).\n\n\nSuppose  \\(H_0: \\mu = \\mu_0 \\quad H_1: \\mu &lt; \\mu_0\\) \nWhen computing a test statistic, we assume \\(H_0\\) is true.\nWhen \\(\\sigma\\) is known, the test statistic for testings about \\(\\mu\\) is\n\n\\[\\small \\boxed{ z_{test} = \\frac{\\overline{x} - \\color{blue}{\\mu_0}}{\\sigma/\\sqrt{n}} }\\]\n\nGuess what test statistic we use when \\(\\sigma\\) is unknown!\n\n\n\nWhen \\(\\sigma\\) is unknown, the test statistic for testings about \\(\\mu\\) is\n\n\\[\\small \\boxed{ t_{test} = \\frac{\\overline{x} - \\color{blue}{\\mu_0}}{s/\\sqrt{n}} }\\]"
  },
  {
    "objectID": "slides/10-ht-slides.html#the-new-treatment-is-effective-4",
    "href": "slides/10-ht-slides.html#the-new-treatment-is-effective-4",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\n A population of hypertension group is normal  and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population,  \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\) .\nGoal: Determine whether a new treatment is effective in reducing BP.\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\) \n\n\n\n The test statistic is \\(\\small t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}} =  \\frac{147.2 - 150}{5.5/\\sqrt{25}} = -2.55\\)"
  },
  {
    "objectID": "slides/10-ht-slides.html#step-4-c-find-the-critical-value",
    "href": "slides/10-ht-slides.html#step-4-c-find-the-critical-value",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 4-c: Find the Critical Value",
    "text": "Step 4-c: Find the Critical Value\n\nThe critical value(s) separates the rejection region or critical region (where we reject \\(H_0\\)) from the values of the test statistic that do not lead to rejection of \\(H_0\\).\n\nThey depends on whether the test is right-tailed, left-tailed or two-tailed.\n\n\n\n\nIn the absence of prior evidence that people typically wish to be lighter (or heavier), it is reasonable to begin with an alternative hypothesis that allows for differences in either direction. A one-sided alternative could be used if, for example, an investigator felt there was prior evidence that people typically wish to weigh less than they currently do:"
  },
  {
    "objectID": "slides/10-ht-slides.html#step-4-c-find-the-critical-value-1",
    "href": "slides/10-ht-slides.html#step-4-c-find-the-critical-value-1",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 4-c: Find the Critical Value",
    "text": "Step 4-c: Find the Critical Value\n\nüëâ \\(z_{\\alpha}\\) is such that \\(P(Z &gt; z_{\\alpha}) = \\alpha\\) and \\(Z \\sim N(0, 1)\\).\nüëâ \\(t_{\\alpha, n-1}\\) is such that \\(P(T &gt; t_{\\alpha, n-1}) = \\alpha\\) and \\(T \\sim t_{n-1}\\).\n\n\n\n\n\n\n\n\n\nCondition ¬† ¬†\n\nRight-tailed \\((H_1: \\mu &gt; \\mu_0)\\)\n\n\nLeft-tailed \\((H_1: \\mu &lt; \\mu_0)\\)\n\n\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\n\\(\\sigma\\) known\n\\(z_{\\alpha}\\)\n\\(-z_{\\alpha}\\)\n\n\\(-z_{\\alpha/2}\\) and \\(z_{\\alpha/2}\\)\n\n\n\n\n\\(\\sigma\\) unknown\n\\(t_{\\alpha, n-1}\\)\n\\(-t_{\\alpha, n-1}\\)\n\n\\(-t_{\\alpha/2, n-1}\\) and \\(t_{\\alpha/2, n-1}\\)\n\n\n\n\n\n\\(z_{0.025} =\\) 1.96, \\(z_{0.05} =\\) 1.64\n\\(z_{\\alpha}\\) and \\(t_{\\alpha, n-1}\\) are always positive."
  },
  {
    "objectID": "slides/10-ht-slides.html#the-new-treatment-is-effective-5",
    "href": "slides/10-ht-slides.html#the-new-treatment-is-effective-5",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\n A population of hypertension group is normal  and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population,  \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\) .\nGoal: Determine whether a new treatment is effective in reducing BP.\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\) \n The test statistic is \\(\\small t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}} =  \\frac{147.2 - 150}{5.5/\\sqrt{25}} = -2.55\\)\n\n\n\n The critical value is \\(\\small -t_{0.05, 25-1} = -t_{0.05, 24} = -1.711\\)"
  },
  {
    "objectID": "slides/10-ht-slides.html#step-5-c-draw-a-conclusion-using-critical-value",
    "href": "slides/10-ht-slides.html#step-5-c-draw-a-conclusion-using-critical-value",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 5-c: Draw a Conclusion Using Critical Value",
    "text": "Step 5-c: Draw a Conclusion Using Critical Value\nIf the test statistic is\n\nin the rejection region, we reject \\(H_0\\).\nnot in the rejection region, we do not or fail to reject \\(H_0\\).\n\nReject \\(H_0\\) if\n\n\n\n\n\n\n\n\n\nCondition ¬† ¬†\n\nRight-tailed \\((H_1: \\mu &gt; \\mu_0)\\)\n\n\nLeft-tailed \\((H_1: \\mu &lt; \\mu_0)\\)\n\n\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\n\\(\\sigma\\) known\n\\(z_{test} &gt; z_{\\alpha}\\)\n\\(z_{test} &lt; -z_{\\alpha}\\)\n\\(\\mid z_{test}\\mid \\, &gt; z_{\\alpha/2}\\)\n\n\n\n\\(\\sigma\\) unknown\n\\(t_{test} &gt; t_{\\alpha, n-1}\\)\n\\(t_{test} &lt; -t_{\\alpha, n-1}\\)\n\\(\\mid t_{test}\\mid \\, &gt; t_{\\alpha/2, n-1}\\)\n\n\n\n\n\n\\(|z_{test}| &gt; z_{\\alpha/2}\\)\\(|t_{test}| &gt; t_{\\alpha/2, n-1}\\)"
  },
  {
    "objectID": "slides/10-ht-slides.html#the-new-treatment-is-effective-6",
    "href": "slides/10-ht-slides.html#the-new-treatment-is-effective-6",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\n A population of hypertension group is normal  and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population,  \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\) .\nGoal: Determine whether a new treatment is effective in reducing BP.\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\) \n The test statistic is \\(\\small t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}} =  \\frac{147.2 - 150}{5.5/\\sqrt{25}} = -2.55\\)\n The critical value is \\(\\small -t_{0.05, 25-1} = -t_{0.05, 24} = -1.711\\) \n\n\n\n We reject \\(H_0\\) if \\(t_{test} &lt; -t_{\\alpha, n-1}\\). Since \\(\\small t_{test} = -2.55 &lt; -1.711 = -t_{\\alpha, n-1}\\), we reject \\(H_0\\)."
  },
  {
    "objectID": "slides/10-ht-slides.html#step-4-p-find-the-p-value",
    "href": "slides/10-ht-slides.html#step-4-p-find-the-p-value",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 4-p: Find the P-Value",
    "text": "Step 4-p: Find the P-Value\n\nThe \\(p\\)-value measures the strength of the evidence against \\(H_0\\) provided by the data.\nThe smaller the \\(p\\)-value, the greater the evidence against \\(H_0\\).\n\nThe \\(p\\)-value is the probability of getting a test statistic value that is at least as extreme as the one obtained from the data, assuming that \\(H_0\\) is true. \\((\\mu = \\mu_0)\\)\n\nFor example, \\(p\\)-value \\(= P(Z \\ge z_{test} \\mid H_0)\\) for a right-tailed test.\n\n\n\n\nWhat is the chance of having evidence that is more unlikely to happen than our current collected evidence?"
  },
  {
    "objectID": "slides/10-ht-slides.html#p-value-illustration",
    "href": "slides/10-ht-slides.html#p-value-illustration",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "P-Value Illustration",
    "text": "P-Value Illustration"
  },
  {
    "objectID": "slides/10-ht-slides.html#the-new-treatment-is-effective-7",
    "href": "slides/10-ht-slides.html#the-new-treatment-is-effective-7",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\n A population of hypertension group is normal  and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population,  \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\) .\nGoal: Determine whether a new treatment is effective in reducing BP.\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\) \nThis is a left-tailed test, so the \\(p\\)-value is \\(P(T &lt; t_{test})=P(T &lt; -2.55) =\\) 0.01 \n\n\nStep 5-p:  We reject \\(H_0\\) if \\(p\\)-value &lt; \\(\\alpha\\). Since \\(p\\)-value \\(= 0.0088 &lt; 0.05 = \\alpha\\), we reject \\(H_0\\).\n\nStep 6:  There is sufficient evidence to support the claim that the new treatment is effective."
  },
  {
    "objectID": "slides/10-ht-slides.html#step-5-p-draw-a-conclusion-using-p-value-method",
    "href": "slides/10-ht-slides.html#step-5-p-draw-a-conclusion-using-p-value-method",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 5-p: Draw a Conclusion Using P-Value Method",
    "text": "Step 5-p: Draw a Conclusion Using P-Value Method\n\nIf \\(p\\)-value \\(\\le \\alpha\\) , reject \\(H_0\\).\nIf \\(p\\)-value \\(&gt; \\alpha\\), do not reject \\(H_0\\).\n\n\n\n\n\n\n\n\n\nCondition ¬† ¬†\n\nRight-tailed \\((H_1: \\mu &gt; \\mu_0)\\)\n\n\nLeft-tailed \\((H_1: \\mu &lt; \\mu_0)\\)\n\n\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\n\\(\\sigma\\) known\n\\(P(Z &gt; z_{test} \\mid H_0)\\)\n\\(P(Z &lt; z_{test} \\mid H_0)\\)\n\\(2P(Z &gt; \\,\\mid z_{test} \\mid \\, \\mid H_0)\\)\n\n\n\n\\(\\sigma\\) unknown\n\\(P(T &gt; t_{test} \\mid H_0)\\)\n\\(P(T &lt; t_{test} \\mid H_0)\\)\n\\(2P(T &gt; \\, \\mid t_{test} \\mid  \\, \\mid H_0)\\)\n\n\n\n\\(2P(Z &gt; z_{test} \\mid H_0)\\) if \\(z_{test}\\) on the right \\(2P(Z &lt; z_{test} \\mid H_0)\\) if \\(z_{test}\\) on the left \\(2P(T &gt; t_{test} \\mid H_0)\\) if \\(t_{test}\\) on the right \\(2P(T &lt; t_{test} \\mid H_0)\\) if \\(t_{test}\\) on the left"
  },
  {
    "objectID": "slides/10-ht-slides.html#the-new-treatment-is-effective-8",
    "href": "slides/10-ht-slides.html#the-new-treatment-is-effective-8",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\n A population of hypertension group is normal  and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population,  \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\) .\nGoal: Determine whether a new treatment is effective in reducing BP.\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\) \nThis is a left-tailed test, so the \\(p\\)-value is \\(P(T &lt; t_{test})=P(T &lt; -2.55) =\\) 0.01 \n\n\n\n We reject \\(H_0\\) if \\(p\\)-value &lt; \\(\\alpha\\). Since \\(p\\)-value \\(= 0.01 &lt; 0.05 = \\alpha\\), we reject \\(H_0\\).\n\n\nStep 6:  There is sufficient evidence to support the claim that the new treatment is effective."
  },
  {
    "objectID": "slides/10-ht-slides.html#both-methods-lead-to-the-same-conclusion",
    "href": "slides/10-ht-slides.html#both-methods-lead-to-the-same-conclusion",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Both Methods Lead to the Same Conclusion",
    "text": "Both Methods Lead to the Same Conclusion"
  },
  {
    "objectID": "slides/10-ht-slides.html#step-6-restate-the-conclusion-in-nontechnical-terms-and-address-the-original-claim",
    "href": "slides/10-ht-slides.html#step-6-restate-the-conclusion-in-nontechnical-terms-and-address-the-original-claim",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim",
    "text": "Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim\n\n\n\n\n\nhttps://www.drdawnwright.com/category/statistics/"
  },
  {
    "objectID": "slides/10-ht-slides.html#section-2",
    "href": "slides/10-ht-slides.html#section-2",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "",
    "text": "https://www.pinterest.com/pin/287878601159173631/\n\n\n\n\nNote! Why can‚Äôt we say we ‚Äúaccept the null‚Äù? The reason is that we are assuming the null hypothesis is true and trying to see if there is evidence against it. Therefore, the conclusion should be in terms of rejecting the null."
  },
  {
    "objectID": "slides/10-ht-slides.html#the-new-treatment-is-effective-9",
    "href": "slides/10-ht-slides.html#the-new-treatment-is-effective-9",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "The New Treatment is Effective?",
    "text": "The New Treatment is Effective?\n\n\n\n\n A population of hypertension group is normal  and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population,  \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\) .\nGoal: Determine whether a new treatment is effective in reducing BP.\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\) \n There is sufficient evidence to support the claim that the new treatment is effective."
  },
  {
    "objectID": "slides/10-ht-slides.html#example-calculation-in-r",
    "href": "slides/10-ht-slides.html#example-calculation-in-r",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Example Calculation in R",
    "text": "Example Calculation in R\n\n## create objects for any information we have\nalpha &lt;- 0.05; mu_0 &lt;- 150; \nx_bar &lt;- 147.2; s &lt;- 5.5; n &lt;- 25\n\n## Test statistic\n(t_test &lt;- (x_bar - mu_0) / (s / sqrt(n))) \n\n[1] -2.545\n\n## Critical value\n(t_cri &lt;- qt(alpha, df = n - 1, lower.tail = TRUE)) \n\n[1] -1.711\n\n## p-value\n(p_val &lt;- pt(t_test, df = n - 1, lower.tail = TRUE)) \n\n[1] 0.008878\n\n\n\n\n[1] \"Reject H0 by crit. value\"\n\n\n[1] \"Reject H0 by p-value\""
  },
  {
    "objectID": "slides/10-ht-slides.html#example-2-two-tailed-z-test",
    "href": "slides/10-ht-slides.html#example-2-two-tailed-z-test",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Example 2: Two-tailed z-test",
    "text": "Example 2: Two-tailed z-test\n\n\nThe milk price of a gallon of 2% milk is normally distributed with standard deviation of $0.10.\n\nLast week the mean milk price was 2.78. This week, based on a sample of size 25, the sample mean milk price \\(\\overline{x} = 2.80\\).\nUnder \\(\\alpha = 0.05\\), determine if this week the mean price is different.\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1: This is a \\(H_1\\) claim  \\(\\small \\begin{align}&H_0: \\mu = 2.78 \\\\ &H_1: \\mu \\ne 2.78 \\end{align}\\) \n\n\n\n\n\nStep 2:  \\(\\small \\alpha = 0.05\\) \n\n\n\n\n\nStep 3:  \\(\\small z_{test} = \\frac{\\overline{x} - \\mu_0}{\\sigma/\\sqrt{n}} =  \\frac{2.8 - 2.78}{0.1/\\sqrt{25}} = 1.00\\) \n\n\n\n\n\nStep 4-c:  \\(\\small z_{0.05/2} = 1.96\\). \n\n\n\n\n\nStep 5-c: This is a two-tailed test and we reject \\(H_0\\) if \\(|z_{test}| &gt; z_{\\alpha/2}\\). Since \\(\\small |z_{test}| = 1 &lt; 1.96 = z_{\\alpha/2}\\), we DO NOT reject \\(H_0\\)."
  },
  {
    "objectID": "slides/10-ht-slides.html#example-2-two-tailed-z-test-contd",
    "href": "slides/10-ht-slides.html#example-2-two-tailed-z-test-contd",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Example 2: two-tailed z-test Cont‚Äôd",
    "text": "Example 2: two-tailed z-test Cont‚Äôd\n\nStep 4-p: This is a two-tailed test, and the test statistic is on the right \\((&gt; 0)\\), so the \\(p\\)-value is \\(2P(Z &gt; z_{test})=\\) 0.317 \n\n\n\n\nStep 5-p:  We reject \\(H_0\\) if \\(p\\)-value &lt; \\(\\alpha\\). Since \\(p\\)-value \\(= 0.317 &gt; 0.05 = \\alpha\\), we DO NOT reject \\(H_0\\).\n\n\n\n\n\nStep 6:  There is insufficient evidence to support the claim that this week the mean milk price is different from the price last week."
  },
  {
    "objectID": "slides/10-ht-slides.html#example-2-calculation-in-r",
    "href": "slides/10-ht-slides.html#example-2-calculation-in-r",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Example 2 Calculation in R",
    "text": "Example 2 Calculation in R\n\n## create objects to be used\nalpha &lt;- 0.05; mu_0 &lt;- 2.78; \nx_bar &lt;- 2.8; sigma &lt;- 0.1; n &lt;- 25\n\n## Test statistic\n(z_test &lt;- (x_bar - mu_0) / (sigma / sqrt(n))) \n\n[1] 1\n\n## Critical value\n(z_crit &lt;- qnorm(alpha/2, lower.tail = FALSE)) \n\n[1] 1.96\n\n## p-value\n(p_val &lt;- 2 * pnorm(z_test, lower.tail = FALSE)) \n\n[1] 0.3173\n\n\n\n\n[1] \"Do not reject H0 by critical value method\"\n\n\n[1] \"Do not reject H0 by p-value method\""
  },
  {
    "objectID": "slides/10-ht-slides.html#testing-summary",
    "href": "slides/10-ht-slides.html#testing-summary",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Testing Summary",
    "text": "Testing Summary\n\n\n\n\n\n\n\n\nNumerical Data, \\(\\sigma\\) known\nNumerical Data, \\(\\sigma\\)  unknown \n\n\n\n\nParameter of Interest\nPopulation Mean \\(\\mu\\)\n\nPopulation Mean \\(\\mu\\)\n\n\n\nTest Type\nOne sample \\(\\color{blue}{z}\\) test \\(H_0: \\mu = \\mu_0\\)\n\nOne sample \\(\\color{blue}{t}\\) test \\(H_0: \\mu = \\mu_0\\)\n\n\n\nConfidence Interval\n\\(\\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\)\n\\(\\bar{x} \\pm t_{\\alpha/2, n-1} \\frac{\\color{blue}{s}}{\\sqrt{n}}\\)\n\n\nTest Stat under \\(H_0\\)\n\\(z_{test} = \\frac{\\overline{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\)\n\\(t_{test} = \\frac{\\overline{x} - \\mu_0}{\\frac{\\color{blue}{s}}{\\sqrt{n}}}\\)\n\n\n\\(p\\)-value under \\(H_0\\)\n\n\\(H_1: \\mu &lt; \\mu_0\\) \\(p\\)-value \\(=P(Z \\le z_{test})\\)\n\n\n\\(H_1: \\mu &lt; \\mu_0\\) \\(p\\)-value \\(=P(T_{n-1} \\le t_{test})\\)\n\n\n\n\n\n\\(H_1: \\mu &gt; \\mu_0\\) \\(p\\)-value \\(=P(Z \\ge z_{test})\\)\n\n\n\\(H_1: \\mu &lt; \\mu_0\\) \\(p\\)-value \\(=P(T_{n-1} \\ge t_{test})\\)\n\n\n\n\n\n\\(H_1: \\mu \\ne \\mu_0\\) \\(p\\)-value \\(=2P(Z \\ge \\, \\mid z_{test}\\mid)\\)\n\n\n\\(H_1: \\mu \\ne \\mu_0\\) \\(p\\)-value \\(=2P(T_{n-1} \\ge  \\, \\mid t_{test} \\mid)\\)"
  },
  {
    "objectID": "slides/10-ht-slides.html#type-i-and-type-ii-errors",
    "href": "slides/10-ht-slides.html#type-i-and-type-ii-errors",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\n\n\nDecision\n\n\\(H_0\\) is true\n\n\\(H_0\\) is false\n\n\n\nReject \\(H_0\\)\n\nType I error\nCorrect decision\n\n\nDo not reject \\(H_0\\)\n\nCorrect decision\nType II error\n\n\n\n\n\nBack to the crime example: \\(H_0:\\) The person is  not guilty  v.s. \\(H_1:\\) The person is  guilty \n\n\n\n\n\n\n\n\nDecision\nTruth is the person innocent\nTruth is the person guilty\n\n\n\nJury decides the person guilty\nType I error\nCorrect decision\n\n\nJury decides the person innocent\nCorrect decision\nType II error\n\n\n\n\nIn practice, we will not know for certain if we made the correct decision or if we made one of these two errors."
  },
  {
    "objectID": "slides/10-ht-slides.html#type-i-and-type-ii-errors-1",
    "href": "slides/10-ht-slides.html#type-i-and-type-ii-errors-1",
    "title": "Statistical Inference: Hypothesis Testing \n",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\n\n\\(\\alpha = P(\\text{type I error}) = P(\\text{rejecting } H_0 \\text{ when } H_0 \\text{ is true})\\)\n\\(\\beta = P(\\text{type II error}) = P(\\text{failing to reject } H_0 \\text{ when } H_0 \\text{ is false})\\)\n\n\n\n\n\n\nhttps://www.statisticssolutions.com/wp-content/uploads/2017/12/rachnovblog.jpg"
  },
  {
    "objectID": "slides/09-ci.html#inference-framework",
    "href": "slides/09-ci.html#inference-framework",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Inference Framework",
    "text": "Inference Framework\n\nInferential statistics uses sample data to learn about an unknown population.\n\nIdea: Assume the target population follows some distribution but with unknown parameters.\n\n Assume the population is normally distributed, but don‚Äôt know its mean and/or variance. Marquette students‚Äô mean GPA for example. \n\n\nGoal: Learning the unknown parameters of the assumed population distribution.\nTwo approaches in parameter learning: Estimation and Hypothesis testing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe finally are going to study inferential statistics that uses sample data to learn about an unknown population. (Getting harder! Be careful!)\nWe are interested in learning unknown parameters of the assumed population distribution, since knowledge of the parameters yield knowledge of the entire population.\nIn parameter learning, we consider two approaches, estimation and hypothesis testing.\n\n Learn the mean and/or variance of the normal distribution."
  },
  {
    "objectID": "slides/09-ci.html#point-estimator",
    "href": "slides/09-ci.html#point-estimator",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Point Estimator",
    "text": "Point Estimator\n\n\n\nIf you can use only one single number to guess the unknown population mean \\(\\mu\\), what would you like to use?\n\n. . .\n\nThe one single point used to estimate the unknown parameter is called a point estimator.\n\n. . .\n\n\nA point estimator is any function of data \\((X_1, X_2, \\dots, X_n)\\) (Before actually being collected).\n\n\nAny statistic is a point estimator.\n\n\nA point estimate is a value of a point estimator used to estimate a population parameter. (A value calculated using the collected data).\nSample mean \\((\\overline{X})\\) is a statistic and a point estimator for the population mean \\(\\mu\\).\n\n\n\n\n\nA point estimator is any function of a sample \\((X_1, X_2, \\dots, X_n)\\).\n\n\nAny statistic is a point estimator.\n\n\nA point estimate is a value of a point estimator used to estimate a population parameter.\nSample mean \\((\\overline{X})\\) is a statistic and a point estimator for the population mean \\(\\mu\\)."
  },
  {
    "objectID": "slides/09-ci.html#sample-mean-as-an-point-estimator",
    "href": "slides/09-ci.html#sample-mean-as-an-point-estimator",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Sample Mean as an Point Estimator",
    "text": "Sample Mean as an Point Estimator\n\nDraw 5 values from the population that follows \\(N(3.2, 0.5)\\) as sample data \\((x_1, x_2, x_3, x_4, x_5)\\).\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nsample mean\n\n\n2.88\n2.94\n3.09\n2.66\n2.38\n2.79\n\n\n\n\n\n\n\\(\\mu = 3.2\\), and we use the point estimate \\(\\overline{x}=\\) 2.79 to estimate it.\n\n\nWhy \\(\\overline{x}\\) is not equal to \\(\\mu\\)?\n\n. . .\nDue to its randomness nature\n\n\n\n\n\n\n\n\n\n\nDue to the randomness nature of drawing a sample value from the population distribution, we do not expect the statistic to be the same as the corresponding parameter."
  },
  {
    "objectID": "slides/09-ci.html#variability-in-estimates",
    "href": "slides/09-ci.html#variability-in-estimates",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Variability in Estimates",
    "text": "Variability in Estimates\n\nIf another sample of size \\(5\\) is drawn from the same population:\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nsample mean\n\n\n2.35\n3.4\n3.97\n1.54\n3.5\n2.95\n\n\n\n\n\nThe second sample mean \\(\\overline{x} =\\) 2.95 is different from the first one.\n\n\nWhy the first sample and the second sample give us different sample means?\n\n. . .\nA point estimator has its own sampling distribution\n\n\n\n\n\n\n\n\n\n\nA statistic value (point estimate) calculated from a sample varies from sample to sample because a point estimator is also a random variable, and it has its own sampling distribution! (why we learn sampling distribution. it measures the variation of a point estimator)"
  },
  {
    "objectID": "slides/09-ci.html#why-point-estimates-are-not-enough",
    "href": "slides/09-ci.html#why-point-estimates-are-not-enough",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Why Point Estimates Are Not Enough",
    "text": "Why Point Estimates Are Not Enough\n\nIf you want to estimate \\(\\mu\\), do you prefer to report a range of values the parameter might be in, or a single estimate like \\(\\overline{x}\\)?\n\n. . .\n\nIf you want to catch a fish, do you prefer a spear or a net?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\nDue to variation of \\(\\overline{X}\\), if we report a point estimate \\(\\overline{x}\\), we probably won‚Äôt hit the exact \\(\\mu\\).\nIf we report a range of plausible values, we have a better shot at capturing the parameter!\n\n\n\nthe estimate \\(\\overline{x}\\) may not be precise or reliable."
  },
  {
    "objectID": "slides/09-ci.html#confidence-intervals",
    "href": "slides/09-ci.html#confidence-intervals",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nA plausible range of values for \\(\\mu\\) is called a confidence interval (CI).\n\nTo construct a CI we need to quantify the variability of our sample mean.  \n\nQuantifying this uncertainty requires a measurement of how much we would expect the sample statistic to vary from sample to sample.\n\nThat is the variance of the sampling distribution of the sample mean!\n\n\n\n\n\nTo construct a CI we need to quantify the variability of our sample mean because the variability determines the upper and lower bound of the range of the plausible values.\nThe variability of the sample mean determines the size of CI.\nThis range depends on how precise and reliable our \\(\\overline{X}\\) is as an estimate of \\(\\mu\\).\nIf the variation of our sample mean is pretty large, meaning that every time we collect a sample, the sample mean varies a lot from one to another, then we are more uncertain about the value of \\(\\mu\\).\nAnd that mean the plausible range of values for \\(\\mu\\) will be pretty wide, or the CI will be much wider.\n\n\n. . .\n\nüëâ The larger variation of \\(\\overline{X}\\) is, the wider the CI for \\(\\mu\\) will be, given the same ‚Äúlevel of confidence‚Äù.\n\n. . .\n\nDo we know the variance of \\(\\overline{X}\\)?\n\n. . .\n\nBy CLT, \\(\\overline{X} \\sim N(\\mu, \\sigma^2/n)\\) regardless of what the population distribution is."
  },
  {
    "objectID": "slides/09-ci.html#confidence-interval-is-for-a-parameter",
    "href": "slides/09-ci.html#confidence-interval-is-for-a-parameter",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Confidence Interval Is for a Parameter",
    "text": "Confidence Interval Is for a Parameter\n\n\nA confidence interval is for a parameter, NOT a statistic.\n\nUse the sample mean to form a confidence interval for the population mean.\n\n\nWe never say ‚ÄúThe confidence interval of the sample mean \\(\\overline{X}\\) is ‚Ä¶‚Äù\nWe say ‚ÄúThe confidence interval for the true population mean \\(\\mu\\) is ‚Ä¶‚Äù\n\n. . .\n\nIn general, a confidence interval for \\(\\mu\\) has the form\n\n\n\n\\(\\large \\overline{x} \\pm m = (\\overline{x} - m, \\overline{x} + m)\\)\n\n\n\nThe \\(m\\) is called margin of error.\n\\(\\overline{x} - m\\) is the lower bound and \\(\\overline{x} + m\\) is the upper bound of the confidence interval.\nThe point estimate \\(\\overline{x}\\) and margin of error \\(m\\) can be obtained from known quantities and our data once sampled.\n\n\n\nUse the sample variance to form a confidence interval for the population variance."
  },
  {
    "objectID": "slides/09-ci.html#precision-vs.-reliability",
    "href": "slides/09-ci.html#precision-vs.-reliability",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Precision vs.¬†Reliability",
    "text": "Precision vs.¬†Reliability\n\nIf we want to be very certain that we capture \\(\\mu\\), should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?\n\n\n\nHere is a question. If we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval?\nWe use a wider interval because a wider interval is more likely to capture the population parameter value. So a 99% confidence interval is wider than a 95% confidence interval.\nBut What drawbacks are associated with using a wider interval?\n\n\n. . .\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs look at this comic.\nI can say I am 100% confident that your exam 1 score is between 0 and 100. Am I right? Yes. But do I provide helpful information? Absolutely not, the interval includes every possible score of the exam. The interval is too wide to be helpful.\n\n\n. . .\n\nWith the sample size fixed, precision and reliability have a trade-off relationship.\n\n\n.question[ How can we get best of both worlds ‚Äì high precision and high reliability/accuracy?] - How can we get best of both worlds ‚Äì high precision and high reliability/accuracy, meaning short interval with high level of confidence. - What we need is larger sample size, given that the sample quality is good. - Easy statement, but sometimes it‚Äôs hard to collect more samples."
  },
  {
    "objectID": "slides/09-ci.html#alpha100-confidence-intervals",
    "href": "slides/09-ci.html#alpha100-confidence-intervals",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "\n\\((1 - \\alpha)100\\%\\) Confidence Intervals",
    "text": "\\((1 - \\alpha)100\\%\\) Confidence Intervals\n\nThe confidence level \\(1-\\alpha\\): the proportion of times that the CI contains the population parameter, assuming that the estimation process is repeated a large number of times.\n\nThe common choices for the confidence level are\n\n90% \\((\\alpha = 0.10)\\)\n\n95% \\((\\alpha = 0.05)\\)\n\n99% \\((\\alpha = 0.01)\\)\n\n\n\n95% is the most common level because of good balance between precision (width of the CI) and reliability (confidence level)\n\n. . .\n\n High reliability and Low precision. I am 100% confident that the mean height of Marquette students is between 3‚Äô0‚Äù and 8‚Äô0‚Äù.  duh‚Ä¶ü§∑\n Low reliability and High precision. I am 20% confident that mean height of Marquette students is between 5‚Äô6‚Äù and 5‚Äô7‚Äù.  far from it‚Ä¶üôÖ\n\n\n\nA confidence interval is associated with a confidence level.\nThe confidence level (often expressed as the percentage value, such as 95%)"
  },
  {
    "objectID": "slides/09-ci.html#confidence-intervals-for-mu-z-score",
    "href": "slides/09-ci.html#confidence-intervals-for-mu-z-score",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "\n\\(95\\%\\) Confidence Intervals for \\(\\mu\\): Z-score",
    "text": "\\(95\\%\\) Confidence Intervals for \\(\\mu\\): Z-score\n\n\n\n\\(\\alpha = 0.05\\)\nStart with the sampling distribution of \\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\)\n\\(\\overline{x}\\) will be within 1.96 SDs of the population mean \\(\\mu\\) \\(95\\%\\) of the time.\nThe \\(z\\)-score of 1.96 is associated with 2.5% area to the right, and called a critical value denoted as \\(z_{0.025}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(and Z = -1.96 has 2.5% area to the left); -At a distance of zŒ±/2 standard deviations to the right of Œº, there is an area of Œ±/2 under the normal density curve."
  },
  {
    "objectID": "slides/09-ci.html#confidence-intervals-for-mu-probability",
    "href": "slides/09-ci.html#confidence-intervals-for-mu-probability",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "\n\\(95\\%\\) Confidence Intervals for \\(\\mu\\): Probability",
    "text": "\\(95\\%\\) Confidence Intervals for \\(\\mu\\): Probability\n\\[P\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\overline{X} &lt; \\mu + 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\\]\n\n\n\nIs the interval \\(\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}}, \\mu+1.96\\frac{\\sigma}{\\sqrt{n}} \\right)\\) our confidence interval?\n\n\n‚ùå No! We don‚Äôt know \\(\\mu\\), the quantity we like to estimate!\nBut we‚Äôre almost there!"
  },
  {
    "objectID": "slides/09-ci.html#confidence-intervals-for-mu-formula",
    "href": "slides/09-ci.html#confidence-intervals-for-mu-formula",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "\n\\(95\\%\\) Confidence Intervals for \\(\\mu\\): Formula",
    "text": "\\(95\\%\\) Confidence Intervals for \\(\\mu\\): Formula\n\n\n\n\n\\[\n\\small P\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\overline{X} &lt; \\mu + 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95 \\iff\nP\\left( \\boxed{\\overline{X}-1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\overline{X} + 1.96\\frac{\\sigma}{\\sqrt{n}}} \\right) = 0.95\\]\n\n\n\n With sample data of size \\(n\\), \\(\\left(\\overline{x}-1.96\\frac{\\sigma}{\\sqrt{n}},  \\overline{x} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right)\\) is our \\(95\\%\\) CI for \\(\\mu\\) if \\(\\sigma\\) is known to us! \nThe margin of error \\(m = 1.96\\frac{\\sigma}{\\sqrt{n}}\\)."
  },
  {
    "objectID": "slides/09-ci.html#confidence-intervals-for-mu-when-sigma-is-known",
    "href": "slides/09-ci.html#confidence-intervals-for-mu-when-sigma-is-known",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known",
    "text": "Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known\nRequirements for estimating \\(\\mu\\) when \\(\\sigma\\) is known:\n\n\nüëâ The sample should be a random sample, i.e.¬†All data \\(X_i\\) are drawn from the same population, and \\(X_i\\) and \\(X_j\\) are independent.\n\n Any methods in the course are based on a random sample \n\n\nüëâ The population standard deviation \\(\\sigma\\) is known.\n\nüëâ The population is either normally distributed or \\(n &gt; 30\\) or both, i.e., \\(X_i \\sim N(\\mu, \\sigma^2)\\).\n\n \\(n &gt; 30\\) allows CLT to be applied and hence normality is satisfied."
  },
  {
    "objectID": "slides/09-ci.html#alpha100-confidence-intervals-for-mu",
    "href": "slides/09-ci.html#alpha100-confidence-intervals-for-mu",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "\n\\((1-\\alpha)100\\%\\) Confidence Intervals for \\(\\mu\\):",
    "text": "\\((1-\\alpha)100\\%\\) Confidence Intervals for \\(\\mu\\):\n\n\n\n\n\n\n\n\n\n\n\n \\(\\left(\\overline{x}-1.96\\frac{\\sigma}{\\sqrt{n}},  \\overline{x} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right)\\)   \\(\\left(\\overline{x}-z_{0.025}\\frac{\\sigma}{\\sqrt{n}},  \\overline{x} + z_{0.025}\\frac{\\sigma}{\\sqrt{n}}\\right)\\) \n\n\n\n\n\n\n\n\n\n\n\n \\(\\left(\\overline{x}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}},  \\overline{x} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right)\\)"
  },
  {
    "objectID": "slides/09-ci.html#confidence-intervals-for-mu-when-sigma-is-known-1",
    "href": "slides/09-ci.html#confidence-intervals-for-mu-when-sigma-is-known-1",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known",
    "text": "Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known\nProcedures for constructing a confidence interval for \\(\\mu\\) when \\(\\sigma\\) known:\n\nCheck that the requirements are satisfied.\n\nDecide \\(\\alpha\\) or confidence level \\((1 - \\alpha)\\).\n\nFind the critical value \\(z_{\\alpha/2}\\).\n\nEvaluate margin of error \\(m = z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\)\n\nConstruct the \\((1 - \\alpha)100\\%\\) CI for \\(\\mu\\) using sample mean \\(\\overline{x}\\) and margin of error \\(m\\):\n\n \\[\\left( \\overline{x} -z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\, \\overline{x} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right)\\]"
  },
  {
    "objectID": "slides/09-ci.html#example-ci-for-mu-when-sigma-is-known",
    "href": "slides/09-ci.html#example-ci-for-mu-when-sigma-is-known",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Example: CI for \\(\\mu\\) When \\(\\sigma\\) is Known",
    "text": "Example: CI for \\(\\mu\\) When \\(\\sigma\\) is Known\n\n\nWe want to know the mean systolic blood pressure (SBP) of a population.\n\nAssume that the population distribution is normal with the standard deviation of 5 mmHg.\nWe have a random sample of 16 subjects of this population with mean 121.5.\nEstimate the mean SBP with a 95% confidence interval.\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\nRequirements:  Normality is assumed, \\(\\sigma = 5\\) is known and a random sample is collected.\n\n\n. . .\n\nDecide \\(\\alpha\\):  \\(\\alpha = 0.05\\) \n\n\n. . .\n\nFind the critical value \\(z_{\\alpha/2}\\):  \\(z_{\\alpha/2} = z_{0.025} = 1.96\\) \n\n\n. . .\n\nEvaluate margin of error \\(m = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\):  \\(m = (1.96) \\frac{5}{\\sqrt{16}} = 2.45\\) \n\n\n. . .\n\nConstruct the \\((1 - \\alpha)100\\%\\) CI:  The 95% CI for the mean SBP is \\(\\overline{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} = (121.5 -2.45, 121.5 + 2.45) = (119.05, 123.95)\\)"
  },
  {
    "objectID": "slides/09-ci.html#computation-in-r",
    "href": "slides/09-ci.html#computation-in-r",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Computation in R",
    "text": "Computation in R\n\n## save all information we have\nalpha &lt;- 0.05\nn &lt;- 16\nx_bar &lt;- 121.5\nsig &lt;- 5\n\n## 95% CI\n## z-critical value\n(cri_z &lt;- qnorm(p = alpha / 2, lower.tail = FALSE))  \n\n[1] 1.96\n\n## margin of error\n(m_z &lt;- cri_z * (sig / sqrt(n)))  \n\n[1] 2.45\n\n## 95% CI for mu when sigma is known\nx_bar + c(-1, 1) * m_z  \n\n[1] 119.1 123.9\n\n\n. . .\n\nConstruct a 99% CI for the mean SBP. Do you expect to have a wider or narrower interval? Why?"
  },
  {
    "objectID": "slides/09-ci.html#interpreting-a-confidence-interval",
    "href": "slides/09-ci.html#interpreting-a-confidence-interval",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Interpreting a Confidence Interval",
    "text": "Interpreting a Confidence Interval\n\n ‚ÄúWe are 95% confident that the mean SBP lies between 119.1 mm and 123.9 mm.‚Äù \nSuppose we were able to collect our dataset many times and build the corresponding CIs.\n\nWe would expect about 95% of those intervals would contain the true population parameter, here the mean systolic blood pressure.\n\n Remember: \\(\\overline{x}\\) varies from sample to sample, so does its corresponding CI.\n\n\nWe never know if in fact 95% of them do, or whether any interval contains the true parameter!"
  },
  {
    "objectID": "slides/09-ci.html#generate-100-confidence-intervals-assuming-mu-120.",
    "href": "slides/09-ci.html#generate-100-confidence-intervals-assuming-mu-120.",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Generate 100 Confidence Intervals Assuming \\(\\mu = 120\\).",
    "text": "Generate 100 Confidence Intervals Assuming \\(\\mu = 120\\)."
  },
  {
    "objectID": "slides/09-ci.html#interpreting-a-confidence-interval-do-not-say",
    "href": "slides/09-ci.html#interpreting-a-confidence-interval-do-not-say",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Interpreting a Confidence Interval DO NOT SAY",
    "text": "Interpreting a Confidence Interval DO NOT SAY\n\n\nWRONG ‚ùå ‚ÄúThere is a 95% chance/probability that the true population mean will fall between 119.1 mm and 123.9 mm.‚Äù\n\n\nWRONG ‚ùå ‚ÄúThe probability that the true population mean falls between 119.1 mm and 123.9 mm is 95%.‚Äù\n\n\n. . .\n\n üëâ The sample mean is a random variable with a sampling distribution, so it makes sense to compute a probability of it being in some interval. \n üëâ The population mean is unknown and FIXED. We cannot assign or compute any probability of it. \n\n. . .\n\nAnother inference method, Bayesian inference, treats \\(\\mu\\) as a random variable and therefore we can compute any probability associated with it. (MATH 4790 Bayesian Statistics)"
  },
  {
    "objectID": "slides/09-ci.html#confidence-intervals-for-mu-when-sigma-is-unknown",
    "href": "slides/09-ci.html#confidence-intervals-for-mu-when-sigma-is-unknown",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Unknown",
    "text": "Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Unknown\n\n\n\\(\\sigma^2 = \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{N}\\), \\(N\\) is the population size.\nIt‚Äôs rare that we do not know \\(\\mu\\), but know \\(\\sigma\\).\nWe use the Student t distribution to construct a confidence interval for \\(\\mu\\) when \\(\\sigma\\) is unknown.\n\nStill need\n\nRandom sample\nPopulation is normally distributed and/or \\(n &gt; 30\\).\n\n\n\n\nWhat is a natural estimator for the unknown \\(\\sigma\\)?\n\n. . .\n\nSince \\(\\sigma\\) is unknown, we use the sample standard deviation \\(S = \\sqrt{\\frac{\\sum_{i=1}^{n}(X_i - \\overline{X})^2}{n-1}}\\) instead when constructing the CI.\n\n\n\nSo far we estimate unknown mean \\(\\mu\\) with known standard deviation \\(\\sigma\\).\n\n\\(\\sigma^2 = \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{N}\\), \\(N\\) is the population size.\nIt‚Äôs rare that we do not know \\(\\mu\\), but know \\(\\sigma\\).\nWe use a so-called Student t distribution to construct a confidence intervals for \\(\\mu\\) when \\(\\sigma\\) is unknown.\nWe still need to satisfy the following:\n\nRandom sample\nPopulation is normally distributed and/or \\(n &gt; 30\\).\n\n\n\n.question[ What is a natural estimator for the unknown \\(\\sigma\\)?]\n\nSince \\(\\sigma\\) is unknown, we use the sample standard deviation \\(S = \\sqrt{\\frac{\\sum_{i=1}^{n}(X_i - \\overline{X})^2}{n-1}}\\) instead when constructing the CI."
  },
  {
    "objectID": "slides/09-ci.html#student-t-distribution",
    "href": "slides/09-ci.html#student-t-distribution",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Student t Distribution",
    "text": "Student t Distribution\n\nIf the population is normally distributed or \\(n &gt; 30\\),\n\n\\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\)\n\\(Z = \\frac{\\overline{X} - \\mu}{\\color{red}\\sigma/\\sqrt{n} } \\sim N(0, 1)\\)\n \\(T = \\frac{\\overline{X} - \\mu}{\\color{red}S/\\sqrt{n} } \\sim t_{n-1}\\) \n\n\\(t_{n-1}\\) denotes the Student t distribution with degrees of freedom (df) \\(n-1\\)."
  },
  {
    "objectID": "slides/09-ci.html#properties-of-student-t-distribution",
    "href": "slides/09-ci.html#properties-of-student-t-distribution",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Properties of Student t Distribution",
    "text": "Properties of Student t Distribution\n\nSymmetric about the mean 0 and bell-shaped as \\(N(0, 1)\\).\nMore variability than \\(N(0, 1)\\) (heavier tails and lower peak).\nThe variability is different for different sample sizes (degrees of freedom).\nAs \\(n \\rightarrow \\infty\\) \\((df \\rightarrow \\infty)\\), the Student t distribution approaches to \\(N(0, 1)\\)."
  },
  {
    "objectID": "slides/09-ci.html#critical-values-of-t_alpha2-n-1",
    "href": "slides/09-ci.html#critical-values-of-t_alpha2-n-1",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Critical Values of \\(t_{\\alpha/2, n-1}\\)\n",
    "text": "Critical Values of \\(t_{\\alpha/2, n-1}\\)\n\n\nWhen \\(\\sigma\\) is unknown, we use \\(t_{\\alpha/2, n-1}\\) as the critical value, instead of \\(z_{\\alpha/2}\\).\n\n\n\n\n\n\n\n\n\n. . .\n\nWith the same \\(\\alpha\\), \\(t_{\\alpha, n-1}\\) or \\(z_{\\alpha}\\) is larger?"
  },
  {
    "objectID": "slides/09-ci.html#critical-values-of-t_alpha2-n-1-1",
    "href": "slides/09-ci.html#critical-values-of-t_alpha2-n-1-1",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Critical Values of \\(t_{\\alpha/2, n-1}\\)\n",
    "text": "Critical Values of \\(t_{\\alpha/2, n-1}\\)\n\n\nGiven the same confidence level \\(1-\\alpha\\), \\(t_{\\alpha/2, n-1} &gt; z_{\\alpha/2}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevel\nt df = 5\nt df = 15\nt df = 30\nt df = 1000\nt df = inf\nz\n\n\n\n90%\n2.02\n1.75\n1.70\n1.65\n1.64\n1.64\n\n\n95%\n2.57\n2.13\n2.04\n1.96\n1.96\n1.96\n\n\n99%\n4.03\n2.95\n2.75\n2.58\n2.58\n2.58"
  },
  {
    "objectID": "slides/09-ci.html#ci-for-mu-when-sigma-is-unknown",
    "href": "slides/09-ci.html#ci-for-mu-when-sigma-is-unknown",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "CI for \\(\\mu\\) When \\(\\sigma\\) is Unknown",
    "text": "CI for \\(\\mu\\) When \\(\\sigma\\) is Unknown\n\nThe \\((1-\\alpha)100\\%\\) confidence interval for \\(\\mu\\) when \\(\\sigma\\) is unknown is  \\[\\left(\\overline{x} - t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}, \\overline{x} + t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\right)\\] \nGiven the same confidence level \\(1-\\alpha\\), \\(t_{\\alpha/2, n-1} &gt; z_{\\alpha/2}\\).\n\n\nWe are more ‚Äúuncertain‚Äù when doing inference about \\(\\mu\\) because we also don‚Äôt have information about \\(\\sigma\\), and replacing it with \\(s\\) adds additional uncertainty."
  },
  {
    "objectID": "slides/09-ci.html#computation-in-r-t-interval",
    "href": "slides/09-ci.html#computation-in-r-t-interval",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Computation in R (t interval)",
    "text": "Computation in R (t interval)\n\nBack to the systolic blood pressure (SBP) example. We have \\(n=16\\) and \\(\\overline{x} = 121.5\\).\nEstimate the mean SBP with a 95% confidence interval with unknown \\(\\sigma\\) and \\(s = 5\\).\n\n\nalpha &lt;- 0.05\nn &lt;- 16\nx_bar &lt;- 121.5\ns &lt;- 5  ## sigma is unknown and s = 5\n\n## t-critical value\n(cri_t &lt;- qt(p = alpha / 2, df = n - 1, lower.tail = FALSE)) \n\n[1] 2.131\n\n## margin of error\n(m_t &lt;- cri_t * (s / sqrt(n)))  \n\n[1] 2.664\n\n## 95% CI for mu when sigma is unknown\nx_bar + c(-1, 1) * m_t  \n\n[1] 118.8 124.2"
  },
  {
    "objectID": "slides/09-ci.html#summary",
    "href": "slides/09-ci.html#summary",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\nNumerical Data, \\(\\sigma\\) known\nNumerical Data, \\(\\sigma\\) unknown\n\n\n\nParameter of Interest\nPopulation Mean \\(\\mu\\)\n\nPopulation Mean \\(\\mu\\)\n\n\n\nConfidence Interval\n\\(\\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\)\n\\(\\bar{x} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\)\n\n\n\n. . .\n\nRemember to check if the population is normally distributed or \\(n&gt;30\\).\n\n. . .\n\nWhat if the population is not normal and \\(n \\le 30\\)?\n\n. . .\n\nUse a so-called nonparametric method, for example bootstrapping. (Your project?!)"
  },
  {
    "objectID": "slides/09-ci-slides.html#inference-framework",
    "href": "slides/09-ci-slides.html#inference-framework",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Inference Framework",
    "text": "Inference Framework\n\nInferential statistics uses sample data to learn about an unknown population.\n\nIdea: Assume the target population follows some distribution but with unknown parameters.\n\n Assume the population is normally distributed, but don‚Äôt know its mean and/or variance. Marquette students‚Äô mean GPA for example. \n\n\nGoal: Learning the unknown parameters of the assumed population distribution.\nTwo approaches in parameter learning: Estimation and Hypothesis testing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe finally are going to study inferential statistics that uses sample data to learn about an unknown population. (Getting harder! Be careful!)\nWe are interested in learning unknown parameters of the assumed population distribution, since knowledge of the parameters yield knowledge of the entire population.\nIn parameter learning, we consider two approaches, estimation and hypothesis testing.\n\n Learn the mean and/or variance of the normal distribution."
  },
  {
    "objectID": "slides/09-ci-slides.html#point-estimator",
    "href": "slides/09-ci-slides.html#point-estimator",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Point Estimator",
    "text": "Point Estimator\n\n\n\nIf you can use only one single number to guess the unknown population mean \\(\\mu\\), what would you like to use?\n\n\n\nThe one single point used to estimate the unknown parameter is called a point estimator.\n\n\n\n\n\nA point estimator is any function of data \\((X_1, X_2, \\dots, X_n)\\) (Before actually being collected).\n\n\nAny statistic is a point estimator.\n\n\nA point estimate is a value of a point estimator used to estimate a population parameter. (A value calculated using the collected data).\nSample mean \\((\\overline{X})\\) is a statistic and a point estimator for the population mean \\(\\mu\\).\n\n\nA point estimator is any function of a sample \\((X_1, X_2, \\dots, X_n)\\).\n\n\nAny statistic is a point estimator.\n\n\nA point estimate is a value of a point estimator used to estimate a population parameter.\nSample mean \\((\\overline{X})\\) is a statistic and a point estimator for the population mean \\(\\mu\\)."
  },
  {
    "objectID": "slides/09-ci-slides.html#sample-mean-as-an-point-estimator",
    "href": "slides/09-ci-slides.html#sample-mean-as-an-point-estimator",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Sample Mean as an Point Estimator",
    "text": "Sample Mean as an Point Estimator\n\nDraw 5 values from the population that follows \\(N(3.2, 0.5)\\) as sample data \\((x_1, x_2, x_3, x_4, x_5)\\).\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nsample mean\n\n\n2.88\n2.94\n3.09\n2.66\n2.38\n2.79\n\n\n\n\n\n\n\\(\\mu = 3.2\\), and we use the point estimate \\(\\overline{x}=\\) 2.79 to estimate it.\n\n\nWhy \\(\\overline{x}\\) is not equal to \\(\\mu\\)?\n\n\nDue to its randomness nature\n\n\n\n\n\n\n\n\n\nDue to the randomness nature of drawing a sample value from the population distribution, we do not expect the statistic to be the same as the corresponding parameter."
  },
  {
    "objectID": "slides/09-ci-slides.html#variability-in-estimates",
    "href": "slides/09-ci-slides.html#variability-in-estimates",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Variability in Estimates",
    "text": "Variability in Estimates\n\nIf another sample of size \\(5\\) is drawn from the same population:\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nsample mean\n\n\n2.35\n3.4\n3.97\n1.54\n3.5\n2.95\n\n\n\n\n\nThe second sample mean \\(\\overline{x} =\\) 2.95 is different from the first one.\n\n\nWhy the first sample and the second sample give us different sample means?\n\n\nA point estimator has its own sampling distribution\n\n\n\n\n\n\n\n\n\nA statistic value (point estimate) calculated from a sample varies from sample to sample because a point estimator is also a random variable, and it has its own sampling distribution! (why we learn sampling distribution. it measures the variation of a point estimator)"
  },
  {
    "objectID": "slides/09-ci-slides.html#why-point-estimates-are-not-enough",
    "href": "slides/09-ci-slides.html#why-point-estimates-are-not-enough",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Why Point Estimates Are Not Enough",
    "text": "Why Point Estimates Are Not Enough\n\nIf you want to estimate \\(\\mu\\), do you prefer to report a range of values the parameter might be in, or a single estimate like \\(\\overline{x}\\)?\n\n\n\nIf you want to catch a fish, do you prefer a spear or a net?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue to variation of \\(\\overline{X}\\), if we report a point estimate \\(\\overline{x}\\), we probably won‚Äôt hit the exact \\(\\mu\\).\nIf we report a range of plausible values, we have a better shot at capturing the parameter!\n\n\nthe estimate \\(\\overline{x}\\) may not be precise or reliable."
  },
  {
    "objectID": "slides/09-ci-slides.html#confidence-intervals",
    "href": "slides/09-ci-slides.html#confidence-intervals",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nA plausible range of values for \\(\\mu\\) is called a confidence interval (CI).\n\nTo construct a CI we need to quantify the variability of our sample mean.  \n\nQuantifying this uncertainty requires a measurement of how much we would expect the sample statistic to vary from sample to sample.\n\nThat is the variance of the sampling distribution of the sample mean!\n\n\n\n\nTo construct a CI we need to quantify the variability of our sample mean because the variability determines the upper and lower bound of the range of the plausible values.\nThe variability of the sample mean determines the size of CI.\nThis range depends on how precise and reliable our \\(\\overline{X}\\) is as an estimate of \\(\\mu\\).\nIf the variation of our sample mean is pretty large, meaning that every time we collect a sample, the sample mean varies a lot from one to another, then we are more uncertain about the value of \\(\\mu\\).\nAnd that mean the plausible range of values for \\(\\mu\\) will be pretty wide, or the CI will be much wider.\n\n\nüëâ The larger variation of \\(\\overline{X}\\) is, the wider the CI for \\(\\mu\\) will be, given the same ‚Äúlevel of confidence‚Äù.\n\n\n\n\nDo we know the variance of \\(\\overline{X}\\)?\n\n\n\n\nBy CLT, \\(\\overline{X} \\sim N(\\mu, \\sigma^2/n)\\) regardless of what the population distribution is."
  },
  {
    "objectID": "slides/09-ci-slides.html#confidence-interval-is-for-a-parameter",
    "href": "slides/09-ci-slides.html#confidence-interval-is-for-a-parameter",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Confidence Interval Is for a Parameter",
    "text": "Confidence Interval Is for a Parameter\n\n\nA confidence interval is for a parameter, NOT a statistic.\n\nUse the sample mean to form a confidence interval for the population mean.\n\n\nWe never say ‚ÄúThe confidence interval of the sample mean \\(\\overline{X}\\) is ‚Ä¶‚Äù\nWe say ‚ÄúThe confidence interval for the true population mean \\(\\mu\\) is ‚Ä¶‚Äù\n\n\n\nIn general, a confidence interval for \\(\\mu\\) has the form\n\n\n\n\\(\\large \\overline{x} \\pm m = (\\overline{x} - m, \\overline{x} + m)\\)\n\n\n\nThe \\(m\\) is called margin of error.\n\\(\\overline{x} - m\\) is the lower bound and \\(\\overline{x} + m\\) is the upper bound of the confidence interval.\nThe point estimate \\(\\overline{x}\\) and margin of error \\(m\\) can be obtained from known quantities and our data once sampled.\n\n\nUse the sample variance to form a confidence interval for the population variance."
  },
  {
    "objectID": "slides/09-ci-slides.html#precision-vs.-reliability",
    "href": "slides/09-ci-slides.html#precision-vs.-reliability",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Precision vs.¬†Reliability",
    "text": "Precision vs.¬†Reliability\n\nIf we want to be very certain that we capture \\(\\mu\\), should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?\n\n\nHere is a question. If we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval?\nWe use a wider interval because a wider interval is more likely to capture the population parameter value. So a 99% confidence interval is wider than a 95% confidence interval.\nBut What drawbacks are associated with using a wider interval?\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs look at this comic.\nI can say I am 100% confident that your exam 1 score is between 0 and 100. Am I right? Yes. But do I provide helpful information? Absolutely not, the interval includes every possible score of the exam. The interval is too wide to be helpful.\n\n\n\n\nWith the sample size fixed, precision and reliability have a trade-off relationship.\n\n.question[ How can we get best of both worlds ‚Äì high precision and high reliability/accuracy?] - How can we get best of both worlds ‚Äì high precision and high reliability/accuracy, meaning short interval with high level of confidence. - What we need is larger sample size, given that the sample quality is good. - Easy statement, but sometimes it‚Äôs hard to collect more samples."
  },
  {
    "objectID": "slides/09-ci-slides.html#alpha100-confidence-intervals",
    "href": "slides/09-ci-slides.html#alpha100-confidence-intervals",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "\n\\((1 - \\alpha)100\\%\\) Confidence Intervals",
    "text": "\\((1 - \\alpha)100\\%\\) Confidence Intervals\n\nThe confidence level \\(1-\\alpha\\): the proportion of times that the CI contains the population parameter, assuming that the estimation process is repeated a large number of times.\n\nThe common choices for the confidence level are\n\n90% \\((\\alpha = 0.10)\\)\n\n95% \\((\\alpha = 0.05)\\)\n\n99% \\((\\alpha = 0.01)\\)\n\n\n\n95% is the most common level because of good balance between precision (width of the CI) and reliability (confidence level)\n\n\n\n High reliability and Low precision. I am 100% confident that the mean height of Marquette students is between 3‚Äô0‚Äù and 8‚Äô0‚Äù.  duh‚Ä¶ü§∑\n Low reliability and High precision. I am 20% confident that mean height of Marquette students is between 5‚Äô6‚Äù and 5‚Äô7‚Äù.  far from it‚Ä¶üôÖ\n\n\nA confidence interval is associated with a confidence level.\nThe confidence level (often expressed as the percentage value, such as 95%)"
  },
  {
    "objectID": "slides/09-ci-slides.html#confidence-intervals-for-mu-z-score",
    "href": "slides/09-ci-slides.html#confidence-intervals-for-mu-z-score",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "\n\\(95\\%\\) Confidence Intervals for \\(\\mu\\): Z-score",
    "text": "\\(95\\%\\) Confidence Intervals for \\(\\mu\\): Z-score\n\n\n\n\\(\\alpha = 0.05\\)\nStart with the sampling distribution of \\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\)\n\\(\\overline{x}\\) will be within 1.96 SDs of the population mean \\(\\mu\\) \\(95\\%\\) of the time.\nThe \\(z\\)-score of 1.96 is associated with 2.5% area to the right, and called a critical value denoted as \\(z_{0.025}\\).\n\n\n\n\n\n\n\n\n\n\n\n(and Z = -1.96 has 2.5% area to the left); -At a distance of zŒ±/2 standard deviations to the right of Œº, there is an area of Œ±/2 under the normal density curve."
  },
  {
    "objectID": "slides/09-ci-slides.html#confidence-intervals-for-mu-probability",
    "href": "slides/09-ci-slides.html#confidence-intervals-for-mu-probability",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "\n\\(95\\%\\) Confidence Intervals for \\(\\mu\\): Probability",
    "text": "\\(95\\%\\) Confidence Intervals for \\(\\mu\\): Probability\n\\[P\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\overline{X} &lt; \\mu + 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\\]\n\n\n\nIs the interval \\(\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}}, \\mu+1.96\\frac{\\sigma}{\\sqrt{n}} \\right)\\) our confidence interval?\n\n\n‚ùå No! We don‚Äôt know \\(\\mu\\), the quantity we like to estimate!\nBut we‚Äôre almost there!"
  },
  {
    "objectID": "slides/09-ci-slides.html#confidence-intervals-for-mu-formula",
    "href": "slides/09-ci-slides.html#confidence-intervals-for-mu-formula",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "\n\\(95\\%\\) Confidence Intervals for \\(\\mu\\): Formula",
    "text": "\\(95\\%\\) Confidence Intervals for \\(\\mu\\): Formula\n\n\n\n\n\\[\n\\small P\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\overline{X} &lt; \\mu + 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95 \\iff\nP\\left( \\boxed{\\overline{X}-1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\overline{X} + 1.96\\frac{\\sigma}{\\sqrt{n}}} \\right) = 0.95\\]\n\n\n\n With sample data of size \\(n\\), \\(\\left(\\overline{x}-1.96\\frac{\\sigma}{\\sqrt{n}},  \\overline{x} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right)\\) is our \\(95\\%\\) CI for \\(\\mu\\) if \\(\\sigma\\) is known to us! \nThe margin of error \\(m = 1.96\\frac{\\sigma}{\\sqrt{n}}\\)."
  },
  {
    "objectID": "slides/09-ci-slides.html#confidence-intervals-for-mu-when-sigma-is-known",
    "href": "slides/09-ci-slides.html#confidence-intervals-for-mu-when-sigma-is-known",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known",
    "text": "Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known\nRequirements for estimating \\(\\mu\\) when \\(\\sigma\\) is known:\n\n\nüëâ The sample should be a random sample, i.e.¬†All data \\(X_i\\) are drawn from the same population, and \\(X_i\\) and \\(X_j\\) are independent.\n\n Any methods in the course are based on a random sample \n\n\nüëâ The population standard deviation \\(\\sigma\\) is known.\n\nüëâ The population is either normally distributed or \\(n &gt; 30\\) or both, i.e., \\(X_i \\sim N(\\mu, \\sigma^2)\\).\n\n \\(n &gt; 30\\) allows CLT to be applied and hence normality is satisfied."
  },
  {
    "objectID": "slides/09-ci-slides.html#alpha100-confidence-intervals-for-mu",
    "href": "slides/09-ci-slides.html#alpha100-confidence-intervals-for-mu",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "\n\\((1-\\alpha)100\\%\\) Confidence Intervals for \\(\\mu\\):",
    "text": "\\((1-\\alpha)100\\%\\) Confidence Intervals for \\(\\mu\\):\n\n\n\n\n\n\n\n\n\n\n\n \\(\\left(\\overline{x}-1.96\\frac{\\sigma}{\\sqrt{n}},  \\overline{x} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right)\\)   \\(\\left(\\overline{x}-z_{0.025}\\frac{\\sigma}{\\sqrt{n}},  \\overline{x} + z_{0.025}\\frac{\\sigma}{\\sqrt{n}}\\right)\\) \n\n\n\n\n\n\n\n\n\n\n\n \\(\\left(\\overline{x}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}},  \\overline{x} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right)\\)"
  },
  {
    "objectID": "slides/09-ci-slides.html#confidence-intervals-for-mu-when-sigma-is-known-1",
    "href": "slides/09-ci-slides.html#confidence-intervals-for-mu-when-sigma-is-known-1",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known",
    "text": "Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known\nProcedures for constructing a confidence interval for \\(\\mu\\) when \\(\\sigma\\) known:\n\nCheck that the requirements are satisfied.\n\nDecide \\(\\alpha\\) or confidence level \\((1 - \\alpha)\\).\n\nFind the critical value \\(z_{\\alpha/2}\\).\n\nEvaluate margin of error \\(m = z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\)\n\nConstruct the \\((1 - \\alpha)100\\%\\) CI for \\(\\mu\\) using sample mean \\(\\overline{x}\\) and margin of error \\(m\\):\n\n \\[\\left( \\overline{x} -z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\, \\overline{x} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right)\\]"
  },
  {
    "objectID": "slides/09-ci-slides.html#example-ci-for-mu-when-sigma-is-known",
    "href": "slides/09-ci-slides.html#example-ci-for-mu-when-sigma-is-known",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Example: CI for \\(\\mu\\) When \\(\\sigma\\) is Known",
    "text": "Example: CI for \\(\\mu\\) When \\(\\sigma\\) is Known\n\n\nWe want to know the mean systolic blood pressure (SBP) of a population.\n\nAssume that the population distribution is normal with the standard deviation of 5 mmHg.\nWe have a random sample of 16 subjects of this population with mean 121.5.\nEstimate the mean SBP with a 95% confidence interval.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRequirements:  Normality is assumed, \\(\\sigma = 5\\) is known and a random sample is collected.\n\n\n\n\n\nDecide \\(\\alpha\\):  \\(\\alpha = 0.05\\) \n\n\n\n\n\nFind the critical value \\(z_{\\alpha/2}\\):  \\(z_{\\alpha/2} = z_{0.025} = 1.96\\) \n\n\n\n\n\nEvaluate margin of error \\(m = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\):  \\(m = (1.96) \\frac{5}{\\sqrt{16}} = 2.45\\) \n\n\n\n\n\nConstruct the \\((1 - \\alpha)100\\%\\) CI:  The 95% CI for the mean SBP is \\(\\overline{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} = (121.5 -2.45, 121.5 + 2.45) = (119.05, 123.95)\\)"
  },
  {
    "objectID": "slides/09-ci-slides.html#computation-in-r",
    "href": "slides/09-ci-slides.html#computation-in-r",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Computation in R",
    "text": "Computation in R\n\n## save all information we have\nalpha &lt;- 0.05\nn &lt;- 16\nx_bar &lt;- 121.5\nsig &lt;- 5\n\n## 95% CI\n## z-critical value\n(cri_z &lt;- qnorm(p = alpha / 2, lower.tail = FALSE))  \n\n[1] 1.96\n\n## margin of error\n(m_z &lt;- cri_z * (sig / sqrt(n)))  \n\n[1] 2.45\n\n## 95% CI for mu when sigma is known\nx_bar + c(-1, 1) * m_z  \n\n[1] 119.1 123.9\n\n\n\n\nConstruct a 99% CI for the mean SBP. Do you expect to have a wider or narrower interval? Why?"
  },
  {
    "objectID": "slides/09-ci-slides.html#interpreting-a-confidence-interval",
    "href": "slides/09-ci-slides.html#interpreting-a-confidence-interval",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Interpreting a Confidence Interval",
    "text": "Interpreting a Confidence Interval\n\n ‚ÄúWe are 95% confident that the mean SBP lies between 119.1 mm and 123.9 mm.‚Äù \nSuppose we were able to collect our dataset many times and build the corresponding CIs.\n\nWe would expect about 95% of those intervals would contain the true population parameter, here the mean systolic blood pressure.\n\n Remember: \\(\\overline{x}\\) varies from sample to sample, so does its corresponding CI.\n\n\nWe never know if in fact 95% of them do, or whether any interval contains the true parameter!"
  },
  {
    "objectID": "slides/09-ci-slides.html#generate-100-confidence-intervals-assuming-mu-120.",
    "href": "slides/09-ci-slides.html#generate-100-confidence-intervals-assuming-mu-120.",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Generate 100 Confidence Intervals Assuming \\(\\mu = 120\\).",
    "text": "Generate 100 Confidence Intervals Assuming \\(\\mu = 120\\)."
  },
  {
    "objectID": "slides/09-ci-slides.html#interpreting-a-confidence-interval-do-not-say",
    "href": "slides/09-ci-slides.html#interpreting-a-confidence-interval-do-not-say",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Interpreting a Confidence Interval DO NOT SAY",
    "text": "Interpreting a Confidence Interval DO NOT SAY\n\n\nWRONG ‚ùå ‚ÄúThere is a 95% chance/probability that the true population mean will fall between 119.1 mm and 123.9 mm.‚Äù\n\n\nWRONG ‚ùå ‚ÄúThe probability that the true population mean falls between 119.1 mm and 123.9 mm is 95%.‚Äù\n\n\n\n\n üëâ The sample mean is a random variable with a sampling distribution, so it makes sense to compute a probability of it being in some interval. \n üëâ The population mean is unknown and FIXED. We cannot assign or compute any probability of it. \n\n\n\n\nAnother inference method, Bayesian inference, treats \\(\\mu\\) as a random variable and therefore we can compute any probability associated with it. (MATH 4790 Bayesian Statistics)"
  },
  {
    "objectID": "slides/09-ci-slides.html#confidence-intervals-for-mu-when-sigma-is-unknown",
    "href": "slides/09-ci-slides.html#confidence-intervals-for-mu-when-sigma-is-unknown",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Unknown",
    "text": "Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Unknown\n\n\n\\(\\sigma^2 = \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{N}\\), \\(N\\) is the population size.\nIt‚Äôs rare that we do not know \\(\\mu\\), but know \\(\\sigma\\).\nWe use the Student t distribution to construct a confidence interval for \\(\\mu\\) when \\(\\sigma\\) is unknown.\n\nStill need\n\nRandom sample\nPopulation is normally distributed and/or \\(n &gt; 30\\).\n\n\n\n\nWhat is a natural estimator for the unknown \\(\\sigma\\)?\n\n\n\nSince \\(\\sigma\\) is unknown, we use the sample standard deviation \\(S = \\sqrt{\\frac{\\sum_{i=1}^{n}(X_i - \\overline{X})^2}{n-1}}\\) instead when constructing the CI.\n\n\nSo far we estimate unknown mean \\(\\mu\\) with known standard deviation \\(\\sigma\\).\n\n\\(\\sigma^2 = \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{N}\\), \\(N\\) is the population size.\nIt‚Äôs rare that we do not know \\(\\mu\\), but know \\(\\sigma\\).\nWe use a so-called Student t distribution to construct a confidence intervals for \\(\\mu\\) when \\(\\sigma\\) is unknown.\nWe still need to satisfy the following:\n\nRandom sample\nPopulation is normally distributed and/or \\(n &gt; 30\\).\n\n\n\n.question[ What is a natural estimator for the unknown \\(\\sigma\\)?]\n\nSince \\(\\sigma\\) is unknown, we use the sample standard deviation \\(S = \\sqrt{\\frac{\\sum_{i=1}^{n}(X_i - \\overline{X})^2}{n-1}}\\) instead when constructing the CI."
  },
  {
    "objectID": "slides/09-ci-slides.html#student-t-distribution",
    "href": "slides/09-ci-slides.html#student-t-distribution",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Student t Distribution",
    "text": "Student t Distribution\n\nIf the population is normally distributed or \\(n &gt; 30\\),\n\n\\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\)\n\\(Z = \\frac{\\overline{X} - \\mu}{\\color{red}\\sigma/\\sqrt{n} } \\sim N(0, 1)\\)\n \\(T = \\frac{\\overline{X} - \\mu}{\\color{red}S/\\sqrt{n} } \\sim t_{n-1}\\) \n\n\\(t_{n-1}\\) denotes the Student t distribution with degrees of freedom (df) \\(n-1\\)."
  },
  {
    "objectID": "slides/09-ci-slides.html#properties-of-student-t-distribution",
    "href": "slides/09-ci-slides.html#properties-of-student-t-distribution",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Properties of Student t Distribution",
    "text": "Properties of Student t Distribution\n\nSymmetric about the mean 0 and bell-shaped as \\(N(0, 1)\\).\nMore variability than \\(N(0, 1)\\) (heavier tails and lower peak).\nThe variability is different for different sample sizes (degrees of freedom).\nAs \\(n \\rightarrow \\infty\\) \\((df \\rightarrow \\infty)\\), the Student t distribution approaches to \\(N(0, 1)\\)."
  },
  {
    "objectID": "slides/09-ci-slides.html#critical-values-of-t_alpha2-n-1",
    "href": "slides/09-ci-slides.html#critical-values-of-t_alpha2-n-1",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Critical Values of \\(t_{\\alpha/2, n-1}\\)\n",
    "text": "Critical Values of \\(t_{\\alpha/2, n-1}\\)\n\n\nWhen \\(\\sigma\\) is unknown, we use \\(t_{\\alpha/2, n-1}\\) as the critical value, instead of \\(z_{\\alpha/2}\\).\n\n\n\n\nWith the same \\(\\alpha\\), \\(t_{\\alpha, n-1}\\) or \\(z_{\\alpha}\\) is larger?"
  },
  {
    "objectID": "slides/09-ci-slides.html#critical-values-of-t_alpha2-n-1-1",
    "href": "slides/09-ci-slides.html#critical-values-of-t_alpha2-n-1-1",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Critical Values of \\(t_{\\alpha/2, n-1}\\)\n",
    "text": "Critical Values of \\(t_{\\alpha/2, n-1}\\)\n\n\nGiven the same confidence level \\(1-\\alpha\\), \\(t_{\\alpha/2, n-1} &gt; z_{\\alpha/2}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevel\nt df = 5\nt df = 15\nt df = 30\nt df = 1000\nt df = inf\nz\n\n\n\n90%\n2.02\n1.75\n1.70\n1.65\n1.64\n1.64\n\n\n95%\n2.57\n2.13\n2.04\n1.96\n1.96\n1.96\n\n\n99%\n4.03\n2.95\n2.75\n2.58\n2.58\n2.58"
  },
  {
    "objectID": "slides/09-ci-slides.html#ci-for-mu-when-sigma-is-unknown",
    "href": "slides/09-ci-slides.html#ci-for-mu-when-sigma-is-unknown",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "CI for \\(\\mu\\) When \\(\\sigma\\) is Unknown",
    "text": "CI for \\(\\mu\\) When \\(\\sigma\\) is Unknown\n\nThe \\((1-\\alpha)100\\%\\) confidence interval for \\(\\mu\\) when \\(\\sigma\\) is unknown is  \\[\\left(\\overline{x} - t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}, \\overline{x} + t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\right)\\] \nGiven the same confidence level \\(1-\\alpha\\), \\(t_{\\alpha/2, n-1} &gt; z_{\\alpha/2}\\).\n\n\nWe are more ‚Äúuncertain‚Äù when doing inference about \\(\\mu\\) because we also don‚Äôt have information about \\(\\sigma\\), and replacing it with \\(s\\) adds additional uncertainty."
  },
  {
    "objectID": "slides/09-ci-slides.html#computation-in-r-t-interval",
    "href": "slides/09-ci-slides.html#computation-in-r-t-interval",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Computation in R (t interval)",
    "text": "Computation in R (t interval)\n\nBack to the systolic blood pressure (SBP) example. We have \\(n=16\\) and \\(\\overline{x} = 121.5\\).\nEstimate the mean SBP with a 95% confidence interval with unknown \\(\\sigma\\) and \\(s = 5\\).\n\n\nalpha &lt;- 0.05\nn &lt;- 16\nx_bar &lt;- 121.5\ns &lt;- 5  ## sigma is unknown and s = 5\n\n## t-critical value\n(cri_t &lt;- qt(p = alpha / 2, df = n - 1, lower.tail = FALSE)) \n\n[1] 2.131\n\n## margin of error\n(m_t &lt;- cri_t * (s / sqrt(n)))  \n\n[1] 2.664\n\n## 95% CI for mu when sigma is unknown\nx_bar + c(-1, 1) * m_t  \n\n[1] 118.8 124.2"
  },
  {
    "objectID": "slides/09-ci-slides.html#summary",
    "href": "slides/09-ci-slides.html#summary",
    "title": "Statistical Inference: Point and Interval Estimation \n",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\nNumerical Data, \\(\\sigma\\) known\nNumerical Data, \\(\\sigma\\) unknown\n\n\n\nParameter of Interest\nPopulation Mean \\(\\mu\\)\n\nPopulation Mean \\(\\mu\\)\n\n\n\nConfidence Interval\n\\(\\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\)\n\\(\\bar{x} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\)\n\n\n\n\n\nRemember to check if the population is normally distributed or \\(n&gt;30\\).\n\n\n\n\nWhat if the population is not normal and \\(n \\le 30\\)?\n\n\n\n\nUse a so-called nonparametric method, for example bootstrapping. (Your project?!)"
  },
  {
    "objectID": "slides/01-syllabus.html#my-journey",
    "href": "slides/01-syllabus.html#my-journey",
    "title": "Welcome Aboard üôå",
    "section": "My Journey",
    "text": "My Journey\n\nAssistant Professor (2020/08 - )\n\n\n\n\n\n\n\n\n\n\nPostdoctoral Fellow\n\n\n\n\n\n\n\n\n\n\nPhD in Statistics\n\n\n\n\n\n\n\n\n\n\nMA in Economics/PhD program in Statistics"
  },
  {
    "objectID": "slides/01-syllabus.html#how-to-reach-me",
    "href": "slides/01-syllabus.html#how-to-reach-me",
    "title": "Welcome Aboard üôå",
    "section": "How to Reach Me",
    "text": "How to Reach Me\n\nOffice hours MoWe 4:50 - 5:50 PM and Tu 1 - 2 PM in Cudahy Hall 353.\n\nüìß cheng-han.yu@marquette.edu\n\nAnswer your question within 24 hours.\nExpect a reply on Monday if shoot me a message on weekends.\nStart your subject line with [math4720] or [mssc5720] followed by a clear description of your question.\n\n\n\n\n\n\n\n\n\n\n\n\nI will NOT reply your e-mail if ‚Ä¶ Check the email policy in the syllabus!"
  },
  {
    "objectID": "slides/01-syllabus.html#ta-information",
    "href": "slides/01-syllabus.html#ta-information",
    "title": "Welcome Aboard üôå",
    "section": "TA Information",
    "text": "TA Information\n\n\n\nStatistics PhD student Qishi Zhan\nüì® qishi.zhan@marquette.edu\nHelp desk hours: To be announced.\nWelcome to set up a meeting with your TA via Teams.\nLet me know if you need any other help! üòÑ"
  },
  {
    "objectID": "slides/01-syllabus.html#help-desk-hours",
    "href": "slides/01-syllabus.html#help-desk-hours",
    "title": "Welcome Aboard üôå",
    "section": "Help Desk Hours",
    "text": "Help Desk Hours\n\n\n\nUnfortunately, no TA for this course. üòø\nHelp desk hours on the 4th floor of Cudahy Hall.\nThe hours will be released later."
  },
  {
    "objectID": "slides/01-syllabus.html#course-materials",
    "href": "slides/01-syllabus.html#course-materials",
    "title": "Welcome Aboard üôå",
    "section": "Course Materials",
    "text": "Course Materials\n\nCourse Website - https://math4720-f25.github.io/website/\n\n\n\n\n\n\n\n\n\n\n\n\nAll course materials and information can be found on this website. Bookmark it!"
  },
  {
    "objectID": "slides/01-syllabus.html#textbook-really",
    "href": "slides/01-syllabus.html#textbook-really",
    "title": "Welcome Aboard üôå",
    "section": "Textbook (Really?!)",
    "text": "Textbook (Really?!)"
  },
  {
    "objectID": "slides/01-syllabus.html#textbook-dr.-yus-online-book",
    "href": "slides/01-syllabus.html#textbook-dr.-yus-online-book",
    "title": "Welcome Aboard üôå",
    "section": "Textbook (Dr.¬†Yu‚Äôs Online Book)",
    "text": "Textbook (Dr.¬†Yu‚Äôs Online Book)\n\nIntroduction to Statistics, by Cheng-Han Yu\nFull detailed explanation of course slides üòé"
  },
  {
    "objectID": "slides/01-syllabus.html#learning-management-system-d2l",
    "href": "slides/01-syllabus.html#learning-management-system-d2l",
    "title": "Welcome Aboard üôå",
    "section": "Learning Management System (D2L)",
    "text": "Learning Management System (D2L)\n\n\n\n\n\n\n\n\n\nSubmit your work: Assessments &gt; Dropbox\nCheck your grade: Assessments &gt; Grades\nNew announcement: News"
  },
  {
    "objectID": "slides/01-syllabus.html#grading-policy",
    "href": "slides/01-syllabus.html#grading-policy",
    "title": "Welcome Aboard üôå",
    "section": "Grading Policy ‚ú®",
    "text": "Grading Policy ‚ú®\n\n\nYour final grade is earned out of 1000 total points distributed as follows:\n\nHomework 1 to 8: 175 pts (25 pts each)\nQuiz 1 to 4: 200 pts (50 pts each)\nExam 1 and 2: 300 pts (150 pts each)\nFinal exam: 150 pts\nAI Project: 150 pts\nClass participation: 25 pts\n\n\n‚ùå No extra credit projects/homework/exam to compensate for a poor grade.\nIndividual grade will NOT be curved."
  },
  {
    "objectID": "slides/01-syllabus.html#grade-percentage-conversion",
    "href": "slides/01-syllabus.html#grade-percentage-conversion",
    "title": "Welcome Aboard üôå",
    "section": "Grade-Percentage Conversion ‚ú®",
    "text": "Grade-Percentage Conversion ‚ú®\n\nYour final grade is based on your percentage of points earned out of 1000 points.\n\n\\([x, y)\\) means greater than or equal to \\(x\\) and less than \\(y\\).\n\n\n\n\n\nGrade\nPercentage\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[73, 77)\n\n\nC-\n[70, 73)\n\n\nD+\n[65, 70)\n\n\nD\n[60, 65)\n\n\nF\n[0, 60)"
  },
  {
    "objectID": "slides/01-syllabus.html#homework-175-pts",
    "href": "slides/01-syllabus.html#homework-175-pts",
    "title": "Welcome Aboard üôå",
    "section": "Homework (175 pts)",
    "text": "Homework (175 pts)\n\nAssessments &gt; Dropbox and upload your homework in PDF format.\nThere are 8 homework sets.\nYou must submit YOUR OWN work. üôè\nEvery homework is due by Friday 11:59 PM  (Don‚Äôt miss it. This is a hard deadline‚ùó).\nMSSC 5720 students may have more or different homework questions.\n‚ùå No make-up homework.\nThe lowest score will not be in your final grade calculation.\nUsing generative AI (GenAI) tools is allowed, but you must carefully cite it or reveal your use of AI."
  },
  {
    "objectID": "slides/01-syllabus.html#quizzes-200-pts",
    "href": "slides/01-syllabus.html#quizzes-200-pts",
    "title": "Welcome Aboard üôå",
    "section": "Quizzes (200 pts)",
    "text": "Quizzes (200 pts)\n\nThere are 4 in-class 15-min quizzes. \nüìö Quizzes are individual and in closed-book and no-tech format, except a calculator.\n‚ùå No cheat sheet or GenAI is allowed.\n‚ùå No make-up quizzes for any reason unless you got an excused absence. Check the syllabus for more details."
  },
  {
    "objectID": "slides/01-syllabus.html#exams-450-pts",
    "href": "slides/01-syllabus.html#exams-450-pts",
    "title": "Welcome Aboard üôå",
    "section": "Exams (450 pts)",
    "text": "Exams (450 pts)\n\nThere are two midterm exams and one final exam.\nThe midterms have in-class and take-home parts, and the final is a in-class written exam.\n\n\nüìÑ For the in-class parts, one piece of letter size cheat sheet is allowed. It has to be turned-in with your exam.\nFor the take-home parts, you are allowed to use GenAI tools. However, you must carefully cite it or reveal your use of AI.\n\n. . .\n\nAssessments &gt; Dropbox to submit your take-home exams in PDF format. \nExam 1 covers Week 1 to 6.\nExam 2 covers Week 7 to 11.\nFinal exam is comprehensive and covers the all course materials. üòé\n‚ùå No make-up exams for any reason unless you got an excused absence."
  },
  {
    "objectID": "slides/01-syllabus.html#ai-project-150-pts",
    "href": "slides/01-syllabus.html#ai-project-150-pts",
    "title": "Welcome Aboard üôå",
    "section": "AI Project (150 pts)",
    "text": "AI Project (150 pts)\n\nThere will be one AI project released around Week 13.\nYou have to use GenAI to self learn a topic that is not covered in my lectures, and generate a data analysis report.\nThe project topic will also be tested in the final exam.\nYou must carefully cite your GenAI tool or reveal your use of AI."
  },
  {
    "objectID": "slides/01-syllabus.html#what-computing-language-we-use-i-teach",
    "href": "slides/01-syllabus.html#what-computing-language-we-use-i-teach",
    "title": "Welcome Aboard üôå",
    "section": "What Computing Language We Use (I Teach)?",
    "text": "What Computing Language We Use (I Teach)?\n\n\n\nüìà The best language for statistical computing! \n\n‚úÖ You may use other tools or software such as Excel, Python, Minitab, etc to do your work, but I will NOT teach any of them.\n‚ùå Drop/Swap deadline: 09/02/2025. Don‚Äôt miss it!"
  },
  {
    "objectID": "slides/01-syllabus.html#document-generative-ai-use",
    "href": "slides/01-syllabus.html#document-generative-ai-use",
    "title": "Welcome Aboard üôå",
    "section": "Document Generative AI Use",
    "text": "Document Generative AI Use\n\n\n\n\nYou may use generative AI tools such as ChatGPT and Gemini to generate a first draft of text for your work, provided that this use is documented and cited.\n\n\nUnless explicitly stated otherwise, you may make use of any online resources, but you must explicitly cite/document where you obtained any code you directly use or use as inspiration in your solutions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you use GenAI, please include the followings in your submitted work:\n\nWhy/How I used AI (prompts or questions)\nGenerated output (screenshot or copy-paste excerpt)\nHow I used the output"
  },
  {
    "objectID": "slides/01-syllabus.html#document-generative-ai-use-1",
    "href": "slides/01-syllabus.html#document-generative-ai-use-1",
    "title": "Welcome Aboard üôå",
    "section": "Document Generative AI Use",
    "text": "Document Generative AI Use\n\n\nWhy/How I used AI (prompts and questions)\n\nI asked ChatGPT to generate a histogram using R.\n\n\nGenerated output (screenshot or copy-paste excerpt)\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I used the output\n\nI reviewed the suggestions, but I did not use the exact code. Instead, I change the code format and breaks value to 50."
  },
  {
    "objectID": "slides/01-syllabus.html#academic-integrity",
    "href": "slides/01-syllabus.html#academic-integrity",
    "title": "Welcome Aboard üôå",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nThis course expects all students to follow University and College statements on academic integrity.\n\n\nHonor Pledge and Honor Code: I recognize the importance of personal integrity in all aspects of life and work. I commit myself to truthfulness, honor, and responsibility, by which I earn the respect of others. I support the development of good character, and commit myself to uphold the highest standards of academic integrity as an important aspect of personal integrity. My commitment obliges me to conduct myself according to the Marquette University Honor Code."
  },
  {
    "objectID": "slides/01-syllabus.html#genai-and-academic-integrity",
    "href": "slides/01-syllabus.html#genai-and-academic-integrity",
    "title": "Welcome Aboard üôå",
    "section": "GenAI and Academic Integrity",
    "text": "GenAI and Academic Integrity"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#my-journey",
    "href": "slides/01-syllabus-slides.html#my-journey",
    "title": "Welcome Aboard üôå",
    "section": "My Journey",
    "text": "My Journey\n\nAssistant Professor (2020/08 - )\n\n\n\n\n\n\n\n\n\n\nPostdoctoral Fellow\n\n\n\n\n\n\n\n\n\n\nPhD in Statistics\n\n\n\n\n\n\n\n\n\n\nMA in Economics/PhD program in Statistics"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#how-to-reach-me",
    "href": "slides/01-syllabus-slides.html#how-to-reach-me",
    "title": "Welcome Aboard üôå",
    "section": "How to Reach Me",
    "text": "How to Reach Me\n\nOffice hours MoWe 4:50 - 5:50 PM and Tu 1 - 2 PM in Cudahy Hall 353.\n\nüìß cheng-han.yu@marquette.edu\n\nAnswer your question within 24 hours.\nExpect a reply on Monday if shoot me a message on weekends.\nStart your subject line with [math4720] or [mssc5720] followed by a clear description of your question.\n\n\n\n\n\nI will NOT reply your e-mail if ‚Ä¶ Check the email policy in the syllabus!"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#help-desk-hours",
    "href": "slides/01-syllabus-slides.html#help-desk-hours",
    "title": "Welcome Aboard üôå",
    "section": "Help Desk Hours",
    "text": "Help Desk Hours\n\n\n\nUnfortunately, no TA for this course. üòø\nHelp desk hours on the 4th floor of Cudahy Hall.\nThe hours will be released later."
  },
  {
    "objectID": "slides/01-syllabus-slides.html#course-materials",
    "href": "slides/01-syllabus-slides.html#course-materials",
    "title": "Welcome Aboard üôå",
    "section": "Course Materials",
    "text": "Course Materials\n\nCourse Website - https://math4720-f25.github.io/website/\n\n\n\n\n\n\n\n\n\n\n\n\nAll course materials and information can be found on this website. Bookmark it!"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#textbook-really",
    "href": "slides/01-syllabus-slides.html#textbook-really",
    "title": "Welcome Aboard üôå",
    "section": "Textbook (Really?!)",
    "text": "Textbook (Really?!)"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#textbook-dr.-yus-online-book",
    "href": "slides/01-syllabus-slides.html#textbook-dr.-yus-online-book",
    "title": "Welcome Aboard üôå",
    "section": "Textbook (Dr.¬†Yu‚Äôs Online Book)",
    "text": "Textbook (Dr.¬†Yu‚Äôs Online Book)\n\nIntroduction to Statistics, by Cheng-Han Yu\nFull detailed explanation of course slides üòé"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#learning-management-system-d2l",
    "href": "slides/01-syllabus-slides.html#learning-management-system-d2l",
    "title": "Welcome Aboard üôå",
    "section": "Learning Management System (D2L)",
    "text": "Learning Management System (D2L)\n\n\nSubmit your work: Assessments &gt; Dropbox\nCheck your grade: Assessments &gt; Grades\nNew announcement: News"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#grading-policy",
    "href": "slides/01-syllabus-slides.html#grading-policy",
    "title": "Welcome Aboard üôå",
    "section": "Grading Policy ‚ú®",
    "text": "Grading Policy ‚ú®\n\n\nYour final grade is earned out of 1000 total points distributed as follows:\n\nHomework 1 to 8: 175 pts (25 pts each)\nQuiz 1 to 4: 200 pts (50 pts each)\nExam 1 and 2: 300 pts (150 pts each)\nFinal exam: 150 pts\nAI Project: 150 pts\nClass participation: 25 pts\n\n\n‚ùå No extra credit projects/homework/exam to compensate for a poor grade.\nIndividual grade will NOT be curved."
  },
  {
    "objectID": "slides/01-syllabus-slides.html#grade-percentage-conversion",
    "href": "slides/01-syllabus-slides.html#grade-percentage-conversion",
    "title": "Welcome Aboard üôå",
    "section": "Grade-Percentage Conversion ‚ú®",
    "text": "Grade-Percentage Conversion ‚ú®\n\nYour final grade is based on your percentage of points earned out of 1000 points.\n\n\\([x, y)\\) means greater than or equal to \\(x\\) and less than \\(y\\).\n\n\n\n\n\nGrade\nPercentage\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[73, 77)\n\n\nC-\n[70, 73)\n\n\nD+\n[65, 70)\n\n\nD\n[60, 65)\n\n\nF\n[0, 60)"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#homework-175-pts",
    "href": "slides/01-syllabus-slides.html#homework-175-pts",
    "title": "Welcome Aboard üôå",
    "section": "Homework (175 pts)",
    "text": "Homework (175 pts)\n\nAssessments &gt; Dropbox and upload your homework in PDF format.\nThere are 8 homework sets.\nYou must submit YOUR OWN work. üôè\nEvery homework is due by Friday 11:59 PM  (Don‚Äôt miss it. This is a hard deadline‚ùó).\nMSSC 5720 students may have more or different homework questions.\n‚ùå No make-up homework.\nThe lowest score will not be in your final grade calculation.\nUsing generative AI (GenAI) tools is allowed, but you must carefully cite it or reveal your use of AI."
  },
  {
    "objectID": "slides/01-syllabus-slides.html#quizzes-200-pts",
    "href": "slides/01-syllabus-slides.html#quizzes-200-pts",
    "title": "Welcome Aboard üôå",
    "section": "Quizzes (200 pts)",
    "text": "Quizzes (200 pts)\n\nThere are 4 in-class 15-min quizzes. \nüìö Quizzes are individual and in closed-book and no-tech format, except a calculator.\n‚ùå No cheat sheet or GenAI is allowed.\n‚ùå No make-up quizzes for any reason unless you got an excused absence. Check the syllabus for more details."
  },
  {
    "objectID": "slides/01-syllabus-slides.html#exams-450-pts",
    "href": "slides/01-syllabus-slides.html#exams-450-pts",
    "title": "Welcome Aboard üôå",
    "section": "Exams (450 pts)",
    "text": "Exams (450 pts)\n\nThere are two midterm exams and one final exam.\nThe midterms have in-class and take-home parts, and the final is a in-class written exam.\n\n\nüìÑ For the in-class parts, one piece of letter size cheat sheet is allowed. It has to be turned-in with your exam.\nFor the take-home parts, you are allowed to use GenAI tools. However, you must carefully cite it or reveal your use of AI.\n\n\n\nAssessments &gt; Dropbox to submit your take-home exams in PDF format. \nExam 1 covers Week 1 to 6.\nExam 2 covers Week 7 to 11.\nFinal exam is comprehensive and covers the all course materials. üòé\n‚ùå No make-up exams for any reason unless you got an excused absence."
  },
  {
    "objectID": "slides/01-syllabus-slides.html#ai-project-150-pts",
    "href": "slides/01-syllabus-slides.html#ai-project-150-pts",
    "title": "Welcome Aboard üôå",
    "section": "AI Project (150 pts)",
    "text": "AI Project (150 pts)\n\nThere will be one AI project released around Week 13.\nYou have to use GenAI to self learn a topic that is not covered in my lectures, and generate a data analysis report.\nThe project topic will also be tested in the final exam.\nYou must carefully cite your GenAI tool or reveal your use of AI."
  },
  {
    "objectID": "slides/01-syllabus-slides.html#what-computing-language-we-use-i-teach",
    "href": "slides/01-syllabus-slides.html#what-computing-language-we-use-i-teach",
    "title": "Welcome Aboard üôå",
    "section": "What Computing Language We Use (I Teach)?",
    "text": "What Computing Language We Use (I Teach)?\n\n\n\nüìà The best language for statistical computing! \n\n‚úÖ You may use other tools or software such as Excel, Python, Minitab, etc to do your work, but I will NOT teach any of them.\n‚ùå Drop/Swap deadline: 09/02/2025. Don‚Äôt miss it!"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#document-generative-ai-use",
    "href": "slides/01-syllabus-slides.html#document-generative-ai-use",
    "title": "Welcome Aboard üôå",
    "section": "Document Generative AI Use",
    "text": "Document Generative AI Use\n\n\n\n\nYou may use generative AI tools such as ChatGPT and Gemini to generate a first draft of text for your work, provided that this use is documented and cited.\n\n\nUnless explicitly stated otherwise, you may make use of any online resources, but you must explicitly cite/document where you obtained any code you directly use or use as inspiration in your solutions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you use GenAI, please include the followings in your submitted work:\n\nWhy/How I used AI (prompts or questions)\nGenerated output (screenshot or copy-paste excerpt)\nHow I used the output"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#document-generative-ai-use-1",
    "href": "slides/01-syllabus-slides.html#document-generative-ai-use-1",
    "title": "Welcome Aboard üôå",
    "section": "Document Generative AI Use",
    "text": "Document Generative AI Use\n\n\nWhy/How I used AI (prompts and questions)\n\nI asked ChatGPT to generate a histogram using R.\n\n\nGenerated output (screenshot or copy-paste excerpt)\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I used the output\n\nI reviewed the suggestions, but I did not use the exact code. Instead, I change the code format and breaks value to 50."
  },
  {
    "objectID": "slides/01-syllabus-slides.html#academic-integrity",
    "href": "slides/01-syllabus-slides.html#academic-integrity",
    "title": "Welcome Aboard üôå",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nThis course expects all students to follow University and College statements on academic integrity.\n\n\nHonor Pledge and Honor Code: I recognize the importance of personal integrity in all aspects of life and work. I commit myself to truthfulness, honor, and responsibility, by which I earn the respect of others. I support the development of good character, and commit myself to uphold the highest standards of academic integrity as an important aspect of personal integrity. My commitment obliges me to conduct myself according to the Marquette University Honor Code."
  },
  {
    "objectID": "slides/01-syllabus-slides.html#genai-and-academic-integrity",
    "href": "slides/01-syllabus-slides.html#genai-and-academic-integrity",
    "title": "Welcome Aboard üôå",
    "section": "GenAI and Academic Integrity",
    "text": "GenAI and Academic Integrity"
  },
  {
    "objectID": "slides/12-infer-variance.html#why-inference-for-population-variances",
    "href": "slides/12-infer-variance.html#why-inference-for-population-variances",
    "title": "Inference About Population Variances \n",
    "section": "Why Inference for Population Variances?",
    "text": "Why Inference for Population Variances?\n\nWe like to know if \\(\\sigma_1 = \\sigma_2\\), so a correct or better method can be used.\n\n\nWhich test we learned needs \\(\\sigma_1 = \\sigma_2\\)?\n\n. . .\n\nIn some situations, we care about variation!\n\n\n\n\n\n the variation in potency of drugs: affects patients‚Äô health\n\n\n\n\n\n\n\n\n\n\n\n\n\n the variance of stock prices : the higher the variance, the riskier the investment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen doing inference for population means,\nWe like to know if \\(\\sigma_1 = \\sigma_2\\), so that a correct or better method can be used. For example, we use the pooled \\(t\\)-test if \\(\\sigma_1 = \\sigma_2\\) or they are very close to each other for higher power.\ndrug for lowering BP. We hope the same amount or dose level of the drug provide the same effect on each individual. We don‚Äôt want some patients BP is lowered a lot, but some other patient‚Äôs BP is lowered just a little bit.\nWe want the new treatment can provide consistent potency and efficacy for all patients."
  },
  {
    "objectID": "slides/12-infer-variance.html#inference-for-population-variances",
    "href": "slides/12-infer-variance.html#inference-for-population-variances",
    "title": "Inference About Population Variances \n",
    "section": "Inference for Population Variances",
    "text": "Inference for Population Variances\n\nThe sample variance \\(S^2 = \\frac{\\sum_{i=1}^n(X_i - \\overline{X})^2}{n-1}\\) is our point estimator for the population variance \\(\\sigma^2\\). \nThe inference for \\(\\sigma^2\\) needs the population to be normal.\n\n\n‚ùó The methods can work poorly if the normality is violated, even the sample is large.\n\n\n\n\n\n\n\n\n\n\n\n\nIn some situations, we do care about variation.\n\n\n the variation in potency of drugs: affects patients‚Äô health\n\n the variance of stock prices : the higher the variance, the riskier the investment\n\n\nIntuitively, the sample variance \\(S^2 = \\frac{\\sum_{i=1}^n(X_i - \\overline{X})^2}{n-1}\\) is our point estimator for the population variance \\(\\sigma^2\\).\nFor a random sample of size \\(n\\) drawn from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\), \\(S^2\\) is an unbiased estimator for \\(\\sigma^2\\).\nIn Chapter 7, we discuss inference methods for \\(\\sigma^2\\) when the population is assumed normal. The methods discussed here can work poorly if normality is violated, even if the sample is large."
  },
  {
    "objectID": "slides/12-infer-variance.html#chi-squared-chi2-distribution",
    "href": "slides/12-infer-variance.html#chi-squared-chi2-distribution",
    "title": "Inference About Population Variances \n",
    "section": "Chi-Squared \\(\\chi^2\\) Distribution",
    "text": "Chi-Squared \\(\\chi^2\\) Distribution\nThe inference for \\(\\sigma^2\\) involves the \\(\\chi^2\\) distribution.\n\n\n\nDefined over positive numbers\nParameter: degrees of freedom \\(df\\)\nRight skewed\nMore symmetric as \\(df\\) gets larger"
  },
  {
    "objectID": "slides/12-infer-variance.html#upper-tail-and-lower-tail-of-chi-square",
    "href": "slides/12-infer-variance.html#upper-tail-and-lower-tail-of-chi-square",
    "title": "Inference About Population Variances \n",
    "section": "Upper Tail and Lower Tail of Chi-Square",
    "text": "Upper Tail and Lower Tail of Chi-Square\n\n\n\\(\\chi^2_{\\frac{\\alpha}{2},\\, df}\\) has area to the right of \\(\\alpha/2\\).\n\\(\\chi^2_{1-\\frac{\\alpha}{2},\\, df}\\) has area to the left of \\(\\alpha/2\\).\nIn \\(N(0, 1)\\), \\(z_{1-\\frac{\\alpha}{2}} = -z_{\\frac{\\alpha}{2}}\\). But \\(\\chi^2_{1-\\frac{\\alpha}{2},\\,df} \\ne -\\chi^2_{\\frac{\\alpha}{2},\\,df}\\) because of non-symmetry of the \\(\\chi^2\\) distribution.\n\n\n\n\n\n\n\n\n\n\n\nDefine critical values \\(\\chi_U^2\\) and \\(\\chi_L^2\\).\nWhen constructing a CI with level \\(1-\\alpha\\), we put \\(1-\\alpha\\) in the middle and"
  },
  {
    "objectID": "slides/12-infer-variance.html#sampling-distribution",
    "href": "slides/12-infer-variance.html#sampling-distribution",
    "title": "Inference About Population Variances \n",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\n\nWhen a random sample of size \\(n\\) is from \\(\\color{red}{N(\\mu, \\sigma^2)}\\), \\[ \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1} \\] \nThe inference method for \\(\\sigma^2\\) introduced here can work poorly if the normality assumption is violated, even for large samples!"
  },
  {
    "objectID": "slides/12-infer-variance.html#alpha100-confidence-interval-for-sigma2",
    "href": "slides/12-infer-variance.html#alpha100-confidence-interval-for-sigma2",
    "title": "Inference About Population Variances \n",
    "section": "\n\\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma^2\\)\n",
    "text": "\\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma^2\\)\n\n\n\n\\((1-\\alpha)100\\%\\) CI for \\(\\sigma^2\\) is \\[\\color{blue}{\\left( \\frac{(n-1)S^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}}, \\frac{(n-1)S^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}} \\right)}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùó The CI for \\(\\sigma^2\\) cannot be expressed as \\((S^2-m, S^2+m)\\) anymore!"
  },
  {
    "objectID": "slides/12-infer-variance.html#example-supermodel-heights",
    "href": "slides/12-infer-variance.html#example-supermodel-heights",
    "title": "Inference About Population Variances \n",
    "section": "Example: Supermodel Heights",
    "text": "Example: Supermodel Heights\n\n\nListed below are heights (cm) for the simple random sample of 16 female supermodels:\n\nheights &lt;- c(178, 177, 176, 174, 175, 178, 175, 178, \n             178, 177, 180, 176, 180, 178, 180, 176)\n\n\nThe supermodels‚Äô height is normally distributed.\nConstruct a \\(95\\%\\) confidence interval for population standard deviation \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\n\\(n = 16\\), \\(s^2 = 3.4\\), \\(\\alpha = 0.05\\).\n\\(\\chi^2_{\\alpha/2, n-1} = \\chi^2_{0.025, 15} = 27.49\\)\n\\(\\chi^2_{1-\\alpha/2, n-1} = \\chi^2_{0.975, 15} = 6.26\\)\n\n. . .\n\nThe \\(95\\%\\) CI for \\(\\sigma\\) is \\(\\small \\left( \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}}}, \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}}} \\right) = \\left( \\sqrt{\\frac{(16-1)(3.4)}{27.49}}, \\sqrt{\\frac{(16-1)(3.4)}{6.26}}\\right) = (1.36, 2.85)\\)"
  },
  {
    "objectID": "slides/12-infer-variance.html#example-computation-in-r",
    "href": "slides/12-infer-variance.html#example-computation-in-r",
    "title": "Inference About Population Variances \n",
    "section": "Example: Computation in R",
    "text": "Example: Computation in R\n\nn &lt;- 16\ns2 &lt;- var(heights)\nal &lt;- 0.05\n\n## two chi-square critical values\nchi2_right &lt;- qchisq(al / 2, df = n - 1, lower.tail = FALSE)\nchi2_left &lt;- qchisq(al / 2, df = n - 1, lower.tail = TRUE)\n\n## two bounds of CI for sigma2\nci_lwr &lt;- (n - 1) * s2 / chi2_right\nci_upr &lt;- (n - 1) * s2 / chi2_left\n\n\n\n## two bounds of CI for sigma\nsqrt(ci_lwr)\n\n[1] 1.36\n\nsqrt(ci_upr)\n\n[1] 2.85"
  },
  {
    "objectID": "slides/12-infer-variance.html#example-contd-testing",
    "href": "slides/12-infer-variance.html#example-contd-testing",
    "title": "Inference About Population Variances \n",
    "section": "Example Cont‚Äôd: Testing",
    "text": "Example Cont‚Äôd: Testing\nUse \\(\\alpha = 0.05\\) to test the claim that ‚Äúsupermodels have heights with a standard deviation that is less than \\(\\sigma = 7.5\\) cm for the population of women‚Äù.\n. . .\n\n\nStep 1: \\(H_0: \\sigma = \\sigma_0\\) vs.¬†\\(H_1: \\sigma &lt; \\sigma_0\\). Here \\(\\sigma_0 = 7.5\\) cm\n\n. . .\n\n\nStep 2: \\(\\alpha = 0.05\\)\n\n\n. . .\n\n\nStep 3: Under \\(H_0\\), \\(\\chi_{test}^2 = \\frac{(n-1)s^2}{\\sigma_0^2} = \\frac{(16-1)(3.4)}{7.5^2} = 0.91\\), a statistic drawn from \\(\\chi^2_{n-1}\\).\n\n. . .\n\n\n\n\nStep 4-c: This is a left-tailed test. The critical value is \\(\\chi_{1-\\alpha, df}^2 = \\chi_{0.95, 15}^2 = 7.26\\)\n\n\nStep-5-c: Reject \\(H_0\\) in favor of \\(H_1\\) if \\(\\chi_{test}^2 &lt; \\chi_{1-\\alpha, df}^2\\). Since \\(0.91 &lt; 7.26\\), we reject \\(H_0\\)."
  },
  {
    "objectID": "slides/12-infer-variance.html#example-contd-testing-1",
    "href": "slides/12-infer-variance.html#example-contd-testing-1",
    "title": "Inference About Population Variances \n",
    "section": "Example Cont‚Äôd: Testing",
    "text": "Example Cont‚Äôd: Testing\nUse \\(\\alpha = 0.05\\) to test the claim that ‚Äúsupermodels have heights with a standard deviation that is less than \\(\\sigma = 7.5\\) cm for the population of women‚Äù.\n\nStep 1: \\(H_0: \\sigma = \\sigma_0\\) vs.¬†\\(H_1: \\sigma &lt; \\sigma_0\\). Here \\(\\sigma_0 = 7.5\\) cm\nStep 2: \\(\\alpha = 0.05\\)\nStep 3: Under \\(H_0\\), \\(\\chi_{test}^2 = \\frac{(n-1)s^2}{\\sigma_0^2} = \\frac{(16-1)(3.4)}{7.5^2} = 0.91\\), a statistic drawn from \\(\\chi^2_{n-1}\\).\n\n\n\n\n\nStep 6: There is sufficient evidence to support the claim that supermodels have heights with a SD that is less than the SD for the population of women.\n\nHeights of supermodels vary less than heights of women in the general population."
  },
  {
    "objectID": "slides/12-infer-variance.html#back-to-pooled-t-test",
    "href": "slides/12-infer-variance.html#back-to-pooled-t-test",
    "title": "Inference About Population Variances \n",
    "section": "Back to Pooled t-Test",
    "text": "Back to Pooled t-Test\nIn a pooled t-test, we assume\n\n both samples are of large size or drawn from a normal population. \n \\(\\sigma_1 = \\sigma_2\\) \n\n. . .\n\nUse QQ-plot (and normality tests, Anderson, Shapiro, etc) to check the assumption of normal distribution.\nWe learn to check the assumption \\(\\sigma_1 = \\sigma_2\\).\n\n\n\nWhenever we use t-statistics, there are assumptions."
  },
  {
    "objectID": "slides/12-infer-variance.html#f-distribution",
    "href": "slides/12-infer-variance.html#f-distribution",
    "title": "Inference About Population Variances \n",
    "section": "F Distribution",
    "text": "F Distribution\n\nWe use \\(F\\) distribution for the inference about two population variances.\n\n\n\nTwo parameters: \\(df_1\\), \\(df_2\\)\nRight skewed\nDefined over positive numbers"
  },
  {
    "objectID": "slides/12-infer-variance.html#upper-and-lower-tail-of-f-distribution",
    "href": "slides/12-infer-variance.html#upper-and-lower-tail-of-f-distribution",
    "title": "Inference About Population Variances \n",
    "section": "Upper and Lower Tail of F Distribution",
    "text": "Upper and Lower Tail of F Distribution\n\nWe denote \\(F_{\\alpha, \\, df_1, \\, df_2}\\) as the \\(F\\) quantile so that \\(P(F_{df_1, df_2} &gt; F_{\\alpha, \\, df_1, \\, df_2}) = \\alpha\\)."
  },
  {
    "objectID": "slides/12-infer-variance.html#sampling-distribution-1",
    "href": "slides/12-infer-variance.html#sampling-distribution-1",
    "title": "Inference About Population Variances \n",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\n\n\nThe random samples of size \\(n_1\\) and \\(n_2\\) are independent from two normal populations, \\(N(\\mu_1, \\sigma_1^2)\\) and \\(N(\\mu_2, \\sigma_2^2)\\).\nThe ratio \\[\\frac{S_1^2/S_2^2}{\\sigma_1^2/\\sigma_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]\n\n\n\n\n\n\\[\\frac{s_1^2/\\sigma_1^2}{s_2^2/\\sigma_2^2} = \\frac{s_1^2/s_2^2}{\\sigma_1^2/\\sigma_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]"
  },
  {
    "objectID": "slides/12-infer-variance.html#alpha100-confidence-interval-for-sigma_12-sigma_22",
    "href": "slides/12-infer-variance.html#alpha100-confidence-interval-for-sigma_12-sigma_22",
    "title": "Inference About Population Variances \n",
    "section": "\n\\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma_1^2 / \\sigma_2^2\\)\n",
    "text": "\\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma_1^2 / \\sigma_2^2\\)\n\n\n\n\\((1-\\alpha)100\\%\\) CI for \\(\\sigma_1^2 / \\sigma_2^2\\) is \\[\\color{blue}{\\left( \\frac{s_1^2/s_2^2}{F_{\\alpha/2, \\, n_1 - 1, \\, n_2 - 1}}, \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2, \\, \\, n_1 - 1, \\, n_2 - 1}} \\right)}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùó The CI for \\(\\sigma_1^2 / \\sigma_2^2\\) cannot be expressed as \\(\\left(\\frac{s_1^2}{s_2^2}-m, \\frac{s_1^2}{s_2^2} + m\\right)\\) anymore!"
  },
  {
    "objectID": "slides/12-infer-variance.html#f-test-for-comparing-sigma_12-and-sigma_22",
    "href": "slides/12-infer-variance.html#f-test-for-comparing-sigma_12-and-sigma_22",
    "title": "Inference About Population Variances \n",
    "section": "F test for comparing \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\)\n",
    "text": "F test for comparing \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\)\n\n\n\n\nStep 1: right-tailed  \\(\\small \\begin{align} &H_0: \\sigma_1 \\le \\sigma_2 \\\\ &H_1: \\sigma_1 &gt; \\sigma_2 \\end{align}\\)  and two-tailed  \\(\\small \\begin{align} &H_0: \\sigma_1 = \\sigma_2 \\\\ &H_1: \\sigma_1 \\ne \\sigma_2 \\end{align}\\) \n\n\n\n\n\nStep 2: \\(\\alpha = 0.05\\)\n\n\n\n\n\n\nStep 3: Under \\(H_0\\), \\(\\sigma_1 = \\sigma_2\\), and the test statistic is\n\n\\[\\small F_{test} = \\frac{s_1^2/s_2^2}{\\sigma_1^2/\\sigma_2^2} = \\frac{s_1^2}{s_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]\n\n\n\n\nStep 4-c:\n\nRight-tailed:  \\(F_{\\alpha, \\, n_1-1, \\, n_2-1}\\) .\nTwo-tailed:  \\(F_{\\alpha/2, \\, n_1-1, \\, n_2-1}\\) or \\(F_{1-\\alpha/2, \\, n_1-1, \\, n_2-1}\\) \n\n\n\n\n\n\n\n\nStep 5-c:\n\nRight-tailed: reject \\(H_0\\) if  \\(F_{test} \\ge F_{\\alpha, \\, n_1-1, \\, n_2-1}\\).\nTwo-tailed: reject \\(H_0\\) if  \\(F_{test} \\ge F_{\\alpha/2, \\, n_1-1, \\, n_2-1}\\) or \\(F_{test} \\le F_{1-\\alpha/2, \\, n_1-1, \\, n_2-1}\\)"
  },
  {
    "objectID": "slides/12-infer-variance.html#back-to-the-weight-loss-example",
    "href": "slides/12-infer-variance.html#back-to-the-weight-loss-example",
    "title": "Inference About Population Variances \n",
    "section": "Back to the Weight Loss Example",
    "text": "Back to the Weight Loss Example\n\n\nA study was conducted to see the effectiveness of a weight loss program.\n\nTwo groups (Control and Experimental) of 10 subjects were selected.\nThe two populations are normally distributed and have the same SD.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data on weight loss was collected at the end of six months\n\n\nControl: \\(n_1 = 10\\), \\(\\overline{x}_1 = 2.1\\, lb\\), \\(s_1 = 0.5\\, lb\\)\n\n\nExperimental: \\(n_2 = 10\\), \\(\\overline{x}_2 = 4.2\\, lb\\), \\(s_2 = 0.7\\, lb\\)\n\n\n\n\nAssumptions:\n\n \\(\\sigma_1 = \\sigma_2\\) \nThe weight loss for both groups are normally distributed."
  },
  {
    "objectID": "slides/12-infer-variance.html#back-to-the-weight-loss-example-check-if-sigma_1-sigma_2",
    "href": "slides/12-infer-variance.html#back-to-the-weight-loss-example-check-if-sigma_1-sigma_2",
    "title": "Inference About Population Variances \n",
    "section": "Back to the Weight Loss Example: Check if \\(\\sigma_1 = \\sigma_2\\)\n",
    "text": "Back to the Weight Loss Example: Check if \\(\\sigma_1 = \\sigma_2\\)\n\n\n\n\n\\(n_1 = 10\\), \\(s_1 = 0.5 \\, lb\\)\n\\(n_2 = 10\\), \\(s_2 = 0.7 \\, lb\\)\nStep 1: \\(\\begin{align} &H_0: \\sigma_1 = \\sigma_2 \\\\ &H_1: \\sigma_1 \\ne \\sigma_2 \\end{align}\\)\nStep 2: \\(\\alpha = 0.05\\)\nStep 3: \\(F_{test} = \\frac{s_1^2}{s_2^2} = \\frac{0.5^2}{0.7^2} = 0.51\\).\nStep 4-c: Two-tailed test. The critical value is \\(F_{0.05/2, \\, 10-1, \\, 10-1} = 4.03\\) or \\(F_{1-0.05/2, \\, 10-1, \\, 10-1} = 0.25\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 5-c: Is \\(F_{test} &gt; 4.03\\) or \\(F_{test} &lt; 0.25\\)? No.\nStep 6: The evidence is not sufficient to reject the claim that \\(\\sigma_1 = \\sigma_2\\)."
  },
  {
    "objectID": "slides/12-infer-variance.html#back-to-the-weight-loss-example-95-ci-for-sigma_12-sigma_22",
    "href": "slides/12-infer-variance.html#back-to-the-weight-loss-example-95-ci-for-sigma_12-sigma_22",
    "title": "Inference About Population Variances \n",
    "section": "Back to the Weight Loss Example: 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\)\n",
    "text": "Back to the Weight Loss Example: 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\)\n\n\n\n\n\n\n\nThe 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\) is \\[\\small \\begin{align} &\\left( \\frac{s_1^2/s_2^2}{F_{\\alpha/2, \\, df_1, \\, df_2}}, \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2, \\, df_1, \\, df_2}} \\right) \\\\ &= \\left( \\frac{0.51}{4.03}, \\frac{0.51}{0.25} \\right) = \\left(0.13, 2.04\\right)\\end{align}\\]\n\nWe are 95% confident that the ratio \\(\\sigma_1^2 / \\sigma_2^2\\) is between 0.13 and 2.04."
  },
  {
    "objectID": "slides/12-infer-variance.html#implementing-f-test-in-r",
    "href": "slides/12-infer-variance.html#implementing-f-test-in-r",
    "title": "Inference About Population Variances \n",
    "section": "Implementing F-test in R",
    "text": "Implementing F-test in R\n\n\n\n\nn1 &lt;- 10; n2 &lt;- 10\ns1 &lt;- 0.5; s2 &lt;- 0.7\nal &lt;- 0.05\n\n## 95% CI for sigma_1^2 / sigma_2^2\nf_small &lt;- qf(p = al / 2, \n              df1 = n1 - 1, df2 = n2 - 1, \n              lower.tail = TRUE)\nf_big &lt;- qf(p = al / 2, \n            df1 = n1 - 1, df2 = n2 - 1, \n            lower.tail = FALSE)\n\n\n## lower bound\n(s1 ^ 2 / s2 ^ 2) / f_big\n\n[1] 0.127\n\n## upper bound\n(s1 ^ 2 / s2 ^ 2) / f_small\n\n[1] 2.05\n\n\n\n\n\n## Testing sigma_1 = sigma_2\n(test_stats &lt;- s1 ^ 2 / s2 ^ 2)\n\n[1] 0.51\n\n(cri_big &lt;- qf(p = al / 2, \n               df1 = n1 - 1, \n               df2 = n2 - 1, \n               lower.tail = FALSE))\n\n[1] 4.03\n\n(cri_small &lt;- qf(p = al / 2, \n                 df1 = n1 - 1, \n                 df2 = n2 - 1, \n                 lower.tail = TRUE))\n\n[1] 0.248\n\n# var.test(x, y, alternative = \"two.sided\")"
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#why-inference-for-population-variances",
    "href": "slides/12-infer-variance-slides.html#why-inference-for-population-variances",
    "title": "Inference About Population Variances \n",
    "section": "Why Inference for Population Variances?",
    "text": "Why Inference for Population Variances?\n\nWe like to know if \\(\\sigma_1 = \\sigma_2\\), so a correct or better method can be used.\n\n\nWhich test we learned needs \\(\\sigma_1 = \\sigma_2\\)?\n\n\n\nIn some situations, we care about variation!\n\n\n\n\n\n the variation in potency of drugs: affects patients‚Äô health\n\n\n\n\n\n\n\n\n\n\n\n\n\n the variance of stock prices : the higher the variance, the riskier the investment\n\n\n\n\n\n\n\n\n\n\n\n\nWhen doing inference for population means,\nWe like to know if \\(\\sigma_1 = \\sigma_2\\), so that a correct or better method can be used. For example, we use the pooled \\(t\\)-test if \\(\\sigma_1 = \\sigma_2\\) or they are very close to each other for higher power.\ndrug for lowering BP. We hope the same amount or dose level of the drug provide the same effect on each individual. We don‚Äôt want some patients BP is lowered a lot, but some other patient‚Äôs BP is lowered just a little bit.\nWe want the new treatment can provide consistent potency and efficacy for all patients."
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#inference-for-population-variances",
    "href": "slides/12-infer-variance-slides.html#inference-for-population-variances",
    "title": "Inference About Population Variances \n",
    "section": "Inference for Population Variances",
    "text": "Inference for Population Variances\n\nThe sample variance \\(S^2 = \\frac{\\sum_{i=1}^n(X_i - \\overline{X})^2}{n-1}\\) is our point estimator for the population variance \\(\\sigma^2\\). \nThe inference for \\(\\sigma^2\\) needs the population to be normal.\n\n\n‚ùó The methods can work poorly if the normality is violated, even the sample is large.\n\n\n\n\nIn some situations, we do care about variation.\n\n\n the variation in potency of drugs: affects patients‚Äô health\n\n the variance of stock prices : the higher the variance, the riskier the investment\n\n\nIntuitively, the sample variance \\(S^2 = \\frac{\\sum_{i=1}^n(X_i - \\overline{X})^2}{n-1}\\) is our point estimator for the population variance \\(\\sigma^2\\).\nFor a random sample of size \\(n\\) drawn from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\), \\(S^2\\) is an unbiased estimator for \\(\\sigma^2\\).\nIn Chapter 7, we discuss inference methods for \\(\\sigma^2\\) when the population is assumed normal. The methods discussed here can work poorly if normality is violated, even if the sample is large."
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#chi-squared-chi2-distribution",
    "href": "slides/12-infer-variance-slides.html#chi-squared-chi2-distribution",
    "title": "Inference About Population Variances \n",
    "section": "Chi-Squared \\(\\chi^2\\) Distribution",
    "text": "Chi-Squared \\(\\chi^2\\) Distribution\nThe inference for \\(\\sigma^2\\) involves the \\(\\chi^2\\) distribution.\n\n\n\nDefined over positive numbers\nParameter: degrees of freedom \\(df\\)\nRight skewed\nMore symmetric as \\(df\\) gets larger"
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#upper-tail-and-lower-tail-of-chi-square",
    "href": "slides/12-infer-variance-slides.html#upper-tail-and-lower-tail-of-chi-square",
    "title": "Inference About Population Variances \n",
    "section": "Upper Tail and Lower Tail of Chi-Square",
    "text": "Upper Tail and Lower Tail of Chi-Square\n\n\n\\(\\chi^2_{\\frac{\\alpha}{2},\\, df}\\) has area to the right of \\(\\alpha/2\\).\n\\(\\chi^2_{1-\\frac{\\alpha}{2},\\, df}\\) has area to the left of \\(\\alpha/2\\).\nIn \\(N(0, 1)\\), \\(z_{1-\\frac{\\alpha}{2}} = -z_{\\frac{\\alpha}{2}}\\). But \\(\\chi^2_{1-\\frac{\\alpha}{2},\\,df} \\ne -\\chi^2_{\\frac{\\alpha}{2},\\,df}\\) because of non-symmetry of the \\(\\chi^2\\) distribution.\n\n\n\nDefine critical values \\(\\chi_U^2\\) and \\(\\chi_L^2\\).\nWhen constructing a CI with level \\(1-\\alpha\\), we put \\(1-\\alpha\\) in the middle and"
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#sampling-distribution",
    "href": "slides/12-infer-variance-slides.html#sampling-distribution",
    "title": "Inference About Population Variances \n",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\n\nWhen a random sample of size \\(n\\) is from \\(\\color{red}{N(\\mu, \\sigma^2)}\\), \\[ \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1} \\] \nThe inference method for \\(\\sigma^2\\) introduced here can work poorly if the normality assumption is violated, even for large samples!"
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#alpha100-confidence-interval-for-sigma2",
    "href": "slides/12-infer-variance-slides.html#alpha100-confidence-interval-for-sigma2",
    "title": "Inference About Population Variances \n",
    "section": "\n\\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma^2\\)\n",
    "text": "\\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma^2\\)\n\n\n\n\\((1-\\alpha)100\\%\\) CI for \\(\\sigma^2\\) is \\[\\color{blue}{\\left( \\frac{(n-1)S^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}}, \\frac{(n-1)S^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}} \\right)}\\]\n\n\n\n\n\n\n\n\n\n\n\n‚ùó The CI for \\(\\sigma^2\\) cannot be expressed as \\((S^2-m, S^2+m)\\) anymore!"
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#example-supermodel-heights",
    "href": "slides/12-infer-variance-slides.html#example-supermodel-heights",
    "title": "Inference About Population Variances \n",
    "section": "Example: Supermodel Heights",
    "text": "Example: Supermodel Heights\n\n\nListed below are heights (cm) for the simple random sample of 16 female supermodels:\n\nheights &lt;- c(178, 177, 176, 174, 175, 178, 175, 178, \n             178, 177, 180, 176, 180, 178, 180, 176)\n\n\nThe supermodels‚Äô height is normally distributed.\nConstruct a \\(95\\%\\) confidence interval for population standard deviation \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(n = 16\\), \\(s^2 = 3.4\\), \\(\\alpha = 0.05\\).\n\\(\\chi^2_{\\alpha/2, n-1} = \\chi^2_{0.025, 15} = 27.49\\)\n\\(\\chi^2_{1-\\alpha/2, n-1} = \\chi^2_{0.975, 15} = 6.26\\)\n\n\n\n\nThe \\(95\\%\\) CI for \\(\\sigma\\) is \\(\\small \\left( \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}}}, \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}}} \\right) = \\left( \\sqrt{\\frac{(16-1)(3.4)}{27.49}}, \\sqrt{\\frac{(16-1)(3.4)}{6.26}}\\right) = (1.36, 2.85)\\)"
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#example-computation-in-r",
    "href": "slides/12-infer-variance-slides.html#example-computation-in-r",
    "title": "Inference About Population Variances \n",
    "section": "Example: Computation in R",
    "text": "Example: Computation in R\n\nn &lt;- 16\ns2 &lt;- var(heights)\nal &lt;- 0.05\n\n## two chi-square critical values\nchi2_right &lt;- qchisq(al / 2, df = n - 1, lower.tail = FALSE)\nchi2_left &lt;- qchisq(al / 2, df = n - 1, lower.tail = TRUE)\n\n## two bounds of CI for sigma2\nci_lwr &lt;- (n - 1) * s2 / chi2_right\nci_upr &lt;- (n - 1) * s2 / chi2_left\n\n\n\n## two bounds of CI for sigma\nsqrt(ci_lwr)\n\n[1] 1.36\n\nsqrt(ci_upr)\n\n[1] 2.85"
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#example-contd-testing",
    "href": "slides/12-infer-variance-slides.html#example-contd-testing",
    "title": "Inference About Population Variances \n",
    "section": "Example Cont‚Äôd: Testing",
    "text": "Example Cont‚Äôd: Testing\nUse \\(\\alpha = 0.05\\) to test the claim that ‚Äúsupermodels have heights with a standard deviation that is less than \\(\\sigma = 7.5\\) cm for the population of women‚Äù.\n\n\n\nStep 1: \\(H_0: \\sigma = \\sigma_0\\) vs.¬†\\(H_1: \\sigma &lt; \\sigma_0\\). Here \\(\\sigma_0 = 7.5\\) cm\n\n\n\n\n\nStep 2: \\(\\alpha = 0.05\\)\n\n\n\n\n\n\nStep 3: Under \\(H_0\\), \\(\\chi_{test}^2 = \\frac{(n-1)s^2}{\\sigma_0^2} = \\frac{(16-1)(3.4)}{7.5^2} = 0.91\\), a statistic drawn from \\(\\chi^2_{n-1}\\).\n\n\n\n\n\n\n\nStep 4-c: This is a left-tailed test. The critical value is \\(\\chi_{1-\\alpha, df}^2 = \\chi_{0.95, 15}^2 = 7.26\\)\n\n\nStep-5-c: Reject \\(H_0\\) in favor of \\(H_1\\) if \\(\\chi_{test}^2 &lt; \\chi_{1-\\alpha, df}^2\\). Since \\(0.91 &lt; 7.26\\), we reject \\(H_0\\)."
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#example-contd-testing-1",
    "href": "slides/12-infer-variance-slides.html#example-contd-testing-1",
    "title": "Inference About Population Variances \n",
    "section": "Example Cont‚Äôd: Testing",
    "text": "Example Cont‚Äôd: Testing\nUse \\(\\alpha = 0.05\\) to test the claim that ‚Äúsupermodels have heights with a standard deviation that is less than \\(\\sigma = 7.5\\) cm for the population of women‚Äù.\n\nStep 1: \\(H_0: \\sigma = \\sigma_0\\) vs.¬†\\(H_1: \\sigma &lt; \\sigma_0\\). Here \\(\\sigma_0 = 7.5\\) cm\nStep 2: \\(\\alpha = 0.05\\)\nStep 3: Under \\(H_0\\), \\(\\chi_{test}^2 = \\frac{(n-1)s^2}{\\sigma_0^2} = \\frac{(16-1)(3.4)}{7.5^2} = 0.91\\), a statistic drawn from \\(\\chi^2_{n-1}\\).\n\n\n\n\n\nStep 6: There is sufficient evidence to support the claim that supermodels have heights with a SD that is less than the SD for the population of women.\n\nHeights of supermodels vary less than heights of women in the general population."
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#back-to-pooled-t-test",
    "href": "slides/12-infer-variance-slides.html#back-to-pooled-t-test",
    "title": "Inference About Population Variances \n",
    "section": "Back to Pooled t-Test",
    "text": "Back to Pooled t-Test\nIn a pooled t-test, we assume\n\n both samples are of large size or drawn from a normal population. \n \\(\\sigma_1 = \\sigma_2\\) \n\n\n\nUse QQ-plot (and normality tests, Anderson, Shapiro, etc) to check the assumption of normal distribution.\nWe learn to check the assumption \\(\\sigma_1 = \\sigma_2\\).\n\n\nWhenever we use t-statistics, there are assumptions."
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#f-distribution",
    "href": "slides/12-infer-variance-slides.html#f-distribution",
    "title": "Inference About Population Variances \n",
    "section": "F Distribution",
    "text": "F Distribution\n\nWe use \\(F\\) distribution for the inference about two population variances.\n\n\n\nTwo parameters: \\(df_1\\), \\(df_2\\)\nRight skewed\nDefined over positive numbers"
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#upper-and-lower-tail-of-f-distribution",
    "href": "slides/12-infer-variance-slides.html#upper-and-lower-tail-of-f-distribution",
    "title": "Inference About Population Variances \n",
    "section": "Upper and Lower Tail of F Distribution",
    "text": "Upper and Lower Tail of F Distribution\n\nWe denote \\(F_{\\alpha, \\, df_1, \\, df_2}\\) as the \\(F\\) quantile so that \\(P(F_{df_1, df_2} &gt; F_{\\alpha, \\, df_1, \\, df_2}) = \\alpha\\)."
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#sampling-distribution-1",
    "href": "slides/12-infer-variance-slides.html#sampling-distribution-1",
    "title": "Inference About Population Variances \n",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\n\n\nThe random samples of size \\(n_1\\) and \\(n_2\\) are independent from two normal populations, \\(N(\\mu_1, \\sigma_1^2)\\) and \\(N(\\mu_2, \\sigma_2^2)\\).\nThe ratio \\[\\frac{S_1^2/S_2^2}{\\sigma_1^2/\\sigma_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]\n\n\\[\\frac{s_1^2/\\sigma_1^2}{s_2^2/\\sigma_2^2} = \\frac{s_1^2/s_2^2}{\\sigma_1^2/\\sigma_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]"
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#alpha100-confidence-interval-for-sigma_12-sigma_22",
    "href": "slides/12-infer-variance-slides.html#alpha100-confidence-interval-for-sigma_12-sigma_22",
    "title": "Inference About Population Variances \n",
    "section": "\n\\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma_1^2 / \\sigma_2^2\\)\n",
    "text": "\\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma_1^2 / \\sigma_2^2\\)\n\n\n\n\\((1-\\alpha)100\\%\\) CI for \\(\\sigma_1^2 / \\sigma_2^2\\) is \\[\\color{blue}{\\left( \\frac{s_1^2/s_2^2}{F_{\\alpha/2, \\, n_1 - 1, \\, n_2 - 1}}, \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2, \\, \\, n_1 - 1, \\, n_2 - 1}} \\right)}\\]\n\n\n\n\n\n\n\n\n\n\n\n‚ùó The CI for \\(\\sigma_1^2 / \\sigma_2^2\\) cannot be expressed as \\(\\left(\\frac{s_1^2}{s_2^2}-m, \\frac{s_1^2}{s_2^2} + m\\right)\\) anymore!"
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#f-test-for-comparing-sigma_12-and-sigma_22",
    "href": "slides/12-infer-variance-slides.html#f-test-for-comparing-sigma_12-and-sigma_22",
    "title": "Inference About Population Variances \n",
    "section": "F test for comparing \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\)\n",
    "text": "F test for comparing \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\)\n\n\n\n\nStep 1: right-tailed  \\(\\small \\begin{align} &H_0: \\sigma_1 \\le \\sigma_2 \\\\ &H_1: \\sigma_1 &gt; \\sigma_2 \\end{align}\\)  and two-tailed  \\(\\small \\begin{align} &H_0: \\sigma_1 = \\sigma_2 \\\\ &H_1: \\sigma_1 \\ne \\sigma_2 \\end{align}\\) \n\n\n\n\n\nStep 2: \\(\\alpha = 0.05\\)\n\n\n\n\n\n\nStep 3: Under \\(H_0\\), \\(\\sigma_1 = \\sigma_2\\), and the test statistic is\n\n\\[\\small F_{test} = \\frac{s_1^2/s_2^2}{\\sigma_1^2/\\sigma_2^2} = \\frac{s_1^2}{s_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]\n\n\n\n\nStep 4-c:\n\nRight-tailed:  \\(F_{\\alpha, \\, n_1-1, \\, n_2-1}\\) .\nTwo-tailed:  \\(F_{\\alpha/2, \\, n_1-1, \\, n_2-1}\\) or \\(F_{1-\\alpha/2, \\, n_1-1, \\, n_2-1}\\) \n\n\n\n\n\n\n\n\nStep 5-c:\n\nRight-tailed: reject \\(H_0\\) if  \\(F_{test} \\ge F_{\\alpha, \\, n_1-1, \\, n_2-1}\\).\nTwo-tailed: reject \\(H_0\\) if  \\(F_{test} \\ge F_{\\alpha/2, \\, n_1-1, \\, n_2-1}\\) or \\(F_{test} \\le F_{1-\\alpha/2, \\, n_1-1, \\, n_2-1}\\)"
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#back-to-the-weight-loss-example",
    "href": "slides/12-infer-variance-slides.html#back-to-the-weight-loss-example",
    "title": "Inference About Population Variances \n",
    "section": "Back to the Weight Loss Example",
    "text": "Back to the Weight Loss Example\n\n\nA study was conducted to see the effectiveness of a weight loss program.\n\nTwo groups (Control and Experimental) of 10 subjects were selected.\nThe two populations are normally distributed and have the same SD.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data on weight loss was collected at the end of six months\n\n\nControl: \\(n_1 = 10\\), \\(\\overline{x}_1 = 2.1\\, lb\\), \\(s_1 = 0.5\\, lb\\)\n\n\nExperimental: \\(n_2 = 10\\), \\(\\overline{x}_2 = 4.2\\, lb\\), \\(s_2 = 0.7\\, lb\\)\n\n\n\n\nAssumptions:\n\n \\(\\sigma_1 = \\sigma_2\\) \nThe weight loss for both groups are normally distributed."
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#back-to-the-weight-loss-example-check-if-sigma_1-sigma_2",
    "href": "slides/12-infer-variance-slides.html#back-to-the-weight-loss-example-check-if-sigma_1-sigma_2",
    "title": "Inference About Population Variances \n",
    "section": "Back to the Weight Loss Example: Check if \\(\\sigma_1 = \\sigma_2\\)\n",
    "text": "Back to the Weight Loss Example: Check if \\(\\sigma_1 = \\sigma_2\\)\n\n\n\n\n\\(n_1 = 10\\), \\(s_1 = 0.5 \\, lb\\)\n\\(n_2 = 10\\), \\(s_2 = 0.7 \\, lb\\)\nStep 1: \\(\\begin{align} &H_0: \\sigma_1 = \\sigma_2 \\\\ &H_1: \\sigma_1 \\ne \\sigma_2 \\end{align}\\)\nStep 2: \\(\\alpha = 0.05\\)\nStep 3: \\(F_{test} = \\frac{s_1^2}{s_2^2} = \\frac{0.5^2}{0.7^2} = 0.51\\).\nStep 4-c: Two-tailed test. The critical value is \\(F_{0.05/2, \\, 10-1, \\, 10-1} = 4.03\\) or \\(F_{1-0.05/2, \\, 10-1, \\, 10-1} = 0.25\\).\n\n\n\n\n\n\n\n\n\n\n\n\nStep 5-c: Is \\(F_{test} &gt; 4.03\\) or \\(F_{test} &lt; 0.25\\)? No.\nStep 6: The evidence is not sufficient to reject the claim that \\(\\sigma_1 = \\sigma_2\\)."
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#back-to-the-weight-loss-example-95-ci-for-sigma_12-sigma_22",
    "href": "slides/12-infer-variance-slides.html#back-to-the-weight-loss-example-95-ci-for-sigma_12-sigma_22",
    "title": "Inference About Population Variances \n",
    "section": "Back to the Weight Loss Example: 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\)\n",
    "text": "Back to the Weight Loss Example: 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\)\n\n\n\n\n\n\n\nThe 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\) is \\[\\small \\begin{align} &\\left( \\frac{s_1^2/s_2^2}{F_{\\alpha/2, \\, df_1, \\, df_2}}, \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2, \\, df_1, \\, df_2}} \\right) \\\\ &= \\left( \\frac{0.51}{4.03}, \\frac{0.51}{0.25} \\right) = \\left(0.13, 2.04\\right)\\end{align}\\]\n\nWe are 95% confident that the ratio \\(\\sigma_1^2 / \\sigma_2^2\\) is between 0.13 and 2.04."
  },
  {
    "objectID": "slides/12-infer-variance-slides.html#implementing-f-test-in-r",
    "href": "slides/12-infer-variance-slides.html#implementing-f-test-in-r",
    "title": "Inference About Population Variances \n",
    "section": "Implementing F-test in R",
    "text": "Implementing F-test in R\n\n\n\n\nn1 &lt;- 10; n2 &lt;- 10\ns1 &lt;- 0.5; s2 &lt;- 0.7\nal &lt;- 0.05\n\n## 95% CI for sigma_1^2 / sigma_2^2\nf_small &lt;- qf(p = al / 2, \n              df1 = n1 - 1, df2 = n2 - 1, \n              lower.tail = TRUE)\nf_big &lt;- qf(p = al / 2, \n            df1 = n1 - 1, df2 = n2 - 1, \n            lower.tail = FALSE)\n\n\n## lower bound\n(s1 ^ 2 / s2 ^ 2) / f_big\n\n[1] 0.127\n\n## upper bound\n(s1 ^ 2 / s2 ^ 2) / f_small\n\n[1] 2.05\n\n\n\n\n\n## Testing sigma_1 = sigma_2\n(test_stats &lt;- s1 ^ 2 / s2 ^ 2)\n\n[1] 0.51\n\n(cri_big &lt;- qf(p = al / 2, \n               df1 = n1 - 1, \n               df2 = n2 - 1, \n               lower.tail = FALSE))\n\n[1] 4.03\n\n(cri_small &lt;- qf(p = al / 2, \n                 df1 = n1 - 1, \n                 df2 = n2 - 1, \n                 lower.tail = TRUE))\n\n[1] 0.248\n\n# var.test(x, y, alternative = \"two.sided\")"
  },
  {
    "objectID": "slides/14-correlation.html#relationship-between-2-numerical-variables",
    "href": "slides/14-correlation.html#relationship-between-2-numerical-variables",
    "title": "Correlation \n",
    "section": "Relationship Between 2 Numerical Variables",
    "text": "Relationship Between 2 Numerical Variables\n\nDepending on the situation, one of the variables is the explanatory variable and the other is the response variable. (Discussed in Regression)\nThere is not always an explanatory-response relationship.\n\nExamples:\n\n height and weight \n income and age \n SAT/ACT math score and verbal score \n amount of time spent studying for an exam and exam grade \n\n\n\n\nCan you provide an example that 2 variables are associated?"
  },
  {
    "objectID": "slides/14-correlation.html#scatterplots",
    "href": "slides/14-correlation.html#scatterplots",
    "title": "Correlation \n",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\n\n\n\n\n\n\n\nDescribe the overall pattern\n\nForm: linear or clusters\nDirection: positively associated or negatively associated\nStrength: how close the points lie to a line/curve"
  },
  {
    "objectID": "slides/14-correlation.html#linear-correlation-coefficient",
    "href": "slides/14-correlation.html#linear-correlation-coefficient",
    "title": "Correlation \n",
    "section": "Linear Correlation Coefficient",
    "text": "Linear Correlation Coefficient\n\nThe sample correlation coefficient, denoted by \\(r\\), measures the direction and strength of the linear relationship between two numerical variables: \\[\\small r :=\\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i-\\overline{x}}{s_x}\\right)\\left(\\frac{y_i-\\overline{y}}{s_y}\\right) = \\frac{1}{(n-1) (s_xs_y)}\\sum_{i=1}^n\\left(x_i-\\overline{x}\\right)\\left(y_i-\\overline{y}\\right)\\]\n\n\n\n\n\n\\(-1 \\le r\\le 1\\)\n\n\\(r &gt; 0\\): The larger value of \\(X\\) is, the larger value of \\(Y\\) tends toward.\n\n\\(r = 1\\): Perfect positive linear relationship."
  },
  {
    "objectID": "slides/14-correlation.html#linear-correlation-coefficient-1",
    "href": "slides/14-correlation.html#linear-correlation-coefficient-1",
    "title": "Correlation \n",
    "section": "Linear Correlation Coefficient",
    "text": "Linear Correlation Coefficient\n\nThe sample correlation coefficient, denoted by \\(r\\), measures the direction and strength of the linear relationship between two numerical variables: \\[\\small r :=\\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i-\\overline{x}}{s_x}\\right)\\left(\\frac{y_i-\\overline{y}}{s_y}\\right)\\]\n\n\n\n\n\n\\(-1 \\le r\\le 1\\)\n\n\\(r &lt; 0\\): The larger value of \\(X\\) is, the smaller value of \\(Y\\) tends toward.\n\n\\(r = -1\\): Perfect negative linear relationship."
  },
  {
    "objectID": "slides/14-correlation.html#linear-correlation-coefficient-2",
    "href": "slides/14-correlation.html#linear-correlation-coefficient-2",
    "title": "Correlation \n",
    "section": "Linear Correlation Coefficient",
    "text": "Linear Correlation Coefficient\n\nThe sample correlation coefficient, denoted by \\(r\\), measures the direction and strength of the linear relationship between two numerical variables: \\[\\small r :=\\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i-\\overline{x}}{s_x}\\right)\\left(\\frac{y_i-\\overline{y}}{s_y}\\right)\\]\n\n\n\n\n\n\n\\(r = 0\\): No linear relationship.\nIf explanatory and response are switched, \\(r\\) remains the same.\n\n\\(r\\) has no units of measurement, so scale changes do not affect \\(r\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProperties\n\n\n\\(-1 \\le r\\le 1\\).\nIf \\(r\\) is negative, variables \\(X\\) and \\(Y\\) are negatively related. That is, the larger value of \\(X\\) is, the smaller value of \\(Y\\) tends toward.\nIf \\(r = -1\\), there is a perfect negative linear relationship.\nIf \\(r\\) is positive, variables \\(X\\) and \\(Y\\) are positively related. That is, the larger value of \\(X\\) is, the larger value of \\(Y\\) tends toward.\nIf \\(r = 1\\), there is a perfect positive linear relationship.\nIf \\(r = 0\\), there is no linear relationship.\nIf explanatory and response are switched, \\(r\\) remains the same.\n\n\\(r\\) has no units of measurement associated with it.\nScale changes do not affect \\(r\\)."
  },
  {
    "objectID": "slides/14-correlation.html#correlation-example",
    "href": "slides/14-correlation.html#correlation-example",
    "title": "Correlation \n",
    "section": "Correlation Example",
    "text": "Correlation Example\n\nIt is possible that there is a strong relationship between two variables but still \\(r = 0\\).\n\n\n\n\n\n\nhttps://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg"
  },
  {
    "objectID": "slides/14-correlation.html#example-in-r",
    "href": "slides/14-correlation.html#example-in-r",
    "title": "Correlation \n",
    "section": "Example in R",
    "text": "Example in R\n\n\n\nplot(x = mtcars$wt, y = mtcars$mpg, \n     main = \"MPG vs. Weight\", \n     xlab = \"Car Weight\", \n     ylab = \"Miles Per Gallon\", \n     pch = 16, col = 4, las = 1)\n\n\n\n\n\n\n\n\n\ncor(x = mtcars$wt,\n    y = mtcars$mpg)\n\n[1] -0.87"
  },
  {
    "objectID": "slides/14-correlation-slides.html#relationship-between-2-numerical-variables",
    "href": "slides/14-correlation-slides.html#relationship-between-2-numerical-variables",
    "title": "Correlation \n",
    "section": "Relationship Between 2 Numerical Variables",
    "text": "Relationship Between 2 Numerical Variables\n\nDepending on the situation, one of the variables is the explanatory variable and the other is the response variable. (Discussed in Regression)\nThere is not always an explanatory-response relationship.\n\nExamples:\n\n height and weight \n income and age \n SAT/ACT math score and verbal score \n amount of time spent studying for an exam and exam grade \n\n\n\n\nCan you provide an example that 2 variables are associated?"
  },
  {
    "objectID": "slides/14-correlation-slides.html#scatterplots",
    "href": "slides/14-correlation-slides.html#scatterplots",
    "title": "Correlation \n",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\nDescribe the overall pattern\n\nForm: linear or clusters\nDirection: positively associated or negatively associated\nStrength: how close the points lie to a line/curve"
  },
  {
    "objectID": "slides/14-correlation-slides.html#linear-correlation-coefficient",
    "href": "slides/14-correlation-slides.html#linear-correlation-coefficient",
    "title": "Correlation \n",
    "section": "Linear Correlation Coefficient",
    "text": "Linear Correlation Coefficient\n\nThe sample correlation coefficient, denoted by \\(r\\), measures the direction and strength of the linear relationship between two numerical variables: \\[\\small r :=\\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i-\\overline{x}}{s_x}\\right)\\left(\\frac{y_i-\\overline{y}}{s_y}\\right) = \\frac{1}{(n-1) (s_xs_y)}\\sum_{i=1}^n\\left(x_i-\\overline{x}\\right)\\left(y_i-\\overline{y}\\right)\\]\n\n\n\n\n\n\\(-1 \\le r\\le 1\\)\n\n\\(r &gt; 0\\): The larger value of \\(X\\) is, the larger value of \\(Y\\) tends toward.\n\n\\(r = 1\\): Perfect positive linear relationship."
  },
  {
    "objectID": "slides/14-correlation-slides.html#linear-correlation-coefficient-1",
    "href": "slides/14-correlation-slides.html#linear-correlation-coefficient-1",
    "title": "Correlation \n",
    "section": "Linear Correlation Coefficient",
    "text": "Linear Correlation Coefficient\n\nThe sample correlation coefficient, denoted by \\(r\\), measures the direction and strength of the linear relationship between two numerical variables: \\[\\small r :=\\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i-\\overline{x}}{s_x}\\right)\\left(\\frac{y_i-\\overline{y}}{s_y}\\right)\\]\n\n\n\n\n\n\\(-1 \\le r\\le 1\\)\n\n\\(r &lt; 0\\): The larger value of \\(X\\) is, the smaller value of \\(Y\\) tends toward.\n\n\\(r = -1\\): Perfect negative linear relationship."
  },
  {
    "objectID": "slides/14-correlation-slides.html#linear-correlation-coefficient-2",
    "href": "slides/14-correlation-slides.html#linear-correlation-coefficient-2",
    "title": "Correlation \n",
    "section": "Linear Correlation Coefficient",
    "text": "Linear Correlation Coefficient\n\nThe sample correlation coefficient, denoted by \\(r\\), measures the direction and strength of the linear relationship between two numerical variables: \\[\\small r :=\\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i-\\overline{x}}{s_x}\\right)\\left(\\frac{y_i-\\overline{y}}{s_y}\\right)\\]\n\n\n\n\n\n\n\\(r = 0\\): No linear relationship.\nIf explanatory and response are switched, \\(r\\) remains the same.\n\n\\(r\\) has no units of measurement, so scale changes do not affect \\(r\\).\n\n\n\n\n\n\n\n\n\n\n\n\nProperties\n\n\n\\(-1 \\le r\\le 1\\).\nIf \\(r\\) is negative, variables \\(X\\) and \\(Y\\) are negatively related. That is, the larger value of \\(X\\) is, the smaller value of \\(Y\\) tends toward.\nIf \\(r = -1\\), there is a perfect negative linear relationship.\nIf \\(r\\) is positive, variables \\(X\\) and \\(Y\\) are positively related. That is, the larger value of \\(X\\) is, the larger value of \\(Y\\) tends toward.\nIf \\(r = 1\\), there is a perfect positive linear relationship.\nIf \\(r = 0\\), there is no linear relationship.\nIf explanatory and response are switched, \\(r\\) remains the same.\n\n\\(r\\) has no units of measurement associated with it.\nScale changes do not affect \\(r\\)."
  },
  {
    "objectID": "slides/14-correlation-slides.html#correlation-example",
    "href": "slides/14-correlation-slides.html#correlation-example",
    "title": "Correlation \n",
    "section": "Correlation Example",
    "text": "Correlation Example\n\nIt is possible that there is a strong relationship between two variables but still \\(r = 0\\).\n\n\n\n\n\n\nhttps://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg"
  },
  {
    "objectID": "slides/14-correlation-slides.html#example-in-r",
    "href": "slides/14-correlation-slides.html#example-in-r",
    "title": "Correlation \n",
    "section": "Example in R",
    "text": "Example in R\n\n\n\nplot(x = mtcars$wt, y = mtcars$mpg, \n     main = \"MPG vs. Weight\", \n     xlab = \"Car Weight\", \n     ylab = \"Miles Per Gallon\", \n     pch = 16, col = 4, las = 1)\n\n\n\n\n\n\n\n\n\ncor(x = mtcars$wt,\n    y = mtcars$mpg)\n\n[1] -0.87"
  },
  {
    "objectID": "slides/06-probability.html#why-study-probability",
    "href": "slides/06-probability.html#why-study-probability",
    "title": "Probability Fundamentals üé≤",
    "section": "Why Study Probability",
    "text": "Why Study Probability\n\nWe live in a world full of chances and uncertainty!\n\n\n\n\nSep 13, 2025\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe always ask questions like what are the chances of XXX\nI just typed what are the chances in Google, and these are the auto-completions.\nIt looks like US people are curious about getting pregnant, getting COVID of course, and getting struck by lightning or tornado."
  },
  {
    "objectID": "slides/06-probability.html#section-1",
    "href": "slides/06-probability.html#section-1",
    "title": "Probability Fundamentals üé≤",
    "section": "",
    "text": "And of course, it is the chance or uncertainty that makes so many games so fun, amusing and even additive."
  },
  {
    "objectID": "slides/06-probability.html#why-probability-before-statistics",
    "href": "slides/06-probability.html#why-probability-before-statistics",
    "title": "Probability Fundamentals üé≤",
    "section": "Why Probability Before Statistics?",
    "text": "Why Probability Before Statistics?\n\n\n Probability : We know the process generating the data and are interested in properties of observations.\n Statistics : We observed the data (sample) and are interested in determining what is the process generating the data (population).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics is based on probability, and while probability is not required for the applied techniques in this book, it may help you gain a deeper understanding of the methods and set a better foundation for future courses."
  },
  {
    "objectID": "slides/06-probability.html#interpretation-of-probability-relative-frequency",
    "href": "slides/06-probability.html#interpretation-of-probability-relative-frequency",
    "title": "Probability Fundamentals üé≤",
    "section": "Interpretation of Probability: Relative Frequency\n",
    "text": "Interpretation of Probability: Relative Frequency\n\n\n\n Relative Frequency : The probability that some outcome of a process will be obtained is interpreted as the relative frequency with which that outcome would be obtained if the process were repeated a large number of times independently under similar conditions.\n\n\n\n\n      Frequency Relative Frequency\nHeads         1                0.1\nTails         9                0.9\nTotal        10                1.0\n---------------------\n      Frequency Relative Frequency\nHeads       515              0.515\nTails       485              0.485\nTotal      1000              1.000\n---------------------\n\n\n\nIf we repeat tossing the coin 10 times, the probability of obtaining heads is 10%.\nIf 1000 times, the probability is 51.5%.\n\n\n\nSuppose we toss a coin.\n\nIf we repeat tossing the coin 10 times, the probability of obtaining heads is 10%.\nIf 1000 times, the probability is 51.5%.\n\n\n\n\n. . .\n\nAny issue of relative frequency probability?"
  },
  {
    "objectID": "slides/06-probability.html#issues-of-relative-frequency",
    "href": "slides/06-probability.html#issues-of-relative-frequency",
    "title": "Probability Fundamentals üé≤",
    "section": "Issues of Relative Frequency\n",
    "text": "Issues of Relative Frequency\n\n\nüòï How large of a number is large enough?\n\n. . .\n\nüòï Meaning of ‚Äúunder similar conditions‚Äù\n\n\n\nairflow, temperature, same person, same coin?\n\n\n. . .\n\nüòï The relative frequency is reliable under identical conditions?\n\n\nskilled person\n\n. . .\n\nüëâ We only obtain an approximation instead of exact value.\n\n. . .\n\nüòÇ How do you compute the probability that Chicago Cubs wins the World Series next year?\n\n\n\n\n\n\n\n\n\n\n\nMost importantly, some process or experiment cannot be replicated or repeated."
  },
  {
    "objectID": "slides/06-probability.html#interpretation-of-probability-classical-approach",
    "href": "slides/06-probability.html#interpretation-of-probability-classical-approach",
    "title": "Probability Fundamentals üé≤",
    "section": "Interpretation of Probability: Classical Approach\n",
    "text": "Interpretation of Probability: Classical Approach\n\n\n Classical probability : The probability is based on the concept of equally likely outcomes.\nIf the outcome of some process must be one of \\(n\\) different outcomes, the probability of each outcome is \\(1/n\\).\n\n. . .\n\nExample:\n\ntoss a fair coin (2 outcomes) ü™ô\nroll a well-balanced die (6 outcomes) üé≤\ndraw one from a deck of cards (52 outcomes) üÉè\n\n\n\n. . .\n\nAny issue of classical probability?\n\n. . .\n\n\nThe probability that [you name it] wins the World Series next year is 1/30?!\n\n\n\n\n\n\n\n\n\n\n\nNo systematic method is given for assigning probabilities to outcomes that are not assumed to be equally likely.\nThis interpretation or definition of probability is way too naive right!\nOnly get pregnant or not get pregnant, but We cannot say the chance of getting pregnant is 1/2.\nOnly get COVID or not get COVID, but We cannot say the chance of getting COVID is 1/2."
  },
  {
    "objectID": "slides/06-probability.html#interpretation-of-probability-subjective-approach",
    "href": "slides/06-probability.html#interpretation-of-probability-subjective-approach",
    "title": "Probability Fundamentals üé≤",
    "section": "Interpretation of Probability: Subjective Approach\n",
    "text": "Interpretation of Probability: Subjective Approach\n\n\n Subjective probability : The probability is assigned or estimated using people‚Äôs knowledge, beliefs and information about the data generating process.\nA person‚Äôs subjective probability of an outcome, rather than the true probability of that outcome.\n\n. . .\n\nI think ‚Äúthe probability that Milwaukee Brewers wins the World Series this year is 30%‚Äù.\n\n. . .\n\nMy probability that Milwaukee Brewers wins the World Series next year is different from an ESPN MLB analyst‚Äôs probability.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubjective probability: The probability is assigned or estimated using people‚Äôs knowledge, beliefs and information about the data generating process, or hoe the outcome is obtained from some experiment.\nI think ‚Äúthe probability that Milwaukee Brewers wins the World Series this year is 30%‚Äù.\nSince the probability is subjective, my probability that Milwaukee Brewers wins the World Series next year is different from an ESPN MLB analyst‚Äôs probability."
  },
  {
    "objectID": "slides/06-probability.html#section-2",
    "href": "slides/06-probability.html#section-2",
    "title": "Probability Fundamentals üé≤",
    "section": "",
    "text": "Any probability operations and rules do NOT depend on interpretation of probability!\n\n\n\nno worries about which interpretation we gonna use before we do probability calculations. OK."
  },
  {
    "objectID": "slides/06-probability.html#experiments-events-and-sample-space",
    "href": "slides/06-probability.html#experiments-events-and-sample-space",
    "title": "Probability Fundamentals üé≤",
    "section": "Experiments, Events and Sample Space",
    "text": "Experiments, Events and Sample Space\n\nExperiment: any process in which the possible outcomes can be identified ahead of time.\nEvent: a set of possible outcomes of the experiment.\nSample space \\((\\mathcal{S})\\) of an experiment: the collection of ALL possible outcomes of the experiment.\n\n\n\n\n\n\n\n\n\nExperiment\nPossible Outcomes\nSome Events\nSample Space\n\n\n\nFlip a coin ü™ô\nHeads, Tails\n{Heads}, {Heads, Tails}, ‚Ä¶\n{Heads, Tails}\n\n\nRoll a die üé≤\n1, 2, 3, 4, 5, 6\n{1, 3, 5}, {2, 4, 6}, {2}, {3, 4, 5, 6}, ‚Ä¶\n{1, 2, 3, 4, 5, 6}\n\n\n\n\n\nFlipping a coin is an experiment because we can identify possible outcomes before we do the experiment, which are heads and tails.\nRolling a die is an experiment because we can identify possible outcomes before we do the experiment, which are numbers 1, 2, 3, 4, 5, 6.\nWe use curly braces to indicate a set. Any elements in the set are inside the curly braces.\n\n\n. . .\n\nIs the sample space also an event?\n\n. . .\n\nYes, the sample space itself is an event because it is also a set of possible outcomes of the experiment."
  },
  {
    "objectID": "slides/06-probability.html#set-concept-example-of-rolling-a-six-side-balanced-die",
    "href": "slides/06-probability.html#set-concept-example-of-rolling-a-six-side-balanced-die",
    "title": "Probability Fundamentals üé≤",
    "section": "Set Concept: Example of Rolling a six-side balanced die\n",
    "text": "Set Concept: Example of Rolling a six-side balanced die\n\n\nDraw a Venn Diagram every time you get stuck!\n\n\n\n Complement  of an event (set) \\(A\\),  \\(A^c\\) : a set of all outcomes (elements) of \\(\\mathcal{S}\\) in which \\(A\\) does not occur.\n\nLet \\(A\\) be an event that a number greater than 2. Then \\(A = \\{3, 4, 5, 6\\}\\) and \\(A^c = \\{1, 2\\}\\).\n\n\n\n\n\nBefore formally introduce probability operations, we need some basic set concepts because probability is defined on a set, or event.\nVenn diagram is a very useful tool for identifying a set, so I encourage you to draw a venn diagram when you get stuck on complicated set operations.\nThis applet is very helpful. Try to check each set and see if you fully understand concept of union, intersection, complement and containment or subset.\nAn Event is usually described by words, and we need to convert the words into the set it is referring to.\n\n\n. . .\n\n\n Union \\((A \\cup B)\\) : a set of all outcomes of \\(\\mathcal{S}\\) in \\(A\\) or \\(B\\).\n\n\nLet \\(B\\) be an event that an even number is obtained. (What is \\(B\\) in terms of a set?)\n \\(B = \\{2, 4, 6\\}\\), \\(A \\cup B = \\{2, 3, 4, 5, 6\\}\\).\n\n\n\n\n\nCollect all elements in A and in B.\nKeep unique ones.\nAn element in \\((A \\cup B)\\) either belongs to event A only, B only or belong to A and B.\n\n\n. . .\n\n\n Intersection \\((A \\cap B)\\) : a set of all outcomes of \\(\\mathcal{S}\\) in both \\(A\\) and \\(B\\).\n\n \\(A \\cap B = \\{4, 6\\}\\).\n\n\n\n\n\nthe elements in the set \\((A \\cap B)\\) should occur in both A and B."
  },
  {
    "objectID": "slides/06-probability.html#set-concept-example-of-rolling-a-six-side-balanced-die-1",
    "href": "slides/06-probability.html#set-concept-example-of-rolling-a-six-side-balanced-die-1",
    "title": "Probability Fundamentals üé≤",
    "section": "Set Concept: Example of Rolling a six-side balanced die\n",
    "text": "Set Concept: Example of Rolling a six-side balanced die\n\n\n\n\\(A\\) and \\(B\\) are disjoint (or mutually exclusive) if they have no outcomes in common \\((A \\cap B = \\emptyset)\\).\n\n\n\\(\\emptyset\\) means an empty set, \\(\\{\\}\\), i.e., no elements in the set.\n Let \\(C\\) be an event that an odd number is obtained. Then \\(C = \\{1, 3, 5\\}\\) and \\(B \\cap C = \\emptyset\\). \n\n\n\n\n\nAll elements in A are also elements in B. But there may be some B‚Äôs elements that are not in A.\nSet D has more elements than B, and B‚Äôs elements also belong to D.\n\n\n. . .\n\n\n\n\nContainment \\((A \\subset B)\\): every elements of \\(A\\) also belongs to \\(B\\). If \\(A\\) occurs then so does \\(B\\).\n\n \\(B\\) is an event that an even number is obtained. \n \\(D\\) is an event that a number greater than 1 is obtained. \n \\(B = \\{2, 4, 6\\}\\) and \\(D = \\{2, 3, 4, 5, 6\\}\\). \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\n\\(B \\subset D\\) or \\(D \\subset B\\)?"
  },
  {
    "objectID": "slides/06-probability.html#probability-rules",
    "href": "slides/06-probability.html#probability-rules",
    "title": "Probability Fundamentals üé≤",
    "section": "Probability Rules",
    "text": "Probability Rules\nDenote the probability of an event \\(A\\) on a sample space \\(\\mathcal{S}\\) as \\(P(A)\\).\n\nTreat the probability of an event as the area of the event in the Venn diagram.\n\n\n\n\nAxioms\n\n\\(P(\\mathcal{S}) = 1\\)\nFor any event \\(A\\), \\(P(A) \\ge 0\\)\n\nIf \\(A\\) and \\(B\\) are disjoint, \\(P(A \\cup B) = P(A) + P(B)\\)\n\n\n\n\n\n. . .\n\n\n\nProperties\n\n\n\\(P(\\emptyset) = 0\\).\n\\(0 \\le P(A) \\le 1\\)\n\\(P(A^c) = 1 - P(A)\\)\n\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) (Addition Rule)\nIf \\(A \\subset B\\), then \\(P(A) \\le P(B)\\)\n\n\n\n\n\n\n\nNow we are ready to formally define a probability and it‚Äôs rules.\nTreat probability as the area in the Venn diagram. For example, \\(P(\\mathcal{S})\\) is the area of the sample space, or the area of the rectangle, which is normalized to be 1."
  },
  {
    "objectID": "slides/06-probability.html#venn-diagram-illustration",
    "href": "slides/06-probability.html#venn-diagram-illustration",
    "title": "Probability Fundamentals üé≤",
    "section": "Venn Diagram Illustration",
    "text": "Venn Diagram Illustration\n\n\nAddition Rule: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\n\n\n\n\n\n\n\n\n. . .\n\n\nDisjoint case: \\(P(A \\cup B) = P(A) + P(B)\\) because \\(P(A \\cap B) = 0\\)!"
  },
  {
    "objectID": "slides/06-probability.html#example-mm-colors",
    "href": "slides/06-probability.html#example-mm-colors",
    "title": "Probability Fundamentals üé≤",
    "section": "Example: M&M Colors",
    "text": "Example: M&M Colors\nThe makers of the candy M&Ms report that their plain M&Ms are composed of\n\n15% Yellow; 10% Red; 20% Orange; 25% Blue; 15% Green; 15% Brown\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you randomly select an M&M, what is the probability of the following?\n\n\nIt is brown.\n\n\nIt is red or green.\n\n\nIt is not blue.\n\n\nIt is red and brown.\n\n\n\n\n\n\n\nGive you 2 minutes!"
  },
  {
    "objectID": "slides/06-probability.html#example-mm-colors-1",
    "href": "slides/06-probability.html#example-mm-colors-1",
    "title": "Probability Fundamentals üé≤",
    "section": "Example: M&M Colors",
    "text": "Example: M&M Colors\n\n15% Yellow; 10% Red; 20% Orange; 25% Blue; 15% Green; 15% Brown\n\n\n\n\nIf you randomly select an M&M, what is the probability of the following?\n\n\nIt is brown.\n\n\nIt is red or green.\n\n\nIt is not blue.\n\n\nIt is red and brown.\n\n\n\n\n\n\\(P(\\mathrm{Brown}) = 0.15\\)\n\\(\\small \\begin{align} P(\\mathrm{Red} \\cup \\mathrm{Green}) &= P(\\mathrm{Red}) + P(\\mathrm{Green}) - P(\\mathrm{Red} \\cap \\mathrm{Green}) \\\\ &= 0.10 + 0.15 - 0 = 0.25 \\end{align}\\)\n\\(P(\\text{Not Blue}) = 1 - P(\\text{Blue}) = 1 - 0.25 = 0.75\\)\n\\(P(\\text{Red and Brown}) = P(\\emptyset) = 0\\)\n\n\n\n. . .\n\nBy the way, which interpretation of probability is used in this question?"
  },
  {
    "objectID": "slides/06-probability.html#conditional-probability-1",
    "href": "slides/06-probability.html#conditional-probability-1",
    "title": "Probability Fundamentals üé≤",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nThe conditional probability of \\(A\\) given \\(B\\) is\n\n\\[ P(A \\mid  B) = \\frac{P(A \\cap B)}{P(B)} \\] if \\(P(B) &gt; 0\\), and it is undefined if \\(P(B) = 0\\). \n\n‚ÄúGiven \\(B\\)‚Äù means that event \\(B\\) has already occurred.\n\n. . .\n\n\n\n\n\n\n\n\n\n\n. . .\n\nMultiplication Rule: \\(P(A \\cap B) = P(A \\mid  B)P(B) = P(B \\mid  A)P(A)\\)\n\\(P(A)\\) and \\(P(B)\\) are unconditional or marginal probabilities.\n\n\n\nRead \\(A | B\\) as ‚Äú\\(A\\) given \\(B\\)‚Äù\nWhen we compute the probability of A, the information about B have been given, and the probability of A is adjusted, according to this information.\nThe probability of A may depend on whether B happens or not.\n\n\\(P(A | B)\\) computes the probability of A given that B has been occurred. At this moment \\(P(B)\\) is 1.\n\n\\(P(B)\\) is scaled up by \\(1 / P(B)\\) so that \\(P(B) \\times \\frac{1}{P(B)} = 1\\).\n\n\\(P(A) \\ne P(A|B)\\) in general.\nThe conditional probability is the ratio of \\(P(A \\cap B)\\) and \\(P(B)\\).\nThe proportion of area of A to the area of B.\nThe probability is the area of \\(A \\cap B\\) / area of B"
  },
  {
    "objectID": "slides/06-probability.html#difference-between-pa-and-pa-mid-b",
    "href": "slides/06-probability.html#difference-between-pa-and-pa-mid-b",
    "title": "Probability Fundamentals üé≤",
    "section": "Difference Between \\(P(A)\\) and \\(P(A \\mid B)\\)\n",
    "text": "Difference Between \\(P(A)\\) and \\(P(A \\mid B)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we don‚Äôt have any specific information, what we can base on is the entire sample space, and the probability of A is the ratio of area of A to the area of entire sample space which is 1.\nNow if we know B has occurred, we don‚Äôt need to consider the entire sample space any more.\nInstead, we focus only on B cuz we know B has occurred, \\(B^c\\) becomes totally irrelevant.\nTo find \\(P(A \\mid  B)\\), we just need to find how large part of B that also belongs to A.\nExample, when we don‚Äôt have any info about women, to compute a woman &gt; 20 yrs old, we base on the entire female population of interest. But if we do know that the woman has a BA degree, we shrink our focus on women who have a BA degree, and compute the proportion of the women pool that has age over 20."
  },
  {
    "objectID": "slides/06-probability.html#example-peanut-butter-and-jelly",
    "href": "slides/06-probability.html#example-peanut-butter-and-jelly",
    "title": "Probability Fundamentals üé≤",
    "section": "Example: Peanut Butter and Jelly",
    "text": "Example: Peanut Butter and Jelly\n\nSuppose 80% of people like peanut butter, 89% like jelly, and 78% like both.\nGiven that a randomly sampled person likes peanut butter, what‚Äôs the probability that she also likes jelly?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe want \\(P(J\\mid PB) = \\frac{P(PB \\cap J)}{P(PB)}\\).\nFrom the problem we have \\(P(PB) = 0.8\\), \\(P(J) = 0.89\\), \\(P(PB \\cap J) = 0.78\\)\n\\(P(J\\mid PB) = \\frac{P(PB \\cap J)}{P(PB)} = \\frac{0.78}{0.8} = 0.975\\).\n\n\n\n\nIf we don‚Äôt know if the person loves peanut butter, the probability that she loves jelly is 89%.\nIf we do know she loves peanut butter, the probability that she loves jelly is going up to 97.5%."
  },
  {
    "objectID": "slides/06-probability.html#independence-1",
    "href": "slides/06-probability.html#independence-1",
    "title": "Probability Fundamentals üé≤",
    "section": "Independence",
    "text": "Independence\n\n\\(A\\) and \\(B\\) are independent if \\(\\begin{align} P(A \\mid  B) &= P(A) \\text{ or }\\\\ P(B \\mid  A) &= P(B) \\text{ or } \\\\P(A\\cap B) &= P(A)P(B)\\end{align}\\) \\(\\text{ if } P(A) &gt; 0 \\text{ and } P(B) &gt; 0\\)\nIntuition: Knowing \\(B\\) occurs does not change the probability that \\(A\\) occurs, and vice versa.\n\n\n\nThe information about B is irrelevant to probability of A.\nExample:\n\n\n. . .\n\nCan we compute \\(P(A \\cap B)\\) if we only know \\(P(A)\\) and \\(P(B)\\)?\n\n. . .\n\nNo, we cannot compute \\(P(A \\cap B)\\) since we do not know if \\(A\\) and \\(B\\) are independent.\nWe could only if \\(A\\) and \\(B\\) were independent.\nIn general, we need the multiplication rule \\(P(A \\cap B) = P(A \\mid B)P(B)\\)."
  },
  {
    "objectID": "slides/06-probability.html#venn-diagram-explanation-of-independence",
    "href": "slides/06-probability.html#venn-diagram-explanation-of-independence",
    "title": "Probability Fundamentals üé≤",
    "section": "Venn Diagram Explanation of Independence",
    "text": "Venn Diagram Explanation of Independence\n\n\n\n\n\n\n\n\n\n\n\n\nIndependence means that the ratio of area of A to area of S is the same as the ratio of area of A&B to area of B.\nLook at the case of non-independence.\nLook at the circles. The area of A&B is very close to the area of B.\nIt means that the ratio of area of \\(A\\cap B\\) to area of B is pretty close to 1.\nSo given B, [ ]\nIn this case, the information about B does matter, and affect probability of A.\nIf we know B occurred, there is pretty high chance that A will occur as well. OK."
  },
  {
    "objectID": "slides/06-probability.html#independence-example",
    "href": "slides/06-probability.html#independence-example",
    "title": "Probability Fundamentals üé≤",
    "section": "Independence Example",
    "text": "Independence Example\n\n\nAssuming that events \\(A\\) and \\(B\\) are independent. \\(P(A) = 0.3\\) and \\(P(B) = 0.7\\).\n\n\n\\(P(A \\cap B)\\)?\n\n\\(P(A \\cup B)\\)?\n\n\\(P(A \\mid B)\\)?\n\n\n\n\n\n. . .\n\n\\(P(A \\cap B) = P(A)P(B)=0.21\\)\n\n. . .\n\n\\(P(A \\cup B) = P(A)+P(B)-P(A\\cap B) = 0.3+0.7-0.21=0.79\\)\n\n. . .\n\n\\(P(A \\mid B) = P(A) = 0.3\\)"
  },
  {
    "objectID": "slides/06-probability.html#why-bayes-formula",
    "href": "slides/06-probability.html#why-bayes-formula",
    "title": "Probability Fundamentals üé≤",
    "section": "Why Bayes‚Äô Formula?",
    "text": "Why Bayes‚Äô Formula?\n\nOften, we know \\(P(B \\mid A)\\) but are much more interested in \\(P(A \\mid B)\\).\nExample: diagnostic tests provide \\(P(\\text{positive test result}  \\mid \\text{COVID})\\), but we are interested in \\(P(\\text{COVID} \\mid \\text{positive test result})\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n\nBayes‚Äô formula provides a way for finding \\(P(A \\mid B)\\) from \\(P(B \\mid A)\\)\n\n\n\n\nOften, we know the conditional probability \\(P(B \\mid A)\\) but are much more interested in \\(P(A \\mid B)\\)\n\nFor example, diagnostic tests provide \\(P(\\text{positive test result}  \\mid \\text{disease})\\), but we are interested in \\(P(\\text{disease} \\mid \\text{positive test result})\\)"
  },
  {
    "objectID": "slides/06-probability.html#bayes-formula-2",
    "href": "slides/06-probability.html#bayes-formula-2",
    "title": "Probability Fundamentals üé≤",
    "section": "Bayes‚Äô Formula",
    "text": "Bayes‚Äô Formula\n\nIf \\(A\\) and \\(B\\) are any events whose probabilities are not 0 or 1, then\n\n\n\\[\\begin{align*} P(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)} \\quad ( \\text{def. of cond. prob.}) \\\\ &= \\frac{P(A \\cap B)}{P((B \\cap A) \\cup (B \\cap A^c))} \\quad ( \\text{partition } B) \\\\ &= \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)}  \\quad ( \\text{multiplication rule}) \\end{align*}\\]\n\n\n\n\n\n\n\n\n\n\n\nHint:  Partition the event \\(B\\) as a union of disjoint sets: \\(B = (B \\cap A) \\cup (B \\cap A^c)\\) (Check Venn diagram) and apply the multiplication rule.\n\nStart from the definition of conditional probability, we have \\[ P(A \\mid  B) = \\frac{P(A \\cap B)}{P(B)} = \\cdots = \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)}\\]"
  },
  {
    "objectID": "slides/06-probability.html#example-passing-rate",
    "href": "slides/06-probability.html#example-passing-rate",
    "title": "Probability Fundamentals üé≤",
    "section": "Example: Passing Rate",
    "text": "Example: Passing Rate\n\n\nAfter taking MATH 4720, \\(80\\%\\) of students understand the Bayes‚Äô formula.\n\nOf those who understand the Bayes‚Äô formula,\n\n\n\\(95\\%\\) passed\n\n\nOf those who do not understand the Bayes‚Äô formula,\n\n\n\\(60\\%\\) passed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the probability that a student understand the Bayes‚Äô formula given the fact that she passed."
  },
  {
    "objectID": "slides/06-probability.html#bayes-formula-step-by-step",
    "href": "slides/06-probability.html#bayes-formula-step-by-step",
    "title": "Probability Fundamentals üé≤",
    "section": "Bayes Formula: Step-by-Step",
    "text": "Bayes Formula: Step-by-Step\n\n\\(80\\%\\) of students understand the Bayes‚Äô formula.\nOf those who understand the Bayes‚Äô formula, \\(95\\%\\) passed ( \\(5\\%\\) failed).\nOf those who do not understand the formula, \\(60\\%\\) passed ( \\(40\\%\\) failed).\n\n\n\\[P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)}\\]\n\n. . .\n\n Step 1: Formulate what we would like to compute \n\n. . .\n\\(P(\\text{understand} \\mid \\text{passed})\\)\n. . .\n\n Step 2: Define relevant events in the formula: \\(A\\), \\(A^c\\) and \\(B\\) \n\n. . .\nLet \\(A =\\) understand. \\(B =\\) passed. Then \\(A^c =\\) don‚Äôt understand and \\(P(\\text{understand} \\mid \\text{passed}) = P(A \\mid B)\\)."
  },
  {
    "objectID": "slides/06-probability.html#bayes-formula-step-by-step-1",
    "href": "slides/06-probability.html#bayes-formula-step-by-step-1",
    "title": "Probability Fundamentals üé≤",
    "section": "Bayes Formula: Step-by-Step",
    "text": "Bayes Formula: Step-by-Step\n\n\\(80\\%\\) of students understand the Bayes‚Äô formula.\nOf those who understand the Bayes‚Äô formula, \\(95\\%\\) passed ( \\(5\\%\\) failed).\nOf those who do not understand the formula, \\(60\\%\\) passed ( \\(40\\%\\) failed).\n\n\n\\[P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)}\\]\n\n\n Step 3: Find probabilities in the Bayes‚Äô formula using provided information. \n\n\\(P(B \\mid A) = P(\\text{passed} \\mid \\text{understand}) = 0.95\\), \\(P(B \\mid A^c) = P(\\text{passed} \\mid \\text{don't understand}) = 0.6\\)\\(P(A) = P(\\text{understand}) = 0.8\\), \\(P(A^c) = 1 - P(A) = 0.2\\).\n. . .\n\n Step 4: Apply Bayes‚Äô formula. \n\n\\(P(\\text{understand} \\mid \\text{passed}) = P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)} = \\frac{(0.95)(0.8)}{(0.95)(0.8) + (0.6)(0.2)} = 0.86\\)"
  },
  {
    "objectID": "slides/06-probability.html#bayes-formula-tree-diagram-illustration",
    "href": "slides/06-probability.html#bayes-formula-tree-diagram-illustration",
    "title": "Probability Fundamentals üé≤",
    "section": "Bayes Formula: Tree Diagram Illustration",
    "text": "Bayes Formula: Tree Diagram Illustration\n\n\\(80\\%\\) of students understand the Bayes‚Äô formula.\nOf those who understand the Bayes‚Äô formula, \\(95\\%\\) passed ( \\(5\\%\\) failed).\nOf those who do not understand the formula, \\(60\\%\\) passed ( \\(40\\%\\) failed).\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*} & P(\\text{yes} \\mid \\text{pass}) \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass})} \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass and yes}) + P(\\text{pass and no})}\\\\ &= \\frac{0.76}{0.76 + 0.12} = 0.86 \\end{align*}\\]\n\n\n\n\\[\\begin{align*} & P(\\text{yes} \\mid \\text{pass}) \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass})} \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass and yes}) + P(\\text{pass and no})}\\\\ &= \\frac{P(\\text{pass | yes})P(\\text{yes})}{P(\\text{pass | yes})P(\\text{yes}) + P(\\text{pass | no})P(\\text{no})} \\\\ &= \\frac{0.76}{0.76 + 0.12} = 0.86 \\end{align*}\\]"
  },
  {
    "objectID": "slides/06-probability-slides.html#why-study-probability",
    "href": "slides/06-probability-slides.html#why-study-probability",
    "title": "Probability Fundamentals üé≤",
    "section": "Why Study Probability",
    "text": "Why Study Probability\n\nWe live in a world full of chances and uncertainty!\n\n\n\n\nSep 13, 2025\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\n\n\n\n\n\n\n\n\n\nWe always ask questions like what are the chances of XXX\nI just typed what are the chances in Google, and these are the auto-completions.\nIt looks like US people are curious about getting pregnant, getting COVID of course, and getting struck by lightning or tornado."
  },
  {
    "objectID": "slides/06-probability-slides.html#section-1",
    "href": "slides/06-probability-slides.html#section-1",
    "title": "Probability Fundamentals üé≤",
    "section": "",
    "text": "And of course, it is the chance or uncertainty that makes so many games so fun, amusing and even additive."
  },
  {
    "objectID": "slides/06-probability-slides.html#why-probability-before-statistics",
    "href": "slides/06-probability-slides.html#why-probability-before-statistics",
    "title": "Probability Fundamentals üé≤",
    "section": "Why Probability Before Statistics?",
    "text": "Why Probability Before Statistics?\n\n\n Probability : We know the process generating the data and are interested in properties of observations.\n Statistics : We observed the data (sample) and are interested in determining what is the process generating the data (population).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics is based on probability, and while probability is not required for the applied techniques in this book, it may help you gain a deeper understanding of the methods and set a better foundation for future courses."
  },
  {
    "objectID": "slides/06-probability-slides.html#interpretation-of-probability-relative-frequency",
    "href": "slides/06-probability-slides.html#interpretation-of-probability-relative-frequency",
    "title": "Probability Fundamentals üé≤",
    "section": "Interpretation of Probability: Relative Frequency\n",
    "text": "Interpretation of Probability: Relative Frequency\n\n\n\n Relative Frequency : The probability that some outcome of a process will be obtained is interpreted as the relative frequency with which that outcome would be obtained if the process were repeated a large number of times independently under similar conditions.\n\n\n\n\n      Frequency Relative Frequency\nHeads         1                0.1\nTails         9                0.9\nTotal        10                1.0\n---------------------\n      Frequency Relative Frequency\nHeads       515              0.515\nTails       485              0.485\nTotal      1000              1.000\n---------------------\n\n\n\nIf we repeat tossing the coin 10 times, the probability of obtaining heads is 10%.\nIf 1000 times, the probability is 51.5%.\n\n\nSuppose we toss a coin.\n\nIf we repeat tossing the coin 10 times, the probability of obtaining heads is 10%.\nIf 1000 times, the probability is 51.5%.\n\n\n\n\nAny issue of relative frequency probability?"
  },
  {
    "objectID": "slides/06-probability-slides.html#issues-of-relative-frequency",
    "href": "slides/06-probability-slides.html#issues-of-relative-frequency",
    "title": "Probability Fundamentals üé≤",
    "section": "Issues of Relative Frequency\n",
    "text": "Issues of Relative Frequency\n\n\nüòï How large of a number is large enough?\n\n\n\nüòï Meaning of ‚Äúunder similar conditions‚Äù\n\n\nairflow, temperature, same person, same coin?\n\n\n\n\nüòï The relative frequency is reliable under identical conditions?\n\nskilled person\n\n\n\n\nüëâ We only obtain an approximation instead of exact value.\n\n\n\n\nüòÇ How do you compute the probability that Chicago Cubs wins the World Series next year?\n\n\n\n\n\n\n\n\n\n\nMost importantly, some process or experiment cannot be replicated or repeated."
  },
  {
    "objectID": "slides/06-probability-slides.html#interpretation-of-probability-classical-approach",
    "href": "slides/06-probability-slides.html#interpretation-of-probability-classical-approach",
    "title": "Probability Fundamentals üé≤",
    "section": "Interpretation of Probability: Classical Approach\n",
    "text": "Interpretation of Probability: Classical Approach\n\n\n Classical probability : The probability is based on the concept of equally likely outcomes.\nIf the outcome of some process must be one of \\(n\\) different outcomes, the probability of each outcome is \\(1/n\\).\n\n\n\nExample:\n\ntoss a fair coin (2 outcomes) ü™ô\nroll a well-balanced die (6 outcomes) üé≤\ndraw one from a deck of cards (52 outcomes) üÉè\n\n\n\n\n\n\nAny issue of classical probability?\n\n\n\n\n\nThe probability that [you name it] wins the World Series next year is 1/30?!\n\n\n\n\n\n\n\n\n\n\nNo systematic method is given for assigning probabilities to outcomes that are not assumed to be equally likely.\nThis interpretation or definition of probability is way too naive right!\nOnly get pregnant or not get pregnant, but We cannot say the chance of getting pregnant is 1/2.\nOnly get COVID or not get COVID, but We cannot say the chance of getting COVID is 1/2."
  },
  {
    "objectID": "slides/06-probability-slides.html#interpretation-of-probability-subjective-approach",
    "href": "slides/06-probability-slides.html#interpretation-of-probability-subjective-approach",
    "title": "Probability Fundamentals üé≤",
    "section": "Interpretation of Probability: Subjective Approach\n",
    "text": "Interpretation of Probability: Subjective Approach\n\n\n Subjective probability : The probability is assigned or estimated using people‚Äôs knowledge, beliefs and information about the data generating process.\nA person‚Äôs subjective probability of an outcome, rather than the true probability of that outcome.\n\n\n\nI think ‚Äúthe probability that Milwaukee Brewers wins the World Series this year is 30%‚Äù.\n\n\n\n\nMy probability that Milwaukee Brewers wins the World Series next year is different from an ESPN MLB analyst‚Äôs probability.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubjective probability: The probability is assigned or estimated using people‚Äôs knowledge, beliefs and information about the data generating process, or hoe the outcome is obtained from some experiment.\nI think ‚Äúthe probability that Milwaukee Brewers wins the World Series this year is 30%‚Äù.\nSince the probability is subjective, my probability that Milwaukee Brewers wins the World Series next year is different from an ESPN MLB analyst‚Äôs probability."
  },
  {
    "objectID": "slides/06-probability-slides.html#section-2",
    "href": "slides/06-probability-slides.html#section-2",
    "title": "Probability Fundamentals üé≤",
    "section": "",
    "text": "Any probability operations and rules do NOT depend on interpretation of probability!\n\n\nno worries about which interpretation we gonna use before we do probability calculations. OK."
  },
  {
    "objectID": "slides/06-probability-slides.html#experiments-events-and-sample-space",
    "href": "slides/06-probability-slides.html#experiments-events-and-sample-space",
    "title": "Probability Fundamentals üé≤",
    "section": "Experiments, Events and Sample Space",
    "text": "Experiments, Events and Sample Space\n\nExperiment: any process in which the possible outcomes can be identified ahead of time.\nEvent: a set of possible outcomes of the experiment.\nSample space \\((\\mathcal{S})\\) of an experiment: the collection of ALL possible outcomes of the experiment.\n\n\n\n\n\n\n\n\n\nExperiment\nPossible Outcomes\nSome Events\nSample Space\n\n\n\nFlip a coin ü™ô\nHeads, Tails\n{Heads}, {Heads, Tails}, ‚Ä¶\n{Heads, Tails}\n\n\nRoll a die üé≤\n1, 2, 3, 4, 5, 6\n{1, 3, 5}, {2, 4, 6}, {2}, {3, 4, 5, 6}, ‚Ä¶\n{1, 2, 3, 4, 5, 6}\n\n\n\n\nFlipping a coin is an experiment because we can identify possible outcomes before we do the experiment, which are heads and tails.\nRolling a die is an experiment because we can identify possible outcomes before we do the experiment, which are numbers 1, 2, 3, 4, 5, 6.\nWe use curly braces to indicate a set. Any elements in the set are inside the curly braces.\n\n\nIs the sample space also an event?\n\n\n\n\nYes, the sample space itself is an event because it is also a set of possible outcomes of the experiment."
  },
  {
    "objectID": "slides/06-probability-slides.html#set-concept-example-of-rolling-a-six-side-balanced-die",
    "href": "slides/06-probability-slides.html#set-concept-example-of-rolling-a-six-side-balanced-die",
    "title": "Probability Fundamentals üé≤",
    "section": "Set Concept: Example of Rolling a six-side balanced die\n",
    "text": "Set Concept: Example of Rolling a six-side balanced die\n\n\nDraw a Venn Diagram every time you get stuck!\n\n\n\n Complement  of an event (set) \\(A\\),  \\(A^c\\) : a set of all outcomes (elements) of \\(\\mathcal{S}\\) in which \\(A\\) does not occur.\n\nLet \\(A\\) be an event that a number greater than 2. Then \\(A = \\{3, 4, 5, 6\\}\\) and \\(A^c = \\{1, 2\\}\\).\n\n\n\n\nBefore formally introduce probability operations, we need some basic set concepts because probability is defined on a set, or event.\nVenn diagram is a very useful tool for identifying a set, so I encourage you to draw a venn diagram when you get stuck on complicated set operations.\nThis applet is very helpful. Try to check each set and see if you fully understand concept of union, intersection, complement and containment or subset.\nAn Event is usually described by words, and we need to convert the words into the set it is referring to.\n\n\n\n Union \\((A \\cup B)\\) : a set of all outcomes of \\(\\mathcal{S}\\) in \\(A\\) or \\(B\\).\n\n\nLet \\(B\\) be an event that an even number is obtained. (What is \\(B\\) in terms of a set?)\n \\(B = \\{2, 4, 6\\}\\), \\(A \\cup B = \\{2, 3, 4, 5, 6\\}\\).\n\n\n\n\nCollect all elements in A and in B.\nKeep unique ones.\nAn element in \\((A \\cup B)\\) either belongs to event A only, B only or belong to A and B.\n\n\n\n\n\n Intersection \\((A \\cap B)\\) : a set of all outcomes of \\(\\mathcal{S}\\) in both \\(A\\) and \\(B\\).\n\n \\(A \\cap B = \\{4, 6\\}\\).\n\n\n\n\nthe elements in the set \\((A \\cap B)\\) should occur in both A and B."
  },
  {
    "objectID": "slides/06-probability-slides.html#set-concept-example-of-rolling-a-six-side-balanced-die-1",
    "href": "slides/06-probability-slides.html#set-concept-example-of-rolling-a-six-side-balanced-die-1",
    "title": "Probability Fundamentals üé≤",
    "section": "Set Concept: Example of Rolling a six-side balanced die\n",
    "text": "Set Concept: Example of Rolling a six-side balanced die\n\n\n\n\\(A\\) and \\(B\\) are disjoint (or mutually exclusive) if they have no outcomes in common \\((A \\cap B = \\emptyset)\\).\n\n\n\\(\\emptyset\\) means an empty set, \\(\\{\\}\\), i.e., no elements in the set.\n Let \\(C\\) be an event that an odd number is obtained. Then \\(C = \\{1, 3, 5\\}\\) and \\(B \\cap C = \\emptyset\\). \n\n\n\n\nAll elements in A are also elements in B. But there may be some B‚Äôs elements that are not in A.\nSet D has more elements than B, and B‚Äôs elements also belong to D.\n\n\n\n\n\nContainment \\((A \\subset B)\\): every elements of \\(A\\) also belongs to \\(B\\). If \\(A\\) occurs then so does \\(B\\).\n\n \\(B\\) is an event that an even number is obtained. \n \\(D\\) is an event that a number greater than 1 is obtained. \n \\(B = \\{2, 4, 6\\}\\) and \\(D = \\{2, 3, 4, 5, 6\\}\\). \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B \\subset D\\) or \\(D \\subset B\\)?"
  },
  {
    "objectID": "slides/06-probability-slides.html#probability-rules",
    "href": "slides/06-probability-slides.html#probability-rules",
    "title": "Probability Fundamentals üé≤",
    "section": "Probability Rules",
    "text": "Probability Rules\nDenote the probability of an event \\(A\\) on a sample space \\(\\mathcal{S}\\) as \\(P(A)\\).\n\nTreat the probability of an event as the area of the event in the Venn diagram.\n\n\n\n\nAxioms\n\n\\(P(\\mathcal{S}) = 1\\)\nFor any event \\(A\\), \\(P(A) \\ge 0\\)\n\nIf \\(A\\) and \\(B\\) are disjoint, \\(P(A \\cup B) = P(A) + P(B)\\)\n\n\n\n\n\n\n\n\n\nProperties\n\n\n\\(P(\\emptyset) = 0\\).\n\\(0 \\le P(A) \\le 1\\)\n\\(P(A^c) = 1 - P(A)\\)\n\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) (Addition Rule)\nIf \\(A \\subset B\\), then \\(P(A) \\le P(B)\\)\n\n\n\n\n\n\nNow we are ready to formally define a probability and it‚Äôs rules.\nTreat probability as the area in the Venn diagram. For example, \\(P(\\mathcal{S})\\) is the area of the sample space, or the area of the rectangle, which is normalized to be 1."
  },
  {
    "objectID": "slides/06-probability-slides.html#venn-diagram-illustration",
    "href": "slides/06-probability-slides.html#venn-diagram-illustration",
    "title": "Probability Fundamentals üé≤",
    "section": "Venn Diagram Illustration",
    "text": "Venn Diagram Illustration\n\n\nAddition Rule: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisjoint case: \\(P(A \\cup B) = P(A) + P(B)\\) because \\(P(A \\cap B) = 0\\)!"
  },
  {
    "objectID": "slides/06-probability-slides.html#example-mm-colors",
    "href": "slides/06-probability-slides.html#example-mm-colors",
    "title": "Probability Fundamentals üé≤",
    "section": "Example: M&M Colors",
    "text": "Example: M&M Colors\nThe makers of the candy M&Ms report that their plain M&Ms are composed of\n\n15% Yellow; 10% Red; 20% Orange; 25% Blue; 15% Green; 15% Brown\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you randomly select an M&M, what is the probability of the following?\n\n\nIt is brown.\n\n\nIt is red or green.\n\n\nIt is not blue.\n\n\nIt is red and brown.\n\n\n\n\n\n  \n    ‚àí\n    +\n \n 02:00\n \n\nGive you 2 minutes!"
  },
  {
    "objectID": "slides/06-probability-slides.html#example-mm-colors-1",
    "href": "slides/06-probability-slides.html#example-mm-colors-1",
    "title": "Probability Fundamentals üé≤",
    "section": "Example: M&M Colors",
    "text": "Example: M&M Colors\n\n15% Yellow; 10% Red; 20% Orange; 25% Blue; 15% Green; 15% Brown\n\n\n\n\nIf you randomly select an M&M, what is the probability of the following?\n\n\nIt is brown.\n\n\nIt is red or green.\n\n\nIt is not blue.\n\n\nIt is red and brown.\n\n\n\n\n\n\\(P(\\mathrm{Brown}) = 0.15\\)\n\\(\\small \\begin{align} P(\\mathrm{Red} \\cup \\mathrm{Green}) &= P(\\mathrm{Red}) + P(\\mathrm{Green}) - P(\\mathrm{Red} \\cap \\mathrm{Green}) \\\\ &= 0.10 + 0.15 - 0 = 0.25 \\end{align}\\)\n\\(P(\\text{Not Blue}) = 1 - P(\\text{Blue}) = 1 - 0.25 = 0.75\\)\n\\(P(\\text{Red and Brown}) = P(\\emptyset) = 0\\)\n\n\n\n\nBy the way, which interpretation of probability is used in this question?"
  },
  {
    "objectID": "slides/06-probability-slides.html#conditional-probability-1",
    "href": "slides/06-probability-slides.html#conditional-probability-1",
    "title": "Probability Fundamentals üé≤",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nThe conditional probability of \\(A\\) given \\(B\\) is\n\n\\[ P(A \\mid  B) = \\frac{P(A \\cap B)}{P(B)} \\] if \\(P(B) &gt; 0\\), and it is undefined if \\(P(B) = 0\\). \n\n‚ÄúGiven \\(B\\)‚Äù means that event \\(B\\) has already occurred.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiplication Rule: \\(P(A \\cap B) = P(A \\mid  B)P(B) = P(B \\mid  A)P(A)\\)\n\\(P(A)\\) and \\(P(B)\\) are unconditional or marginal probabilities.\n\n\nRead \\(A | B\\) as ‚Äú\\(A\\) given \\(B\\)‚Äù\nWhen we compute the probability of A, the information about B have been given, and the probability of A is adjusted, according to this information.\nThe probability of A may depend on whether B happens or not.\n\n\\(P(A | B)\\) computes the probability of A given that B has been occurred. At this moment \\(P(B)\\) is 1.\n\n\\(P(B)\\) is scaled up by \\(1 / P(B)\\) so that \\(P(B) \\times \\frac{1}{P(B)} = 1\\).\n\n\\(P(A) \\ne P(A|B)\\) in general.\nThe conditional probability is the ratio of \\(P(A \\cap B)\\) and \\(P(B)\\).\nThe proportion of area of A to the area of B.\nThe probability is the area of \\(A \\cap B\\) / area of B"
  },
  {
    "objectID": "slides/06-probability-slides.html#difference-between-pa-and-pa-mid-b",
    "href": "slides/06-probability-slides.html#difference-between-pa-and-pa-mid-b",
    "title": "Probability Fundamentals üé≤",
    "section": "Difference Between \\(P(A)\\) and \\(P(A \\mid B)\\)\n",
    "text": "Difference Between \\(P(A)\\) and \\(P(A \\mid B)\\)\n\n\n\n\n\n\n\n\n\n\n\n\nIf we don‚Äôt have any specific information, what we can base on is the entire sample space, and the probability of A is the ratio of area of A to the area of entire sample space which is 1.\nNow if we know B has occurred, we don‚Äôt need to consider the entire sample space any more.\nInstead, we focus only on B cuz we know B has occurred, \\(B^c\\) becomes totally irrelevant.\nTo find \\(P(A \\mid  B)\\), we just need to find how large part of B that also belongs to A.\nExample, when we don‚Äôt have any info about women, to compute a woman &gt; 20 yrs old, we base on the entire female population of interest. But if we do know that the woman has a BA degree, we shrink our focus on women who have a BA degree, and compute the proportion of the women pool that has age over 20."
  },
  {
    "objectID": "slides/06-probability-slides.html#example-peanut-butter-and-jelly",
    "href": "slides/06-probability-slides.html#example-peanut-butter-and-jelly",
    "title": "Probability Fundamentals üé≤",
    "section": "Example: Peanut Butter and Jelly",
    "text": "Example: Peanut Butter and Jelly\n\nSuppose 80% of people like peanut butter, 89% like jelly, and 78% like both.\nGiven that a randomly sampled person likes peanut butter, what‚Äôs the probability that she also likes jelly?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe want \\(P(J\\mid PB) = \\frac{P(PB \\cap J)}{P(PB)}\\).\nFrom the problem we have \\(P(PB) = 0.8\\), \\(P(J) = 0.89\\), \\(P(PB \\cap J) = 0.78\\)\n\\(P(J\\mid PB) = \\frac{P(PB \\cap J)}{P(PB)} = \\frac{0.78}{0.8} = 0.975\\).\n\n\n\n\nIf we don‚Äôt know if the person loves peanut butter, the probability that she loves jelly is 89%.\nIf we do know she loves peanut butter, the probability that she loves jelly is going up to 97.5%."
  },
  {
    "objectID": "slides/06-probability-slides.html#independence-1",
    "href": "slides/06-probability-slides.html#independence-1",
    "title": "Probability Fundamentals üé≤",
    "section": "Independence",
    "text": "Independence\n\n\\(A\\) and \\(B\\) are independent if \\(\\begin{align} P(A \\mid  B) &= P(A) \\text{ or }\\\\ P(B \\mid  A) &= P(B) \\text{ or } \\\\P(A\\cap B) &= P(A)P(B)\\end{align}\\) \\(\\text{ if } P(A) &gt; 0 \\text{ and } P(B) &gt; 0\\)\nIntuition: Knowing \\(B\\) occurs does not change the probability that \\(A\\) occurs, and vice versa.\n\n\nThe information about B is irrelevant to probability of A.\nExample:\n\n\nCan we compute \\(P(A \\cap B)\\) if we only know \\(P(A)\\) and \\(P(B)\\)?\n\n\n\n\nNo, we cannot compute \\(P(A \\cap B)\\) since we do not know if \\(A\\) and \\(B\\) are independent.\nWe could only if \\(A\\) and \\(B\\) were independent.\nIn general, we need the multiplication rule \\(P(A \\cap B) = P(A \\mid B)P(B)\\)."
  },
  {
    "objectID": "slides/06-probability-slides.html#venn-diagram-explanation-of-independence",
    "href": "slides/06-probability-slides.html#venn-diagram-explanation-of-independence",
    "title": "Probability Fundamentals üé≤",
    "section": "Venn Diagram Explanation of Independence",
    "text": "Venn Diagram Explanation of Independence\n\n\n\n\n\n\n\n\n\n\n\nIndependence means that the ratio of area of A to area of S is the same as the ratio of area of A&B to area of B.\nLook at the case of non-independence.\nLook at the circles. The area of A&B is very close to the area of B.\nIt means that the ratio of area of \\(A\\cap B\\) to area of B is pretty close to 1.\nSo given B, [ ]\nIn this case, the information about B does matter, and affect probability of A.\nIf we know B occurred, there is pretty high chance that A will occur as well. OK."
  },
  {
    "objectID": "slides/06-probability-slides.html#independence-example",
    "href": "slides/06-probability-slides.html#independence-example",
    "title": "Probability Fundamentals üé≤",
    "section": "Independence Example",
    "text": "Independence Example\n\n\nAssuming that events \\(A\\) and \\(B\\) are independent. \\(P(A) = 0.3\\) and \\(P(B) = 0.7\\).\n\n\n\\(P(A \\cap B)\\)?\n\n\\(P(A \\cup B)\\)?\n\n\\(P(A \\mid B)\\)?\n\n\n\n\n  \n    ‚àí\n    +\n \n 02:00\n \n\n\n\n\\(P(A \\cap B) = P(A)P(B)=0.21\\)\n\n\n\n\n\\(P(A \\cup B) = P(A)+P(B)-P(A\\cap B) = 0.3+0.7-0.21=0.79\\)\n\n\n\n\n\\(P(A \\mid B) = P(A) = 0.3\\)"
  },
  {
    "objectID": "slides/06-probability-slides.html#why-bayes-formula",
    "href": "slides/06-probability-slides.html#why-bayes-formula",
    "title": "Probability Fundamentals üé≤",
    "section": "Why Bayes‚Äô Formula?",
    "text": "Why Bayes‚Äô Formula?\n\nOften, we know \\(P(B \\mid A)\\) but are much more interested in \\(P(A \\mid B)\\).\nExample: diagnostic tests provide \\(P(\\text{positive test result}  \\mid \\text{COVID})\\), but we are interested in \\(P(\\text{COVID} \\mid \\text{positive test result})\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes‚Äô formula provides a way for finding \\(P(A \\mid B)\\) from \\(P(B \\mid A)\\)\n\n\n\nOften, we know the conditional probability \\(P(B \\mid A)\\) but are much more interested in \\(P(A \\mid B)\\)\n\nFor example, diagnostic tests provide \\(P(\\text{positive test result}  \\mid \\text{disease})\\), but we are interested in \\(P(\\text{disease} \\mid \\text{positive test result})\\)"
  },
  {
    "objectID": "slides/06-probability-slides.html#bayes-formula-2",
    "href": "slides/06-probability-slides.html#bayes-formula-2",
    "title": "Probability Fundamentals üé≤",
    "section": "Bayes‚Äô Formula",
    "text": "Bayes‚Äô Formula\n\nIf \\(A\\) and \\(B\\) are any events whose probabilities are not 0 or 1, then\n\n\n\\[\\begin{align*} P(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)} \\quad ( \\text{def. of cond. prob.}) \\\\ &= \\frac{P(A \\cap B)}{P((B \\cap A) \\cup (B \\cap A^c))} \\quad ( \\text{partition } B) \\\\ &= \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)}  \\quad ( \\text{multiplication rule}) \\end{align*}\\]\n\n\n\n\n\n\n\n\n\n\nHint:  Partition the event \\(B\\) as a union of disjoint sets: \\(B = (B \\cap A) \\cup (B \\cap A^c)\\) (Check Venn diagram) and apply the multiplication rule.\n\nStart from the definition of conditional probability, we have \\[ P(A \\mid  B) = \\frac{P(A \\cap B)}{P(B)} = \\cdots = \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)}\\]"
  },
  {
    "objectID": "slides/06-probability-slides.html#example-passing-rate",
    "href": "slides/06-probability-slides.html#example-passing-rate",
    "title": "Probability Fundamentals üé≤",
    "section": "Example: Passing Rate",
    "text": "Example: Passing Rate\n\n\nAfter taking MATH 4720, \\(80\\%\\) of students understand the Bayes‚Äô formula.\n\nOf those who understand the Bayes‚Äô formula,\n\n\n\\(95\\%\\) passed\n\n\nOf those who do not understand the Bayes‚Äô formula,\n\n\n\\(60\\%\\) passed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the probability that a student understand the Bayes‚Äô formula given the fact that she passed."
  },
  {
    "objectID": "slides/06-probability-slides.html#bayes-formula-step-by-step",
    "href": "slides/06-probability-slides.html#bayes-formula-step-by-step",
    "title": "Probability Fundamentals üé≤",
    "section": "Bayes Formula: Step-by-Step",
    "text": "Bayes Formula: Step-by-Step\n\n\\(80\\%\\) of students understand the Bayes‚Äô formula.\nOf those who understand the Bayes‚Äô formula, \\(95\\%\\) passed ( \\(5\\%\\) failed).\nOf those who do not understand the formula, \\(60\\%\\) passed ( \\(40\\%\\) failed).\n\n\n\\[P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)}\\]\n\n\n\n Step 1: Formulate what we would like to compute \n\n\n\n\\(P(\\text{understand} \\mid \\text{passed})\\)\n\n\n\n Step 2: Define relevant events in the formula: \\(A\\), \\(A^c\\) and \\(B\\) \n\n\n\nLet \\(A =\\) understand. \\(B =\\) passed. Then \\(A^c =\\) don‚Äôt understand and \\(P(\\text{understand} \\mid \\text{passed}) = P(A \\mid B)\\)."
  },
  {
    "objectID": "slides/06-probability-slides.html#bayes-formula-step-by-step-1",
    "href": "slides/06-probability-slides.html#bayes-formula-step-by-step-1",
    "title": "Probability Fundamentals üé≤",
    "section": "Bayes Formula: Step-by-Step",
    "text": "Bayes Formula: Step-by-Step\n\n\\(80\\%\\) of students understand the Bayes‚Äô formula.\nOf those who understand the Bayes‚Äô formula, \\(95\\%\\) passed ( \\(5\\%\\) failed).\nOf those who do not understand the formula, \\(60\\%\\) passed ( \\(40\\%\\) failed).\n\n\n\\[P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)}\\]\n\n\n Step 3: Find probabilities in the Bayes‚Äô formula using provided information. \n\n\\(P(B \\mid A) = P(\\text{passed} \\mid \\text{understand}) = 0.95\\), \\(P(B \\mid A^c) = P(\\text{passed} \\mid \\text{don't understand}) = 0.6\\)\\(P(A) = P(\\text{understand}) = 0.8\\), \\(P(A^c) = 1 - P(A) = 0.2\\).\n\n\n Step 4: Apply Bayes‚Äô formula. \n\n\\(P(\\text{understand} \\mid \\text{passed}) = P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)} = \\frac{(0.95)(0.8)}{(0.95)(0.8) + (0.6)(0.2)} = 0.86\\)"
  },
  {
    "objectID": "slides/06-probability-slides.html#bayes-formula-tree-diagram-illustration",
    "href": "slides/06-probability-slides.html#bayes-formula-tree-diagram-illustration",
    "title": "Probability Fundamentals üé≤",
    "section": "Bayes Formula: Tree Diagram Illustration",
    "text": "Bayes Formula: Tree Diagram Illustration\n\n\\(80\\%\\) of students understand the Bayes‚Äô formula.\nOf those who understand the Bayes‚Äô formula, \\(95\\%\\) passed ( \\(5\\%\\) failed).\nOf those who do not understand the formula, \\(60\\%\\) passed ( \\(40\\%\\) failed).\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*} & P(\\text{yes} \\mid \\text{pass}) \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass})} \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass and yes}) + P(\\text{pass and no})}\\\\ &= \\frac{0.76}{0.76 + 0.12} = 0.86 \\end{align*}\\]\n\n\\[\\begin{align*} & P(\\text{yes} \\mid \\text{pass}) \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass})} \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass and yes}) + P(\\text{pass and no})}\\\\ &= \\frac{P(\\text{pass | yes})P(\\text{yes})}{P(\\text{pass | yes})P(\\text{yes}) + P(\\text{pass | no})P(\\text{no})} \\\\ &= \\frac{0.76}{0.76 + 0.12} = 0.86 \\end{align*}\\]"
  },
  {
    "objectID": "slides/03-r.html#r-and-rstudio",
    "href": "slides/03-r.html#r-and-rstudio",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "R and RStudio",
    "text": "R and RStudio\n\n\n\n\n\n\n\n\n\n\n\n\nR: free open-source programming language\n\nR is mainly for doing data science with strength in statistical modeling, computing and data visualization \n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio 1: interface for R, Python, etc called an IDE (integrated development environment), e.g.¬†‚ÄúI write R code in the RStudio IDE‚Äù.\nRStudio is not a requirement for programming with R, but it‚Äôs commonly used by R developers, statisticians and data scientists.\n\n\n\n\n\nRStudio is a product of Posit data science company.\n\n\nThe first tool we need is R and RStudio. They are two different things.\nWith packages, we can do lots of things without writing our own code, and we can just use the functions provided in packages to do our job. It saves us lots of time.\nA language with many great add-on packages can become a very useful language.\npackages: which is a collection of functions and data for people to use.\nUnlike other languages, R was created by statisticians, for statisticians to use.\nSo R is not a general-purpose programming language like python or java, but it is specifically for data science. And that‚Äôs probably why you don‚Äôt learn R in an intro programming course.\nSo you can view Python as a smart phone. It can do many different things, phone, camera, email, social media. But R is like a high quality camera. It can‚Äôt receive emails, but it produces high quality photos.\nRStudio is not a programming language, but a software application.\nCode itself is just a text file. You can use any text editor or software to write your code."
  },
  {
    "objectID": "slides/03-r.html#r-and-rstudio-interface",
    "href": "slides/03-r.html#r-and-rstudio-interface",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "R and RStudio Interface",
    "text": "R and RStudio Interface\n\nRStudio IDE includes\n\na viewable environment, a file browser, data viewer, and a plotting pane. üëç\nalso features integrated help, syntax highlighting, context-aware tab completion and more! üòÑ\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nRStudio"
  },
  {
    "objectID": "slides/03-r.html#r-packages",
    "href": "slides/03-r.html#r-packages",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "R Packages üì¶",
    "text": "R Packages üì¶\n\nPackages wrap up reusable R functions, the documentation that describes how to use them, and data sets all together.1\nAs of August 2025, there are about 22510 R packages available on CRAN (the Comprehensive R Archive Network)!2\nLet‚Äôs work with an important subset of these!"
  },
  {
    "objectID": "slides/03-r.html#posit-cloud---statistics-wo-hardware-hassles",
    "href": "slides/03-r.html#posit-cloud---statistics-wo-hardware-hassles",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "‚òÅÔ∏è Posit Cloud - Statistics w/o hardware hassles",
    "text": "‚òÅÔ∏è Posit Cloud - Statistics w/o hardware hassles\n\nüòé We can implement R programs without installing R and RStudio in your laptop!\nüòé Posit Cloud lets you do, share and learn data science online for free!\n\n\n\nüòû R/RStudio: Lots of friction\n\nDownload and install R\nDownload and install RStudio\nInstall wanted R packages:\n\nrmarkdown\ntidyverse\n‚Ä¶\n\n\nLoad these packages\nDownload and install tools like Git\n\n\nü§ì Posit Cloud: Much less friction\n\n\n\n\n\n\n\n\n\nGo to https://posit.cloud/\n\nLog in\n\n&gt;hello R!\n\n\n\n\n\nWhy I choose use RStudio in the cloud for you to code in R? Well if I want every one of you use R and RStudio locally in your computer, you have to (_____). And we need some magic to make sure everyone gets the coding environment ready.\nInstead, with the Cloud-based solution, you just need to login, then you start writing code right away.\nPosit Cloud provides you with the latest version of R and RStudio. No installation is required.\nDoesn‚Äôt mean you should always use Posit Cloud. It‚Äôs not free if you need more resources.\nAfter you write more and more R code, you should absolutely install R and Rstudio into your local machine."
  },
  {
    "objectID": "slides/03-r.html#sign-up-posit-cloud",
    "href": "slides/03-r.html#sign-up-posit-cloud",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Sign Up Posit Cloud",
    "text": "Sign Up Posit Cloud\n\n\n\nStep 1: In the Posit website https://posit.co/, choose Products &gt; Posit Cloud as shown below."
  },
  {
    "objectID": "slides/03-r.html#sign-up-posit-cloud-1",
    "href": "slides/03-r.html#sign-up-posit-cloud-1",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Sign Up Posit Cloud",
    "text": "Sign Up Posit Cloud\n\n\nStep 2: Click GET STARTED.\nStep 3: Free &gt; Sign Up. Please sign up using your Marquette email address or the one you prefer."
  },
  {
    "objectID": "slides/03-r.html#new-projects",
    "href": "slides/03-r.html#new-projects",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "New Projects",
    "text": "New Projects\nIn Posit Cloud, click New Project &gt; New RStudio Project, then you are all set!\n\n\n\n\n\n\n\n\n\n\n\nNew RStudio Project"
  },
  {
    "objectID": "slides/03-r.html#first-r-code-in-posit-cloud",
    "href": "slides/03-r.html#first-r-code-in-posit-cloud",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "First R Code in Posit Cloud!",
    "text": "First R Code in Posit Cloud!\n\nGive your project a nice name (click Untitled Project), math-4720 for example.\nFirst R code: \"Hello WoRld!\" or 2 + 4 after &gt; in the Console pane.\nChange the editor theme: Tools &gt; Global Options &gt; Appearance"
  },
  {
    "objectID": "slides/03-r.html#rstudio-panes",
    "href": "slides/03-r.html#rstudio-panes",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "RStudio Panes",
    "text": "RStudio Panes\n\n\n\n\n\n\n\n\n\n\nIn RStudio, there are 4 main panes, source pane, console pane, pane for environment/history and version control, and the pane for files plots packages and help page.\nSource pane is where you write your code. Your code will not be evaluated or interpreted until you ‚Äúrun‚Äù them or source them to the console.\nTry to write your code in R scripts in the Source, so that the code can be saved and reused later.\nYou type code into the Console if the code is short or you want to do some quick calculations or analysis. The code you type in the Console will not be saved in a script.\nIn the environment/history, you can check any objects you create in the R environment and you can also view your command history in the history tab.\nAnd you will see how the pane for file/plot/package/help can be used as we learn more about RStudio."
  },
  {
    "objectID": "slides/03-r.html#r-script",
    "href": "slides/03-r.html#r-script",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "R Script",
    "text": "R Script\n\nA R script is a .R file that contains R code.\nTo create a R script, go to File &gt; New &gt; R Script, or click the green-plus icon on the topleft corner, and select R Script."
  },
  {
    "objectID": "slides/03-r.html#run-code",
    "href": "slides/03-r.html#run-code",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Run Code",
    "text": "Run Code\n\n Run : run the current line or selection of code.\n Icon right to the Run : re-run the previous selected code.\n Source : run all the code in the R script.\n\n\n\n\n\n\n\n\n\n\n\n\n Source : run all the code in the R script with NO output\n\n Source with Echo : run all the code in the R script and show output\n\nDepending on your purpose, you can run code line by line or run the entire code.\nTo run the R code line by line, Click Run icon to run the current line or selection of code. Or use key-binding ctrl + enter (windows) or cmd + enter (mac)"
  },
  {
    "objectID": "slides/03-r.html#environment-tab",
    "href": "slides/03-r.html#environment-tab",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Environment Tab",
    "text": "Environment Tab\n\nThe (global) environment is where we are currently working.\nAnything created or imported into the current R session is stored in our environment and shown in the Environment tab.\nAfter we run the R script, objects stored in the environment are\n\nData set mtcars\n\nObject x storing integer values 1 to 10.\nObject y storing three numeric values 3, 5, 9."
  },
  {
    "objectID": "slides/03-r.html#help",
    "href": "slides/03-r.html#help",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Help",
    "text": "Help\n\nDon‚Äôt know how a function works or what a data set is about ‚ùì\nüëâ Simply type ? followed by the data name or function name like\n\n\nA document will show up in the Help tab, teaching you how to use the function or explaining the data set.\n\n. . .\n\nWhat does the function mean() do?"
  },
  {
    "objectID": "slides/03-r.html#section-1",
    "href": "slides/03-r.html#section-1",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "",
    "text": "What is the size of mtcars data?\nType mtcars and hit Enter in the Console to see the data set.\nDiscuss data type of each variable.\nType mtcars[, 1] and hit Enter in the Console. What do you see?"
  },
  {
    "objectID": "slides/03-r.html#install-r-step-1",
    "href": "slides/03-r.html#install-r-step-1",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Install R ‚Äì Step 1",
    "text": "Install R ‚Äì Step 1\n\nGo to https://cloud.r-project.org\n\nClick Download R for [your operating system]"
  },
  {
    "objectID": "slides/03-r.html#install-r-step-2",
    "href": "slides/03-r.html#install-r-step-2",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Install R ‚Äì Step 2",
    "text": "Install R ‚Äì Step 2\n\nIf you are a Mac user, you should see the page as below. You are recommended to download and install the latest version of R (now R-4.5.1 (Great Square Root)), if your OS version allows to do so. Otherwise, choose a previous version, R-3.6.3.\nIf you are a Windows user, after clicking Download R for Windows, please choose base version, then click Download R-4.5.1 for Windows."
  },
  {
    "objectID": "slides/03-r.html#install-r-step-3",
    "href": "slides/03-r.html#install-r-step-3",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Install R ‚Äì Step 3",
    "text": "Install R ‚Äì Step 3\n\nOnce you install R successfully, when you open R, you should be able to see the following R terminal or console:\n\n\n\nWindows\n\n\nMac"
  },
  {
    "objectID": "slides/03-r.html#welcome-to-the-r-world",
    "href": "slides/03-r.html#welcome-to-the-r-world",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Welcome to the R World!",
    "text": "Welcome to the R World!\n\nNow you are ready to use R to do statistical computation.\nYou can use R like a calculator. After typing your formula, simply hit Enter, you get the answer! For example,\n\n\n1 + 2\n\n[1] 3\n\n30 * 42 / 3\n\n[1] 420\n\nlog(5) - exp(3) * sqrt(7)\n\n[1] -51.5319"
  },
  {
    "objectID": "slides/03-r.html#install-rstudio-step-1",
    "href": "slides/03-r.html#install-rstudio-step-1",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Install RStudio ‚Äì Step 1",
    "text": "Install RStudio ‚Äì Step 1\n\nIn the Posit website, on top go to OPEN SOURCE &gt; DOWNLOAD RSTUDIO -&gt;"
  },
  {
    "objectID": "slides/03-r.html#install-rstudio-step-2",
    "href": "slides/03-r.html#install-rstudio-step-2",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Install RStudio ‚Äì Step 2",
    "text": "Install RStudio ‚Äì Step 2\n\nClick DOWNLOAD RSTUDIO."
  },
  {
    "objectID": "slides/03-r.html#install-rstudio-step-3",
    "href": "slides/03-r.html#install-rstudio-step-3",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Install RStudio ‚Äì Step 3",
    "text": "Install RStudio ‚Äì Step 3\nThe page will automatically detect your operating system and recommend a version of RStudio that works the best for you that is usually the latest version.\n\nClick DOWNLOAD RSTUDIO DESKTOP FOR [Your OS version].\nFollow the standard installation steps and you should get the software.\nMake sure that R is installed successfully on your computer before you download and install RStudio."
  },
  {
    "objectID": "slides/03-r.html#rstudio-screen",
    "href": "slides/03-r.html#rstudio-screen",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "RStudio Screen",
    "text": "RStudio Screen\n\nWhen you open RStudio, you should see something similar to the figure below.\nIf you do, congratulations! You are able to do every statistical computation in R using RStudio locally in your computer."
  },
  {
    "objectID": "slides/03-r.html#footnotes",
    "href": "slides/03-r.html#footnotes",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Footnotes",
    "text": "Footnotes\n\nWickham and Bryan, R Packages.‚Ü©Ô∏é\nCRAN contributed packages.‚Ü©Ô∏é"
  },
  {
    "objectID": "slides/03-r-slides.html#r-and-rstudio",
    "href": "slides/03-r-slides.html#r-and-rstudio",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "R and RStudio",
    "text": "R and RStudio\n\n\n\n\n\n\n\n\n\n\n\n\nR: free open-source programming language\n\nR is mainly for doing data science with strength in statistical modeling, computing and data visualization \n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio 1: interface for R, Python, etc called an IDE (integrated development environment), e.g.¬†‚ÄúI write R code in the RStudio IDE‚Äù.\nRStudio is not a requirement for programming with R, but it‚Äôs commonly used by R developers, statisticians and data scientists.\n\n\n\n\nThe first tool we need is R and RStudio. They are two different things.\nWith packages, we can do lots of things without writing our own code, and we can just use the functions provided in packages to do our job. It saves us lots of time.\nA language with many great add-on packages can become a very useful language.\npackages: which is a collection of functions and data for people to use.\nUnlike other languages, R was created by statisticians, for statisticians to use.\nSo R is not a general-purpose programming language like python or java, but it is specifically for data science. And that‚Äôs probably why you don‚Äôt learn R in an intro programming course.\nSo you can view Python as a smart phone. It can do many different things, phone, camera, email, social media. But R is like a high quality camera. It can‚Äôt receive emails, but it produces high quality photos.\nRStudio is not a programming language, but a software application.\nCode itself is just a text file. You can use any text editor or software to write your code.\n\nRStudio is a product of Posit data science company."
  },
  {
    "objectID": "slides/03-r-slides.html#posit-cloud---statistics-wo-hardware-hassles",
    "href": "slides/03-r-slides.html#posit-cloud---statistics-wo-hardware-hassles",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "‚òÅÔ∏è Posit Cloud - Statistics w/o hardware hassles",
    "text": "‚òÅÔ∏è Posit Cloud - Statistics w/o hardware hassles\n\nüòé We can implement R programs without installing R and RStudio in your laptop!\nüòé Posit Cloud lets you do, share and learn data science online for free!\n\n\n\nüòû R/RStudio: Lots of friction\n\nDownload and install R\nDownload and install RStudio\nInstall wanted R packages:\n\nrmarkdown\ntidyverse\n‚Ä¶\n\n\nLoad these packages\nDownload and install tools like Git\n\n\nü§ì Posit Cloud: Much less friction\n\n\n\n\n\n\n\n\n\nGo to https://posit.cloud/\n\nLog in\n\n&gt;hello R!\n\n\nWhy I choose use RStudio in the cloud for you to code in R? Well if I want every one of you use R and RStudio locally in your computer, you have to (_____). And we need some magic to make sure everyone gets the coding environment ready.\nInstead, with the Cloud-based solution, you just need to login, then you start writing code right away.\nPosit Cloud provides you with the latest version of R and RStudio. No installation is required.\nDoesn‚Äôt mean you should always use Posit Cloud. It‚Äôs not free if you need more resources.\nAfter you write more and more R code, you should absolutely install R and Rstudio into your local machine."
  },
  {
    "objectID": "slides/03-r-slides.html#sign-up-posit-cloud",
    "href": "slides/03-r-slides.html#sign-up-posit-cloud",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Sign Up Posit Cloud",
    "text": "Sign Up Posit Cloud\n\n\n\nStep 1: In the Posit website https://posit.co/, choose Products &gt; Posit Cloud as shown below."
  },
  {
    "objectID": "slides/03-r-slides.html#sign-up-posit-cloud-1",
    "href": "slides/03-r-slides.html#sign-up-posit-cloud-1",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Sign Up Posit Cloud",
    "text": "Sign Up Posit Cloud\n\n\nStep 2: Click GET STARTED.\nStep 3: Free &gt; Sign Up. Please sign up using your Marquette email address or the one you prefer."
  },
  {
    "objectID": "slides/03-r-slides.html#new-projects",
    "href": "slides/03-r-slides.html#new-projects",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "New Projects",
    "text": "New Projects\nIn Posit Cloud, click New Project &gt; New RStudio Project, then you are all set!\n\n\n\n\n\n\n\n\n\n\n\nNew RStudio Project"
  },
  {
    "objectID": "slides/03-r-slides.html#first-r-code-in-posit-cloud",
    "href": "slides/03-r-slides.html#first-r-code-in-posit-cloud",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "First R Code in Posit Cloud!",
    "text": "First R Code in Posit Cloud!\n\nGive your project a nice name (click Untitled Project), math-4720 for example.\nFirst R code: \"Hello WoRld!\" or 2 + 4 after &gt; in the Console pane.\nChange the editor theme: Tools &gt; Global Options &gt; Appearance"
  },
  {
    "objectID": "slides/03-r-slides.html#rstudio-panes",
    "href": "slides/03-r-slides.html#rstudio-panes",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "RStudio Panes",
    "text": "RStudio Panes\n\n\nIn RStudio, there are 4 main panes, source pane, console pane, pane for environment/history and version control, and the pane for files plots packages and help page.\nSource pane is where you write your code. Your code will not be evaluated or interpreted until you ‚Äúrun‚Äù them or source them to the console.\nTry to write your code in R scripts in the Source, so that the code can be saved and reused later.\nYou type code into the Console if the code is short or you want to do some quick calculations or analysis. The code you type in the Console will not be saved in a script.\nIn the environment/history, you can check any objects you create in the R environment and you can also view your command history in the history tab.\nAnd you will see how the pane for file/plot/package/help can be used as we learn more about RStudio."
  },
  {
    "objectID": "slides/03-r-slides.html#r-script",
    "href": "slides/03-r-slides.html#r-script",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "R Script",
    "text": "R Script\n\nA R script is a .R file that contains R code.\nTo create a R script, go to File &gt; New &gt; R Script, or click the green-plus icon on the topleft corner, and select R Script."
  },
  {
    "objectID": "slides/03-r-slides.html#run-code",
    "href": "slides/03-r-slides.html#run-code",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Run Code",
    "text": "Run Code\n\n Run : run the current line or selection of code.\n Icon right to the Run : re-run the previous selected code.\n Source : run all the code in the R script.\n\n\n\n\n Source : run all the code in the R script with NO output\n\n Source with Echo : run all the code in the R script and show output\n\nDepending on your purpose, you can run code line by line or run the entire code.\nTo run the R code line by line, Click Run icon to run the current line or selection of code. Or use key-binding ctrl + enter (windows) or cmd + enter (mac)"
  },
  {
    "objectID": "slides/03-r-slides.html#environment-tab",
    "href": "slides/03-r-slides.html#environment-tab",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Environment Tab",
    "text": "Environment Tab\n\nThe (global) environment is where we are currently working.\nAnything created or imported into the current R session is stored in our environment and shown in the Environment tab.\nAfter we run the R script, objects stored in the environment are\n\nData set mtcars\n\nObject x storing integer values 1 to 10.\nObject y storing three numeric values 3, 5, 9."
  },
  {
    "objectID": "slides/03-r-slides.html#help",
    "href": "slides/03-r-slides.html#help",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Help",
    "text": "Help\n\nDon‚Äôt know how a function works or what a data set is about ‚ùì\nüëâ Simply type ? followed by the data name or function name like\n\n\nA document will show up in the Help tab, teaching you how to use the function or explaining the data set.\n\n\n\nWhat does the function mean() do?"
  },
  {
    "objectID": "slides/03-r-slides.html#section-1",
    "href": "slides/03-r-slides.html#section-1",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "",
    "text": "What is the size of mtcars data?\nType mtcars and hit Enter in the Console to see the data set.\nDiscuss data type of each variable.\nType mtcars[, 1] and hit Enter in the Console. What do you see?\n\n\n\n\n\n\n\n\n\n\n\n  \n    ‚àí\n    +\n \n 05:00"
  },
  {
    "objectID": "slides/03-r-slides.html#install-r-step-1",
    "href": "slides/03-r-slides.html#install-r-step-1",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Install R ‚Äì Step 1",
    "text": "Install R ‚Äì Step 1\n\nGo to https://cloud.r-project.org\n\nClick Download R for [your operating system]"
  },
  {
    "objectID": "slides/03-r-slides.html#install-r-step-2",
    "href": "slides/03-r-slides.html#install-r-step-2",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Install R ‚Äì Step 2",
    "text": "Install R ‚Äì Step 2\n\nIf you are a Mac user, you should see the page as below. You are recommended to download and install the latest version of R (now R-4.5.1 (Great Square Root)), if your OS version allows to do so. Otherwise, choose a previous version, R-3.6.3.\nIf you are a Windows user, after clicking Download R for Windows, please choose base version, then click Download R-4.5.1 for Windows."
  },
  {
    "objectID": "slides/03-r-slides.html#install-r-step-3",
    "href": "slides/03-r-slides.html#install-r-step-3",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Install R ‚Äì Step 3",
    "text": "Install R ‚Äì Step 3\n\nOnce you install R successfully, when you open R, you should be able to see the following R terminal or console:\n\n\n\nWindows\n\n\nMac"
  },
  {
    "objectID": "slides/03-r-slides.html#welcome-to-the-r-world",
    "href": "slides/03-r-slides.html#welcome-to-the-r-world",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Welcome to the R World!",
    "text": "Welcome to the R World!\n\nNow you are ready to use R to do statistical computation.\nYou can use R like a calculator. After typing your formula, simply hit Enter, you get the answer! For example,\n\n\n1 + 2\n\n[1] 3\n\n30 * 42 / 3\n\n[1] 420\n\nlog(5) - exp(3) * sqrt(7)\n\n[1] -51.5319"
  },
  {
    "objectID": "slides/03-r-slides.html#install-rstudio-step-1",
    "href": "slides/03-r-slides.html#install-rstudio-step-1",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Install RStudio ‚Äì Step 1",
    "text": "Install RStudio ‚Äì Step 1\n\nIn the Posit website, on top go to OPEN SOURCE &gt; DOWNLOAD RSTUDIO -&gt;"
  },
  {
    "objectID": "slides/03-r-slides.html#install-rstudio-step-2",
    "href": "slides/03-r-slides.html#install-rstudio-step-2",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Install RStudio ‚Äì Step 2",
    "text": "Install RStudio ‚Äì Step 2\n\nClick DOWNLOAD RSTUDIO."
  },
  {
    "objectID": "slides/03-r-slides.html#install-rstudio-step-3",
    "href": "slides/03-r-slides.html#install-rstudio-step-3",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "Install RStudio ‚Äì Step 3",
    "text": "Install RStudio ‚Äì Step 3\nThe page will automatically detect your operating system and recommend a version of RStudio that works the best for you that is usually the latest version.\n\nClick DOWNLOAD RSTUDIO DESKTOP FOR [Your OS version].\nFollow the standard installation steps and you should get the software.\nMake sure that R is installed successfully on your computer before you download and install RStudio."
  },
  {
    "objectID": "slides/03-r-slides.html#rstudio-screen",
    "href": "slides/03-r-slides.html#rstudio-screen",
    "title": "You RRR a Beginner: R, RStudio and Posit Cloud üë©‚Äçüíª",
    "section": "RStudio Screen",
    "text": "RStudio Screen\n\nWhen you open RStudio, you should see something similar to the figure below.\nIf you do, congratulations! You are able to do every statistical computation in R using RStudio locally in your computer."
  },
  {
    "objectID": "exam.html",
    "href": "exam.html",
    "title": "Take-home Exams",
    "section": "",
    "text": "More exams to be added as the semester progresses.\n\n\n\n\n\n\n\n\n\n\nNo.\n\n\n\nTitle\n\n\n\nDue date\n\n\n\n\n\n\n\n\nTake-home Exam 1\n\n\nTake-home Exam for Week 1 to 6\n\n\n\n\n\n\n\n\n\nTake-home Exam 2\n\n\nTake-home Exam for Week 7 to 11\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Take-home Exams"
    ]
  },
  {
    "objectID": "exercise/exercise1.html",
    "href": "exercise/exercise1.html",
    "title": "Exercise 1",
    "section": "",
    "text": "The General Social Survey asked the question, After an average work day, about how many hours do you have to relax or pursue activities that you enjoy?‚Äù to a random sample of 1,155 Americans. The average relaxing time was found to be 1.65 hours. Determine which of the following is an observation, a variable, a sample statistic (value calculated based on the observed sample), or a population parameter. \n\nAn American in the sample.\nNumber of hours spent relaxing after an average work day.\n1.65.\nAverage number of hours all Americans spend relaxing after an average work day. \n\n\n\n\n\n# (a) Observation.\n# (b) Variable.\n# (c) Sample statistic (mean).\n# (d) Population parameter (mean).\n\n\n\nIdentify which value represents the sample mean and which value represents the claimed population mean. \n\nAmerican households spent an average of about $52 in 2007 on Halloween merchandise such as costumes, decorations and candy. To see if this number had changed, researchers conducted a new survey in 2008 before industry numbers were reported. The survey included 1,500 households and found that average Halloween spending was $58 per household.\nThe average GPA of students in 2001 at a private university was 3.37. A survey on a sample of 203 students from this university yielded an average GPA of 3.59 a decade later. \n\n\n\n\n\n# (a) Population mean, 2007 = 52; sample mean, 2008 = 58.\n# (b) Population mean, 2001 = 3.37; sample mean, 2012 = 3.59.\n\n\n\nData collected at elementary schools in DeKalb County, GA suggest that each year roughly 25% of students miss exactly one day of school, 15% miss 2 days, and 28% miss 3 or more days due to sickness. \n\nWhat is the probability that a student chosen at random doesn‚Äôt miss any days of school due to sickness this year?\nWhat is the probability that a student chosen at random misses no more than one day?\nWhat is the probability that a student chosen at random misses at least one day?\nIf a parent has two kids at a DeKalb County elementary school, what is the probability that neither kid will miss any school? Note any assumption you must make to answer this question.\nIf a parent has two kids at a DeKalb County elementary school, what is the probability that both kids will miss some school, i.e.¬†at least one day? Note any assumption you make.\nIf you made an assumption in part (d) or (e), do you think it was reasonable? If you didn‚Äôt make any assumptions, double check your earlier answers. \n\n\n\n\n\n# (a) P(no misses) = 1 - (0.25 + 0.15 + 0.28) = 0.32\n# (b) P(at most 1 miss) = P(no misses) + P(1 miss) = 0.32 + 0.25 = 0.57\n# (c) P(at least 1 miss) = P(1 miss) + P(2 misses) + P(3+ misses) \n#                        = 1 - P(no misses)\n#                        = 1 - 0.32 = 0.68\n# (d) For parts (d) and (e) assume that whether or not one kid misses school is\n# independent of the other.\n# P(neither miss any) = P(no miss) * P(no miss) = 0.32^2 = 0.1024\n# (e) P(both miss some) = P(at least 1 miss) * P(at least 1 miss) = 0:682 = 0.4624\n# (f) These kids are siblings, and if one gets sick it probably raises the \n# chance that the other one will get sick as well. So whether or not one misses\n# school due to sickness is probably not independent of the other.\n\n\n\nIn the United States, approximately \\(9\\%\\) of the population have diabetes, while about \\(30\\%\\) of adults have high blood pressure. An estimated \\(6\\%\\) of the population have both diabetes (\\(D\\)) and hypertension (\\(H\\)). \n\nWhat is the probability that a randomly selected American adult has both two diseases (\\(D \\cap H\\)) doesn‚Äôt have any of the two diseases (\\(D^c \\cap H^c\\))? (Drawing a Venn diagram may help)\nIf a randomly selected American adult has diabetes, what‚Äôs the probability that he also has hypertension, i.e., \\(P(H \\mid D)\\)? Based on your result, is the event of someone being hypertensive independent of the event that someone has diabetes? How knowing someone having diabetes help us predict whether or not he also has hypertension? \n\n\n\n\n\n## (a) P(H and D) + P(H^c and D^c) = 6% + (1 - c(24% + 6% + 3%)) = 73%\n## (b) P(H|D) = 6%/9% = 66%. H and D not independent.\n\n\n\nAssume that females have pulse rates that are normally distributed with a mean of 74.0 beats per minute and a standard deviation of 12.5 beats per minute. \n\nIf 1 adult female is randomly selected, find the probability that her pulse rate is less than 80 beats per minute.\nIf 16 adult female are randomly selected, find the probability that their mean pulse rate is less than 80 beats per minute. \n\n\n\n\n\n## (a)\npnorm(q = 80, mean = 74, sd = 12.5)\n\n[1] 0.6843863\n\n## (b)\npnorm(q = 80, mean = 74, sd = 12.5/sqrt(16))\n\n[1] 0.9725711\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn parts (a) and (b), identify whether the events are disjoint, independent, or neither (events cannot be both disjoint and independent).\n\nYou and a randomly selected student from your class both earn A‚Äôs in this course.\nYou and your class study partner both earn A‚Äôs in this course.\nIf two events can occur at the same time, must they be dependent?\n\n\n\n\n# (a) If the class is not graded on a curve, they are independent. If graded on a curve, then neither independent nor disjoint { unless the instructor will only give one A, which is a situation we will ignore in parts (b) and (c).\n# (b) They are probably not independent: if you study together, your study habits would be related, which suggests your course performances are also related.\n# (c) No. See the answer to part (a) when the course is not graded on a curve. More generally: if two things are unrelated (independent), then one occurring does not preclude the other from occurring.\n\n\n\nA portfolio‚Äôs value increases by 18% during a financial boom and by 9% during normal times. It decreases by 12% during a recession. What is the expected return on this portfolio if each scenario is equally likely?\n\n\n# E(X) = 0.05\n\n\n\nWhat percentage of data that follow a standard normal distribution \\(N(\\mu=0, \\sigma=1)\\) is found in each region? Drawing a normal graph may help.\n\n\\(Z &lt; -1.75\\)\n\\(-0.7 &lt; Z &lt; 1.3\\)\n\\(|Z| &gt; 1\\)\n\n\n\n\n#(a)\npnorm(-1.75)\n\n[1] 0.04005916\n\n#(b)\npnorm(1.3) - pnorm(-0.7)\n\n[1] 0.6612359\n\n#(c)\npnorm(-1) + (1 - pnorm(1))\n\n[1] 0.3173105\n\n\n\n\nAt a university, 13% of students smoke.\n\nCalculate the expected number of smokers in a random sample of 100 students from this university.\nThe university gym opens at 9 am on Saturday mornings. One Saturday morning at 8:55 am there are 27 students outside the gym waiting for it to open. Should you use the same approach from part (a) to calculate the expected number of smokers among these 27 students?\n\n\n\n\n# (a) E(X) = 100 * 0.13 = 13\n# (b) No, these 27 students are not a random sample from the university's\n# student population. For example, it might be argued that the proportion of\n# smokers among students who go to the gym at 9am on a Saturday morning would\n# be lower than the proportion of smokers in the university as a whole.\n\n\n\nHead lengths of Virginia opossums follow a normal distribution with mean 104 mm and standard deviation 6 mm.\n\nCompute the \\(z\\)-scores for opossums with head lengths of 97 mm and 108 mm.\nWhich observation (97 mm or 108 mm) is more unusual or less likely to happen than another observation? Why?\n\n\n\n\n#(a)\n(97-104)/6\n\n[1] -1.166667\n\n(108-104)/6\n\n[1] 0.6666667\n\n#(b)\n# 97 mm more unusal\n\n\n\nSuppose weights of the checked baggage of airline passengers follow a nearly normal distribution with mean 45 pounds and standard deviation 3.2 pounds. Most airlines charge a fee for baggage that weigh in excess of 50 pounds. Determine what percent of airline passengers incur this fee.\n\n\npnorm(q = 50, mean = 45, sd = 3.2, lower.tail = FALSE)\n\n[1] 0.05908512\n\n\n\n\nWeights of adult human brains are normally distributed. Samples of weights of adult human brains, each of size \\(n=15\\), are randomly collected and the sample means are found. Is it correct to conclude that the sample means cannot be treated as being from a normal distribution because the sample size is too small?\n\n\n# No. because the original population is normally distributed, the sample means\n# will be normally distributed for any sample size, not just for n &gt; 30.\n\n\n\nAnnual incomes of physicians are known to have a distribution that is skewed to the right instead of being normally distributed. Assume that we collect a large (\\(n &gt; 30\\)) random sample of annual income of physicians. Can the distribution of those incomes in that sample be approximated by a normal distribution because the sample is large?\n\n\n# No. The sample of annual incomes will tend to have a distribution that is\n# skewed to the right, no matter how large the sample is. If we compute the\n# sample mean, we can consider that value to be one value in a normally\n# distributed population."
  },
  {
    "objectID": "exercise/exercise2.html",
    "href": "exercise/exercise2.html",
    "title": "Exercise 2",
    "section": "",
    "text": "An independent random sample is selected from an approximately normal population with an unknown standard deviation. Find the p-value for the given sample size and test statistic. Also determine if the null hypothesis of a two-tailed test would be rejected at \\(\\alpha = 0.05\\).\n\n\n\\(n = 11\\), \\(t_{test} = 1.91\\)\n\n\n\\(n = 17\\), \\(t_{test} = -3.45\\)\n\n\n\n\n\n\na) p-value= 0.08520488\n\n\nb) p-value= 0.003293571\n\n\n\n\nGeorgianna claims that in a small city renowned for its music school, the average child takes less than 5 years of piano lessons. We have a random sample of 20 children from the city, with a mean of 4.6 years of piano lessons and a standard deviation of 2.2 years.\n\nEvaluate Georgianna‚Äôs claim (or that the opposite might be true) using a hypothesis test.\nConstruct a 95% confidence interval for the number of years students in this city take piano lessons, and interpret it in context of the data.\nDo your results from the hypothesis test and the confidence interval agree? Explain your reasoning.\n\n\n\n\n## a) One sample t test with alpha 0.05\n## H0: mu &gt;= 5; H1: mu &lt; 5\n(t_test &lt;- (4.6 - 5) / (2.2/sqrt(20)))\n\n[1] -0.8131156\n\n(t_cri &lt;- qt(p = 0.05, df = 20 - 1)) ## Do not reject H0\n\n[1] -1.729133\n\n## b) \n4.6 + c(-1, 1) * qt(p = 0.975, df = 20 - 1) * (2.2 / sqrt(20))\n\n[1] 3.570368 5.629632\n\n## c)\n# Not agree. The test is one-sided, but the CI is two-sided.\n\n\n\nIn each of the following scenarios, determine if the data are paired.\n\nCompare pre- (beginning of semester) and post-test (end of semester) scores of students.\nAssess gender-related salary gap by comparing salaries of randomly sampled men and women.\nCompare artery thicknesses at the beginning of a study and after 2 years of taking Vitamin E for the same group of patients.\nAssess effectiveness of a diet regimen by comparing the before and after weights of subjects.\n\n\n\n\n\na) paired. \n\n\nb) independent. \n\n\nc) paired. \n\n\nd) paired. \n\n\n\n\nDr.¬†Yu decided to run two slight variations of the same exam. Prior to passing out the exams, he shuffled the exams together to ensure each student received a random version. Summary statistics for how students performed on these two exams are shown below. Anticipating complaints from students who took Version B, he would like to evaluate whether the difference observed in the groups is so large that it provides convincing evidence that Version B was more difficult (on average) than Version A. Test the claim with \\(\\alpha = 0.01\\).           \n\n\n\n\n\n\n\n\nVersion\n\\(n\\)\n\\(\\bar{x}\\)\n\\(s\\)\nmin\nmax\n\n\n\nA\n30\n79.4\n14\n45\n100\n\n\nB\n27\n74.1\n20\n32\n100\n\n\n\n\n\n\n\nn1 = 30; x1_bar = 79.4; s1 = 14\nn2 = 27; x2_bar = 74.1; s2 = 20\nA &lt;- s1^2 / n1; B &lt;- s2^2 / n2\ndf &lt;- (A + B)^2 / (A^2/(n1-1) + B^2/(n2-1))\n(df &lt;- floor(df))\n\n[1] 45\n\n## t_test\n(t_test &lt;- (x1_bar - x2_bar) / sqrt(s1^2/n1 + s2^2/n2))\n\n[1] 1.147085\n\n## t_cv\nqt(p = 0.01, df = df, lower.tail = FALSE)\n\n[1] 2.412116\n\n## p_value\npt(q = t_test, df = df, lower.tail = FALSE)\n\n[1] 0.1287044\n\n# the data do not convincingly show that one exam\n# version is more difficult than the other\n\n\n\nUndergraduate students taking an introductory statistics course at Marquette University conducted a survey about GPA and major. The ANOVA output is provided.\n\nWrite the hypotheses for testing for a difference between average GPA across majors.\nWhat is the conclusion of the hypothesis test?\nHow many students answered these questions on the survey, i.e.¬†what is the sample size?\n\n\n\n\n\n\n\n\n\nSource\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\nmajor\n2\n0.03\n0.015\n0.185\n0.8313\n\n\nResiduals\n195\n15.77\n0.081\n\n\n\n\n\n\n\n\n\n## (a) \n# H0: mu1 = mu2 = mu3; H1: not all mus are equal\n\n## (b)\n# p-value &gt; 0.05. The data do not provide convincing evidence of a\n# difference between the average GPAs across three groups of majors.\n\n## (c)\n# The total degrees of freedom is 195 + 2 = 197, so the sample size is 197 + 1 = 198.\n\n\n\nFind the test statistic, critical value(s) of \\(\\chi^2\\), and p-value, then determine whether there is sufficient evidence to support the given alternative hypothesis.\n\n\n\\(H_1: \\sigma \\ne 15\\), \\(\\alpha = 0.05\\), \\(n = 20\\), \\(s = 10\\).\n\n\\(H_1: \\sigma &gt; 12\\), \\(\\alpha = 0.01\\), \\(n = 5\\), \\(s = 18\\).\n\n\n\n\n## (a)\n## test statistic\n(20 - 1)*10^2/(15^2)\n\n[1] 8.444444\n\n## critical values\nqchisq(p = 0.05/2, df = 19, lower.tail = TRUE)\n\n[1] 8.906516\n\nqchisq(p = 0.05/2, df = 19, lower.tail = FALSE)\n\n[1] 32.85233\n\n## p-value\n2*pchisq((20 - 1)*10^2/(15^2), df = 19)\n\n[1] 0.03677387\n\n## (b)\n## test statistic\n(5 - 1)*18^2/(12^2)\n\n[1] 9\n\n## critical value\nqchisq(p = 0.01, df = 4, lower.tail = FALSE)\n\n[1] 13.2767\n\n## p-value\npchisq((5 - 1)*18^2/(12^2), df = 4, lower.tail = FALSE)\n\n[1] 0.06109948"
  },
  {
    "objectID": "exam/take_home_exam2.html",
    "href": "exam/take_home_exam2.html",
    "title": "Take-home Exam 2",
    "section": "",
    "text": "Important\n\n\n\nDue Wednesday, Nov 12, 11:59 PM\n\n\n\nPlease submit your work in one PDF file to D2L &gt; Assessments &gt; Dropbox. Multiple files or a file that is not in pdf format is not allowed.\nWith your submission, you commit to academic integrity through the following honor pledge:\n\n\nI recognize the importance of personal integrity in all aspects of life and work. I commit myself to truthfulness, honor and responsibility, by which I earn the respect of others. I support the development of good character and commit myself to uphold the highest standards of academic integrity as an important aspect of personal integrity. My commitment obliges me to conduct myself according to the Marquette University Honor Code.\n\n\nThis exam is to be entirely your own efforts. You may use any resources, including Generative AI tools, while working on this exam. However, you are NOT allowed to discuss the exam with anyone except Dr.¬†Yu, nor may you post any of the exam questions on any platform to solicit answers. Any violation of this policy will be treated as academic misconduct. If you are caught cheating, your case will be reported immediately to the Academic Integrity Council.\nIn your exam, please number problems/questions in order.\nEven if you use R to compute your final answer, you must show your calculation steps and/or explain your reasoning in order to receive full credit.\nHandwritten tables and figures receive no credits.\n\nExam Problems (50 points)\n\n\n[30 points] A Marquette professor of College of Education believes that Marquette engineering students are talented and their mean ACT score is greater than 30. She would like to collect evidence of her claim and do a hypothesis testing. Marquette data center director said that the population of ACT score of engineering majors follows a normal distribution with the population variance of value 9.\n\nIs the claim a \\(H_0\\) or \\(H_1\\) claim? Write down \\(H_0\\) and \\(H_1\\) based on her claim. Is this a left-tailed, right-tailed, or two-tailed test? Please explain.\nDue to COVID-19, Marquette‚Äôs research budget is cut. The professor‚Äôs assistant can only collect a small random sample of size 16. Are one-sample \\(z\\)-test and/or \\(t\\)-test procedures appropriate with such a small size? If yes, why? If not, add conditions to make the procedures appropriate.\nThe assistant never takes MATH 4720 and she only remembers that the professor said 31.7 is the threshold value to be compared with the observed sample mean \\(\\overline{x}\\). Should she reject \\(H_0\\) when \\(\\bar{x} &gt; 31.7\\) or when \\(\\bar{x} &lt; 31.7\\)? Please explain.\nGiven the rejection region in (c), write down the definition of the Type I error rate in the problem and determine its value.\nIf the \\(p\\)-value is 0.1, what is the value of observed sample mean \\(\\overline{x}\\)?\nBased on (c), (d), (e), Is \\(H_0\\) rejected?  \n\n\n\n\n\n# ------------\n## (a) \n# ------------\n# right-tailed, H1 claim\n# H0: mu &lt;= 30\n# H1: mu &gt; 30\n\n# ------------\n## (b) \n# ------------\n# yes because the ACT score is normally distributed\n\n# ------------\n## (c) \n# ------------\n# x_bar &gt; 31.7 because it is a right-tailed test\n\n# ------------\n## (d)\n# ------------\n# type I error rate = P(reject H0 | H0 is true)\nmu0 &lt;- 30; x_bar_cv &lt;- 31.7; sig2 &lt;- 9; n &lt;- 16\npnorm((x_bar_cv - mu0) / sqrt(sig2 / n), lower.tail = FALSE)\n\n[1] 0.0117053\n\n# ------------\n## (e)\n# ------------\np_val &lt;- 0.1\nqnorm(p_val, mean = mu0, sd = sqrt(sig2 / n), lower.tail = FALSE)\n\n[1] 30.96116\n\n# ------------\n## (f)\n# ------------\n# Do not reject H0\n# There is insufficient evidence to support the claim that engineering students has mean ACT &gt; 30.\n\n# ------------\n## (g) \n# ------------\n# type II error rate = P(do not reject H0 | H0 is false)\nx_bar_cv_new &lt;- 31.5\nmu1 &lt;- 31\npnorm((x_bar_cv_new - mu1) / sqrt(sig2 / n))\n\n[1] 0.7475075\n\n\n\n\n\n[20 points] A corporation in Chicago makes insulation shields for electrical wires using three types of machines. The company wants to evaluate the variation in the inside diameter dimensions of the shields produced by the machine A, B and C. A quality control engineer randomly selects shields produced by each of the machines and records the inside diameter of each shield (in millimeters), as shown in Table¬†1. The data can be downloaded at shield_machine.csv. She wants to determine whether the mean diameter produced by the three machines differ.\n\nCheck normality and homogeneity of variance using QQ-plot and boxplot. Comment on your plots.\nBased on your findings in (a), would it be appropriate to proceed with an analysis of variance (ANOVA)? Please explain.\nIf your answer is YES in (b), conduct ANOVA on the original data. If NOT, do the natural log (log with base \\(e = 2.71828...\\)) transformation on the data and show the transformed data satisfy ANOVA assumptions. Then conduct ANOVA on the transformed data. Show the ANOVA table and determine whether the mean diameters differ.\n\n\n\n\n\n\nTable¬†1: Sampled data of the inside diameter of shields produced by machine A, B and C.\n\n\n\n\nShield\nMachine A\nMachine B\nMachine C\n\n\n\n1\n31.0\n34.2\n39.3\n\n\n2\n7.0\n39.8\n36.2\n\n\n3\n8.2\n15.8\n47.8\n\n\n4\n9.1\n29.8\n58.7\n\n\n5\n14.4\n61.1\n82.3\n\n\n6\n10.5\n31.1\n30.8\n\n\n7\n10.5\n18.3\n11.0\n\n\n8\n5.1\n10.6\n15.0\n\n\n9\n18.2\n11.2\n46.3\n\n\n10\n25.2\n20.0\n36.4\n\n\n11\n6.5\n26.9\n54.5\n\n\n12\n13.4\n52.6\n25.3\n\n\n13\n23.7\n65.2\n24.6\n\n\n14\n17.0\n37.3\n90.5\n\n\n15\n8.6\n19.0\n64.0\n\n\n\n\n\n\n\n\n\n# ------------\n## (a)\n# ------------\nshield_machine &lt;- read.csv(\"shield_machine.csv\")\nshield_machine_new &lt;- matrix(shield_machine[, 1], 15, 3)\n\ncar::qqPlot(shield_machine_new[, 1])\n\n\n\n\n\n\n\n[1]  1 10\n\ncar::qqPlot(shield_machine_new[, 2])\n\n\n\n\n\n\n\n[1] 13  5\n\ncar::qqPlot(shield_machine_new[, 3])\n\n\n\n\n\n\n\n[1] 14  5\n\n# not very normal but OK\n\nboxplot(shield_machine_new)\n\n\n\n\n\n\n## variances are not homogeneous\n\n# ------------\n## (b)\n# ------------\n## Not appropriate because ANONA requires normality of data and homogeneous variances.\n\n# ------------\n## (d)\n# ------------\napply(shield_machine_new, 2, var)\n\n[1]  59.37781 296.37210 524.69314\n\napply(shield_machine_new, 2, mean)\n\n[1] 13.89333 31.52667 44.18000\n\napply(shield_machine_new, 2, var)/ (apply(shield_machine_new, 2, mean)^2)\n\n[1] 0.3076177 0.2981818 0.2688153\n\n## log-transformation\nshield_machine_log &lt;- shield_machine\nshield_machine_log[, 1] &lt;- log(shield_machine_log[, 1])\nlm_res_log &lt;- lm(diameter ~ machine, data = shield_machine_log)\nanova(lm_res_log)\n\nAnalysis of Variance Table\n\nResponse: diameter\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nmachine    2 10.456  5.2278  16.404 5.435e-06 ***\nResiduals 42 13.385  0.3187                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nshield_machine_log_new &lt;- apply(shield_machine_new, 2, log)\napply(shield_machine_log_new, 2, var)\n\n[1] 0.2871122 0.3234484 0.3454946\n\nboxplot(shield_machine_log_new)\n\n\n\n\n\n\n# fitted_val_log &lt;- matrix(lm_res_log$fitted.values, 15, 3)\n# colnames(fitted_val_log) &lt;- c(\"A\", \"B\", \"C\")\n# qqPlot(lm_res_log$residuals, pch = 19, id = FALSE, ylab = \"residuals\", \n#        main = \"Normal Probability Plot for Residuals\")\n# plot(lm_res_log$fitted.values, lm_res_log$residuals, xlab = \"Fitted Value\",\n#      ylab = \"Residual\", main = \"Versus Fits\", pch = 19, col = \"red\")\n# abline(h = 0)\n\n## another way\n# aov_res &lt;- aov(diameter ~ machine, data = data_sim_log)\n# summary(aov_res)\n\n\n\nAI Usage Declaration. Using GenAI is permitted for this course. If you choose to use GenAI to assist with your exam, you must include a brief statement documenting your use. Please provide the following information:\n\n\nWhy/How I Used AI Why do you need to use GenAI? Which tool did you use? Describe your prompts or questions. What and how did you ask the AI to help you?\n\nGenerated Output Include a screenshot or excerpt (copy and paste) of the AI‚Äôs response.\n\nHow I Used the Output Did you revise it? Did you use it directly, or compare it with your answers? What decisions did you make based on the output?"
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Final Project Report Guidelines",
    "section": "",
    "text": "Please send me your entire work (written report, code, data, etc) by May 8, 2025 10 AM.\nYou receive 0 point if you miss the deadline."
  },
  {
    "objectID": "project-description.html#deadline",
    "href": "project-description.html#deadline",
    "title": "Final Project Report Guidelines",
    "section": "",
    "text": "Please send me your entire work (written report, code, data, etc) by May 8, 2025 10 AM.\nYou receive 0 point if you miss the deadline."
  },
  {
    "objectID": "project-description.html#project-writing",
    "href": "project-description.html#project-writing",
    "title": "Final Project Report Guidelines",
    "section": "Project Writing",
    "text": "Project Writing\nYour project can be in either of the following categories:\n\nData Analysis (DA) using one or more machine learning methods learned in class.\nIntroduce a new machine learning model/method/algorithm (ML) and compare it with the model/method/algorithms learned in class.\n\n\nStructure\nIf you choose to do DA, your paper should include the following sections:\n\nIntroduction: State why you think the questions you would like to answer are important or interesting, and why you think the method(s) you consider is an appropriate one to answer your questions.\nData: Describe the selected data set. Perform a thorough exploratory data analysis.\nAnalysis:\n\nExplain the chosen model/method.\nShow why the chosen model(s) is appropriate and better than others.\nAnswer your research questions by the analysis result.\n\nConclusion: Restate your research question, and summarize how you learn from data to answer your questions. What is the contribution of this project? Discuss any limitation of your model/method, and how it could be improved for better inference or prediction results.\nReferences/Bibliography: Include a detailed list of references, including papers, books, websites, code, and any idea/work that is not produced by yourself.\n\nIf you choose to do ML, your paper should include the following sections:\n\nIntroduction: State why you choose to learn this new method. Provide an overview and little history of the method. Describe the intuition and idea of the method. What are the pros and cons of the method?\nModel/Method: Provide the mathematical expression of the model. Explain the model and its properties, and how we do the supervised or unsupervised learning with the model.\nSimulation: Do a simulation study, and compare the chosen method with other methods learned in class. Determine which method performs better under what conditions.\nDiscussion: Based on the simulation results, discuss the advantages and disadvantages of the chosen method. Discuss any variants of the chosen method.\nReferences/Bibliography: Include a detailed list of references, including papers, books, websites, code, and any idea/work that is not produced by yourself.\n\n\n\nFormat and Layout\n\nYour project paper is saved as one PDF.\nYour paper should have your project title and your name on the first page. Date, Abstract, Keywords are optional.\nExcept the first title page, the margins should be no larger than one inch.\nExcept the project title and section title, the font size is 12 pt.\nPlease use 1.5 or double line spacing.\nYour report, including everything, should be at least 12 pages long, but no more than 15 pages.\nYour code should NOT be printed in the paper.\n\n\n\nCode\n\nYour code should be able to reproduce all the numerical results, outputs, tables, and figures shown in the report, including the source of the raw data (where you find and load the data) if the project is about data analysis."
  },
  {
    "objectID": "project-description.html#project-evaluation",
    "href": "project-description.html#project-evaluation",
    "title": "Final Project Report Guidelines",
    "section": "Project Evaluation",
    "text": "Project Evaluation\nYour project will be evaluated soley by Dr.¬†Yu based on\n\nContent:\n\nThe quality of research question and relevancy of data to those questions? For example, the relationship between human height and weight is a BAD question. An elementary-school height and weight data set is a BAD data set.\nThe quality of the chosen model. For example, one-way ANOVA is a BAD model.\n\nCorrectness, Completeness and Complexity:\n\nAre machine learning methods carried out and explained correctly?\nDoes project include rigorous analysis and models? Simple linear regression model lacks complexity.\n\nWriting: What is the quality of the machine learning model/method presentation, visualization, writing, and explanations.\nFormat: Does the report follow the required format?\nCreativity and Critical Thought: Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\nReproducibility: Can your code reproduce what you show in the paper?\nReference: Do you cite others work properly?"
  },
  {
    "objectID": "exercise.html",
    "href": "exercise.html",
    "title": "Exercise",
    "section": "",
    "text": "More extra exercises to be added as the semester progresses.\n\n\n\n\n\n\n\n\n\n\nNo.\n\n\n\nTitle\n\n\n\n\n\n\n\n\nExercise 1\n\n\nExercises for Exam 1\n\n\n\n\n\n\nExercise 2\n\n\nExercises for Exam 2\n\n\n\n\n\n\nExercise 3\n\n\nExercises for Final Exam\n\n\n\n\n\n\nKey Topics of Multiple Linear Regression\n\n\nMultiple Linear Regression for Final Exam\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Exercise"
    ]
  }
]