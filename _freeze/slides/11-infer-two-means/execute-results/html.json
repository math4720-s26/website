{
  "hash": "b0d627bcf63448986a9a47a856d20665",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Comparing Two Population Means `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 640 512\" style=\"height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M64 64a64 64 0 1 1 128 0A64 64 0 1 1 64 64zM25.9 233.4C29.3 191.9 64 160 105.6 160h44.8c27 0 51 13.4 65.5 34.1c-2.7 1.9-5.2 4-7.5 6.3l-64 64c-21.9 21.9-21.9 57.3 0 79.2L192 391.2V464c0 26.5-21.5 48-48 48H112c-26.5 0-48-21.5-48-48V348.3c-26.5-9.5-44.7-35.8-42.2-65.6l4.1-49.3zM448 64a64 64 0 1 1 128 0A64 64 0 1 1 448 64zM431.6 200.4c-2.3-2.3-4.9-4.4-7.5-6.3c14.5-20.7 38.6-34.1 65.5-34.1h44.8c41.6 0 76.3 31.9 79.7 73.4l4.1 49.3c2.5 29.8-15.7 56.1-42.2 65.6V464c0 26.5-21.5 48-48 48H496c-26.5 0-48-21.5-48-48V391.2l47.6-47.6c21.9-21.9 21.9-57.3 0-79.2l-64-64zM272 240v32h96V240c0-9.7 5.8-18.5 14.8-22.2s19.3-1.7 26.2 5.2l64 64c9.4 9.4 9.4 24.6 0 33.9l-64 64c-6.9 6.9-17.2 8.9-26.2 5.2s-14.8-12.5-14.8-22.2V336H272v32c0 9.7-5.8 18.5-14.8 22.2s-19.3 1.7-26.2-5.2l-64-64c-9.4-9.4-9.4-24.6 0-33.9l64-64c6.9-6.9 17.2-8.9 26.2-5.2s14.8 12.5 14.8 22.2z\"/></svg>`{=html}'\ntitle-slide-attributes:\n  data-background-image: ../images/bg.png\n  # data-background-size: stretch\n  # data-slide-number: none\nformat: \n  live-revealjs: \n    output-file: 11-infer-two-means-slides.html\n    # theme: slides.scss\nwebr:\n  cell-options:\n    autorun: false\n  packages:\n    - tidyverse\nknitr:\n  opts_chunk:\n    out-width: 100%\n    echo: false\n---\n\n\n# {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n<!-- begin: webr fodder -->\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n<!-- end: webr fodder -->\n\n\n<!-- # [Hypothesis Testing]{.orange}{background-image=\"https://upload.wikimedia.org/wikipedia/commons/4/42/William_Sealy_Gosset.jpg\" background-size=\"cover\" background-position=\"50% 50%\" background-color=\"#447099\"} -->\n\n# Comparing Two Population Means\n\n- ### Dependent Samples (Matched Pairs)\n- ### Independent Samples\n\n\n## Why Comparing Two Populations\n\n- Often faced with a comparison of parameters from different populations.\n\n  + <span style=\"color:blue\"> Comparing the _mean_ annual income for _Male_ and _Female_ groups. </span>\n  \n  + <span style=\"color:blue\"> Testing if a diet used for losing weight is effective from _Placebo_ and _New Diet_ samples. </span>\n\n. . .\n\n- If these two samples are drawn from populations with means $\\mu_1$ and $\\mu_2$ respectively,\n<span style=\"color:blue\"> $$\\begin{align} \n  &H_0: \\mu_1 = \\mu_2 \\\\ \n  &H_1: \\mu_1 > \\mu_2 \n  \\end{align}$$ </span>\n  + $\\mu_1$: *male* mean annual income; $\\mu_2$: *female* mean annual income \n  + $\\mu_1$: mean weight loss from the *New Diet* group; $\\mu_2$: mean weight loss from the *Placebo* group\n\n\n::: notes\n- Often we are faced with an inference involving a comparison of parameters from different populations.\n\n  + <span style=\"color:blue\"> Comparing the mean annual income for male and female groups. </span>\n  \n  + <span style=\"color:blue\"> Testing if a diet used for losing weight is effective from Placebo samples and New Diet samples. </span>\n\n- If these two samples are drawn from populations with means $\\mu_1$ and $\\mu_2$ respectively, then the testing problem can be formulated as\n<span style=\"color:blue\"> $$\\begin{align} \n  &H_0: \\mu_1 = \\mu_2 \\\\ \n  &H_1: \\mu_1 > \\mu_2 \n  \\end{align}$$ </span>\n  + $\\mu_1$: male mean annual income; $\\mu_2$: female mean annual income \n  + $\\mu_1$: weight loss from the New Diet group; $\\mu_2$: weight loss from the Placebo group\n  \n:::\n\n## Dependent and Independent Samples\n\n- The two samples can be *independent* or *dependent*.\n\n\n:::: columns\n\n::: {.column width=\"70%\"}\n\nTwo samples are **dependent** or **matched pairs** if the sample values are matched, where *the matching is based on some inherent relationship*.\n\n  + <span style=\"color:blue\"> Height data of fathers and daughters. The height of each dad is **matched** with the height of his daughter. </span>\n  \n  + <span style=\"color:blue\"> Weights of subjects measure before and after some diet treatment. The subjects are the **same before and after measurements**. </span>\n\n:::\n\n::: {.column width=\"30%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/11-infer-two-means/dad_daughter.jpeg){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n::: notes\n- The statistical methods are different for these two types of samples.\n:::\n\n\n\n## Dependent Samples (Matched Pairs)\n\n- Subject 1 may refer to \n  + the *first matched pair* (dad-daughter)\n  + the *same* person with two measurements (before and after)\n\n:::: columns\n\n::: {.column width=\"70%\"}\n\n::: center\n\n| Subject  |  (Dad) Before |  (Daughter) After \n| :--------------: | :-----------------: | :------------------:\n| 1 | $x_{b1}$ | $x_{a1}$\n| 2 | $x_{b2}$ | $x_{a2}$\n| 3 | $x_{b3}$ | $x_{a3}$\n| $\\vdots$ | $\\vdots$ |  $\\vdots$ \n| $n$ | $x_{bn}$ | $x_{an}$\n\n:::\n\n:::\n\n::: {.column width=\"30%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/11-infer-two-means/weight.jpeg){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n::: notes\n- Subject 1 dad is 7 feet tall, subject 2 is 5 feet fall\n- Subject 1 has weight 200 pounds, subject 2 weight 100 pounds\n- It makes more sense to tie the two samples together\n:::\n\n\n## Independent Samples\n\n:::: columns\n\n::: {.column width=\"80%\"}\n\nTwo samples are **independent** if the sample values from one population are *not related to* the sample values from the other.\n\n  + <span style=\"color:blue\"> Salary samples of men and women. Two samples are drawn independently from the male and female groups. </span>\n  \n:::\n\n\n::: {.column width=\"20%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/11-infer-two-means/male_female.jpeg){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n- Subject 1 of Group 1 has nothing to do with the subject 1 of Group 2.\n\n::: midi\n\n|Subject of Group 1 (Male)  | Measurement of Group 1   | Subject of Group 2 (Female) |  Measurement of Group 2 \n| :-----------------: | :------------------: | :------------------: | :------------------:\n| 1 | $x_{11}$ |  1  | $x_{21}$\n| 2 | $x_{12}$ |  2  | $x_{22}$\n| 3 | $x_{13}$ |  3  | $x_{23}$\n| $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$\n| $n_1$ | $x_{1n_1}$ | $\\vdots$ | $\\vdots$\n|  |  |  $n_2$ | $x_{2n_2}$\n\n:::\n\n\n## Inference from Two Samples\n\n<!-- - We start with the inference methods for independent samples. -->\n\n<!-- - Remember, the inference methods include estimation and hypothesis testing, and we start with hypothesis testing.-->\n\n<!-- - To do inference for independent two samples, we start with the sampling distribution of $\\$ -->\n- The statistical methods are different for these two types of samples.\n\n. . .\n\n- **Good news**: The concepts of CI and HT for one population can be applied to two-population cases.\n\n. . .\n\n- **$\\text{CI = point estimate} \\pm \\text{margin of error (E)}$**, e.g., $\\overline{x} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}$\n\n- Margin of error = critical value $\\times$ standard error of the point estimator\n\n. . .\n\n- The 6 testing steps are the same, and both critical value and $p$-value method can be applied too, e.g., $t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}}$\n\n\n::: notes\n\n- To compare two populations from two samples, we will learn and find\n\n  + the point estimate for the parameter we are interested and its standard error\n  \n  + which critical value and test statistic should be used in which cases\n\n:::\n\n\n\n# [Inferences About Two Means: Dependent Samples (Matched Pairs)]{.orange}{background-image=\"./images/11-infer-two-means/nebulae.jpg\" background-size=\"cover\" background-position=\"50% 50%\" background-color=\"#447099\"}\n\n\n## Hypothesis Testing for Dependent Samples\n\n::: alert\nTo analyze a **paired** data set, simply analyze the differences! \n:::\n\n\n:::: columns\n\n::: {.column width=\"60%\"}\n\n| Subject  |  $x_1$  | $x_2$ | **Difference $d = x_1 - x_2$** \n| :------------: | :--------------: | :--------------: | :-------------------------:\n| 1 | $x_{11}$ | $x_{21}$ | $\\color{red}{d_1}$\n| 2 | $x_{12}$ | $x_{22}$ | $\\color{red}{d_2}$\n| 3 | $x_{13}$ | $x_{23}$ | $\\color{red}{d_3}$\n| $\\vdots$ | $\\vdots$ |  $\\vdots$ | $\\color{red}{\\vdots}$\n| $n$ | $x_{1n}$ | $x_{2n}$ | $\\color{red}{d_n}$\n\n:::\n\n\n::: {.column width=\"40%\"}\n\n- $\\mu_d = \\mu_1 - \\mu_2$\n\n- <span style=\"color:blue\"> $\\begin{align} & H_0: \\mu_1 - \\mu_2 = 0 \\iff \\mu_d = 0 \\\\ & H_1: \\mu_1 - \\mu_2 > 0 \\iff \\mu_d > 0 \\\\ & H_1: \\mu_1 - \\mu_2 < 0 \\iff \\mu_d < 0  \\\\ & H_1: \\mu_1 - \\mu_2 \\ne 0 \\iff \\mu_d \\ne 0 \\end{align}$ </span>\n\n:::\n\n::::\n\n<br>\n\n\n::: alert\nThe point estimate of $\\mu_1 - \\mu_2$ is $\\overline{x}_1 - \\overline{x}_2 = \\overline{d}$.\n:::\n\n\n\n\n::: notes\n- Transform two samples into _one sample_ by taking the difference between paired measurements.\n:::\n\n\n## Inference for Paired Data\n\n- Requirements: the sample differences $\\color{blue}{d_i}$s are\n  + random sample\n  + from a normal distribution and/or $n > 30$ (tested by QQ-plot of $d_i$s)\n  \n- Follow the same procedure as the **one-sample $t$-test**!\n\n- The **test statistic** is \n$\\color{blue}{t_{test} = \\frac{\\overline{d}-0}{s_d/\\sqrt{n}}} \\sim T_{n-1}$ under $H_0$ where $\\overline{d}$ and $s_d$ are the mean and SD of the difference samples $(d_1, d_2, \\dots, d_n)$.\n\n- The **critical value** $t_{\\alpha, n-1}$ and $t_{\\alpha/2, n-1}$.\n\n. . .\n\nPaired $t$-test| Test Statistic  | Confidence Interval for $\\mu_d = \\mu_1 - \\mu_2$\n---------------- | --------------- | --------------------------------------\n$\\sigma_d$ is unknown | $\\large t_{test} = \\frac{\\overline{d}}{s_d/\\sqrt{n}}$ | $\\large \\overline{d} \\pm t_{\\alpha/2, n-1} \\frac{s_d}{\\sqrt{n}}$\n\n- The test from matched pairs is called a **paired $t$-test**.\n\n::: notes\n\n<!-- - Use $df = n-1$ to get the $p$-value, critical value and CI. -->\n(Yes, the same as one-sample $t$-test)\n\n:::\n\n\n\n## Example\n\n- Consider a capsule used to reduce blood pressure (BP) for the hypertensive individuals. Sample of 10 hypertensive individuals take the medicine for 4 weeks.\n\n- Does the data provide sufficient evidence that the treatment is effective in reducing BP?\n\n:::: columns\n\n::: {.column width=\"70%\"}\n\n::: midi\n| Subject  | Before $(x_b)$  | After $(x_a)$ | **Difference $d = x_b - x_a$** \n| :--------------: | :-----------------: | :------------------: | :-----------------------:\n| 1 | 143 | 124 | 19\n| 2 | 153 | 129 | 24\n| 3 | 142 | 131 | 11\n| 4 | 139 | 145 | -6\n| 5 | 172 | 152 | 20\n| 6 | 176 | 150 | 26\n| 7 | 155 | 125 | 30\n| 8 | 149 | 142 | 7\n| 9 | 140 | 145 | -5\n| 10 | 169| 160 | 9\n:::\n\n:::\n\n\n::: {.column width=\"30%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/11-infer-two-means/drug.jpeg){fig-align='center' width=80%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n## Example Cont'd\n\n- $\\overline{d} = 13.5$, $s_d= 12.48$.\n\n- $\\mu_1 =$ Mean Before, $\\mu_2 =$ Mean After, and $\\mu_d = \\mu_1 - \\mu_2$.\n\n. . .\n\n- Step 1: <span style=\"color:blue\"> $\\begin{align} &H_0: \\mu_1 = \\mu_2 \\iff \\mu_d = 0\\\\ &H_1: \\mu_1 > \\mu_2 \\iff \\mu_d > 0 \\end{align}$ </span>\n\n. . .\n\n- Step 2:  <span style=\"color:blue\"> $\\alpha = 0.05$ </span>\n\n. . .\n\n- Step 3: <span style=\"color:blue\"> $t_{test} = \\frac{\\overline{d}}{s_d/\\sqrt{n}} = \\frac{13.5}{12.48/\\sqrt{10}} = 3.42$ </span>\n\n. . .\n\n- Step 4-c:  <span style=\"color:blue\"> $t_{\\alpha, n-1} = t_{0.05, 9} = 1.833$.</span>\n\n. . .\n\n- Step 5-c:  <span style=\"color:blue\"> Since $\\small t_{test} = 3.42 > 1.833  = t_{\\alpha, n-1}$, we reject $H_0$.</span>\n\n<!-- > - Step 4-p:  <span style=\"color:blue\"> The $p$-value is $P(T < t_{test}) \\approx 0$ </span> -->\n\n<!-- > - Step 5-p:  <span style=\"color:blue\"> We reject $H_0$ if $p$-value < $\\alpha$. Since $p$-value $\\approx 0 < 0.05  = \\alpha$, we reject $H_0$.</span> -->\n\n. . .\n\n- Step 6:  <span style=\"color:blue\"> **There is sufficient evidence to support the claim that the drug is effective in reducing blood pressure.** </span>\n\n\n::: notes\nWe reject $H_0$ if $\\small t_{test} > t_{\\alpha, n-1}$.\n:::\n\n\n##\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-infer-two-means_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## Example Cont'd\n\n- The 95% CI for $\\mu_d = \\mu_1 - \\mu_2$ is $$\\begin{align}\\overline{d} \\pm t_{\\alpha/2, df} \\frac{s_d}{\\sqrt{n}} &= 13.5 \\pm t_{0.025, 9}\\frac{12.48}{\\sqrt{10}}\\\\ &= 13.5 \\pm 8.927 \\\\ &= (4.573, 22.427).\\end{align}$$\n\n- 95% confident that the *mean difference* in blood pressure is between 4.57 and 22.43. \n\n- Since the interval *does NOT include 0*, it leads to the same conclusion as rejection of $H_0$.\n\n\n## Two-Sample Paired Test in R\n\n:::: columns\n\n::: {.column width=\"50%\"}\n\n::: midi\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npair_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   before after\n1     143   124\n2     153   129\n3     142   131\n4     139   145\n5     172   152\n6     176   150\n7     155   125\n8     149   142\n9     140   145\n10    169   160\n```\n\n\n:::\n:::\n\n:::\n\n<br>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(d <- pair_data$before - pair_data$after)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 19 24 11 -6 20 26 30  7 -5  9\n```\n\n\n:::\n\n```{.r .cell-code}\n(d_bar <- mean(d))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 14\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: fragment\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(s_d <- sd(d))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## t_test\n(t_test <- d_bar/(s_d/sqrt(length(d))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.4\n```\n\n\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## t_cv\nqt(p = 0.95, df = length(d) - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.8\n```\n\n\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## p_value\npt(q = t_test, df = length(d) - 1, \n   lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0038\n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n::::\n\n\n## Two-Sample Paired Test in R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## CI\nd_bar + c(-1, 1) * qt(p = 0.975, df = length(d) - 1) * (s_d / sqrt(length(d))) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  4.6 22.4\n```\n\n\n:::\n:::\n\n\n. . .\n\n::: midi\n\n<br>\n\n\n\n::: {.cell layout-align=\"center\" highlight.output='c(5, 6)'}\n\n```{.r .cell-code}\n## t.test() function\nt.test(x = pair_data$before, y = pair_data$after, alternative = \"greater\", mu = 0, paired = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  pair_data$before and pair_data$after\nt = 3, df = 9, p-value = 0.004\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 6.3 Inf\nsample estimates:\nmean difference \n             14 \n```\n\n\n:::\n:::\n\n\n:::\n\n. . .\n\n- Be careful about the one-sided CI! **We should use the two-sided CI!** \n\n\n\n# [Inferences About Two Means: Independent Samples]{.orange}{background-image=\"./images/11-infer-two-means/dice1.jpg\" background-size=\"cover\" background-position=\"50% 50%\" background-color=\"#447099\"}\n\n\n## Compare Population Means: Independent Samples\n\n- Whether stem cells can improve heart function.\n\n- The relationship between pregnant womens' smoking habits and newborns' weights.\n\n- Whether one variation of an exam is harder than another variation.\n\n\n:::: columns\n\n::: {.column width=\"55%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-infer-two-means_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/11-infer-two-means/exam.jpeg){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n## \n\n::: midi\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/11-infer-two-means/two_indept_sample_overview_new1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n\n## Testing for Independent Samples $(\\sigma_1 \\ne \\sigma_2)$\n\n- Requirements:\n  \n  + The two samples are *independent*.\n  \n  + Both samples are a *random sample*.\n  \n  + $n_1 > 30$, $n_2 > 30$ and/or both samples are from a *normally* distributed population.\n\n- Interested in whether the two population means **$\\mu_1$ and $\\mu_2$ are equal or not**, or one is larger than the other. \n\n- $H_0: \\mu_1 = \\mu_2$\n\n- It is equivalent to testing if **their difference is zero**. \n\n- $H_0: \\mu_1 - \\mu_2 = 0$\n\n<!-- - We start with finding a point estimate for $\\mu_1 - \\mu_2$.  -->\n::: question\nWe start with finding a point estimate for $\\mu_1 - \\mu_2$. What is the best point estimator for $\\mu_1 - \\mu_2$?\n:::\n\n. . .\n\n$\\overline{X}_1 - \\overline{X}_2$ is the **best point estimator** for $\\mu_1 - \\mu_2$!\n\n\n::: notes\n\nThe difference between the sample means ** $\\overline{X}_1 - \\overline{X}_2$ is the best point estimator for $\\mu_1 - \\mu_2$ **!\n\n:::\n\n\n## Sampling Distribution of $\\overline{X}_1 - \\overline{X}_2$\n\nIf the two samples are from *independent normally* distributed populations or $n_1 > 30$ and $n_2 > 30$,\n$$\\small \\overline{X}_1 \\sim N\\left(\\mu_1, \\frac{\\sigma_1^2}{n_1} \\right), \\quad \\overline{X}_2 \\sim N\\left(\\mu_2, \n\\frac{\\sigma_2^2}{n_2} \\right)$$\n\n. . .\n\n$\\overline{X}_1 - \\overline{X}_2$ has the sampling distribution \n$$\\small \\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} {\\color{red}{+}} \\frac{\\sigma_2^2}{n_2} \\right) $$\n\n. . .\n\n$$\\small Z = \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\sim N(0, 1)$$\n\n\n\n## Test Statistic for Independent Samples $(\\sigma_1 \\ne \\sigma_2)$\n\n- With $D_0$ a hypothesized value (often 0),\n\n  + <span style=\"color:blue\"> $\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 \\le D_0\\\\ &H_1: \\mu_1 - \\mu_2 > D_0 \\end{align}$ </span> (right-tailed)\n\n  + <span style=\"color:blue\"> $\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 \\ge D_0\\\\ &H_1: \\mu_1 - \\mu_2 < D_0 \\end{align}$ </span> (left-tailed)\n  \n  + <span style=\"color:blue\"> $\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 = D_0\\\\ &H_1: \\mu_1 - \\mu_2 \\ne D_0 \\end{align}$ </span> (two-tailed)\n  \n- If $\\sigma_1$ and $\\sigma_2$ are **known**, the **test statistic** is the z-score of $\\small \\overline{X}_1 - \\overline{X}_2$ under $H_0$: $$z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} $$\n\n- Then find $z_{\\alpha}$ or $z_{\\alpha/2}$ and follow our testing steps!\n\n\n## Test Statistic for Independent Samples $(\\sigma_1 \\ne \\sigma_2)$\n\n- If $\\sigma_1$ and $\\sigma_2$ are **unknown**, the test statistic becomes $t_{test}$:\n\n$$t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} $$\n\n<!-- If $\\sigma_1$ and $\\sigma_2$ are **known** -->\n\n. . .\n\n- The **critical value** $t_{\\alpha, df}$ (one-tailed) and $t_{\\alpha/2, df}$ (two-tailed), and the $t$ distribution used to compute the **$p$-value** has the degrees of freedom\n$$\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},$$ where $\\small A = \\dfrac{s_1^2}{n_1}$ and $\\small B = \\dfrac{s_2^2}{n_2}$.\n\n<!-- $$ df = \\frac{(n_1-1)(n_2-1)}{(1-c)^2(n_1-1)+c^2(n_2-1)},$$ where $c = \\frac{\\frac{s_1^2}{n_1}}{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$. -->\n\n- If the $df$ is not an integer, we round it down to an integer.\n<!-- > - <span style=\"color:blue\"> No need to memorize it. No need to know why this ugly value. Just use it! </span> -->\n\n\n::: notes\nsimilar to the one sample case,\n:::\n\n\n## Inference from Independent Samples $(\\sigma_1 \\ne \\sigma_2)$\n\n$\\large \\color{red}{\\sigma_1 \\ne \\sigma_2}$       | Test Statistic  | Confidence Interval for $\\mu_1 - \\mu_2$\n--------- | ------------ | ------------------\nknown   | $\\large z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}$    |  $\\large (\\overline{x}_1 - \\overline{x}_2) \\pm z_{\\alpha/2} \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}$\nunknown | $\\large t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}}$ | $\\large (\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}$\n\n<!-- - use $df = \\frac{(n_1-1)(n_2-1)}{(1-c)^2(n_1-1)+c^2(n_2-1)}$ where $c = \\frac{\\frac{s_1^2}{n_1}}{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$ to get the $p$-value, critical value, and CI. -->\n\n- Use $\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},$ where $\\small A = \\dfrac{s_1^2}{n_1}$ and $\\small B = \\dfrac{s_2^2}{n_2}$ to get the $p$-value, critical value, and CI.\n\n- The unequal-variance t-test is called **Welch's t-test**.\n\n\n## Example: Two-Sample t-Test\n\n\n:::: columns\n\n::: {.column width=\"75%\"}\n\nDoes an oversized tennis racket exert less stress/force on the elbow? The data show\n\n  + **Oversized**: $n_1 = 33$, $\\overline{x}_1 = 25.2$, $s_1 = 8.6$\n  \n  + **Conventional**: $n_2 = 12$, $\\overline{x}_2 = 33.9$, $s_2 = 17.4$\n\n- The two populations are nearly normal. \n- The large difference in the sample SD suggests $\\sigma_1 \\ne \\sigma_2$. \n- Form a hypothesis test with $\\alpha = 0.05$ and construct a 95% CI for the mean difference of force on the elbow.\n\n:::\n\n::: {.column width=\"25%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/11-infer-two-means/tennis.jpeg){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n- Step 1: <span style=\"color:blue\"> $\\begin{align} &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 < \\mu_2 \\end{align}$ </span>\n\n. . .\n\n- Step 2: <span style=\"color:blue\"> $\\alpha = 0.05$ </span>\n\n\n\n## Example: Two-Sample t-Test Cont'd\n\n- Step 3: <span style=\"color:blue\"> $t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(25.2 - 33.9) - 0}{\\sqrt{\\frac{\\color{red}{8.6^2}}{33} + \\frac{\\color{red}{17.4^2}}{12}}} = -1.66$</span>\n\n. . .\n\n<!-- - $\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},$ $\\small A = \\dfrac{s_1^2}{n_1}$ and $\\small B = \\dfrac{s_2^2}{n_2}$ -->\n\n<!-- - $\\small A = \\dfrac{8.6^2}{33}$, $\\small B = \\dfrac{17.4^2}{12}$, $\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{33-1}+ \\dfrac{B^2}{12-1}} = 13.01$ -->\n\n<!-- - $\\small c = \\frac{\\frac{8.6^2}{33}}{\\frac{8.6^2}{33} + \\frac{17.4^2}{12}} = 0.0816$, $\\small df = \\frac{(33-1)(12-1)}{(1-0.0816)^2(33-1)+0.0816^2(12-1)} =13.01$ -->\n\n$\\small A = \\dfrac{8.6^2}{33}$, $\\small B = \\dfrac{17.4^2}{12}$, $\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{33-1}+ \\dfrac{B^2}{12-1}} = 13.01$\n\n::: alert\nIf the computed value of $df$ is not an integer, always **round down** to the nearest integer.\n:::\n\n. . .\n\n- Step 4-c:  <span style=\"color:blue\"> $-t_{0.05, 13} = -1.77$. </span>\n\n- Step 5-c:  <span style=\"color:blue\"> We reject $H_0$ if $\\small t_{test} < -t_{\\alpha, df}$. $\\small t_{test} = -1.66 > -1.77 = -t_{\\alpha, df}$, we fail to reject $H_0$. </span>\n\n- Step 6:  <span style=\"color:blue\"> **There is insufficient evidence to support the claim that the oversized racket delivers less stress to the elbow**. </span>\n\n\n## Example: Two-Sample t-Test Cont'd\n\n- The 95% CI for $\\mu_1 - \\mu_2$ is\n\n$$\\begin{align}(\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}} &= (25.2 - 33.9) \\pm t_{0.025,13}\\sqrt{\\frac{8.6^2}{33} + \\frac{17.4^2}{12}}\\\\&= -8.7 \\pm 11.32 = (-20.02, 2.62).\\end{align}$$ \n\n- We are 95% confident that the difference in the mean forces is between -20.02 and 2.62.\n\n- Since the interval includes 0, it leads to the same conclusion as failing to reject $H_0$.\n\n\n## Two-Sample t-Test in R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn1 = 33; x1_bar = 25.2; s1 = 8.6\nn2 = 12; x2_bar = 33.9; s2 = 17.4\nA <- s1^2 / n1; B <- s2^2 / n2\ndf <- (A + B)^2 / (A^2/(n1-1) + B^2/(n2-1))\n(df <- floor(df))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 13\n```\n\n\n:::\n\n```{.r .cell-code}\n## t_test\n(t_test <- (x1_bar - x2_bar) / sqrt(s1^2/n1 + s2^2/n2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.7\n```\n\n\n:::\n\n```{.r .cell-code}\n## t_cv\nqt(p = 0.05, df = df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.8\n```\n\n\n:::\n\n```{.r .cell-code}\n## p_value\npt(q = t_test, df = df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06\n```\n\n\n:::\n:::\n\n\n\n## Test Statistic for Independent Samples $(\\sigma_1 = \\sigma_2 = \\sigma)$\n\n- If $\\sigma_1$ and $\\sigma_2$ are **known**,\n\n$$z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} $$\n\n. . .\n\n- If $\\sigma_1$ and $\\sigma_2$ are **unknown**, similar to the one sample case, we use $t_{test}$. \n\n- As $\\sigma_1 = \\sigma_2 = \\sigma$, we don't need two but **one** sample SD to replace the $\\sigma$.\n\n- Use the **pooled sample variance** to estimate the common $\\sigma^2$:\n\n$$ s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2} $$ which is the *weighted average* of $s_1^2$ and $s_2^2$.\n\n\n::: notes\n- But not so similar to the equal-variance case.\n:::\n\n\n## Test Statistic for Independent Samples $(\\sigma_1 = \\sigma_2 = \\sigma)$\n\n- If $\\sigma_1$ and $\\sigma_2$ are **unknown**,\n$$t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{ {\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$$\n\n- Here, the **critical value** $t_{\\alpha, df}$ (for one-tailed tests) and $t_{\\alpha/2, df}$ (for two-tailed tests), and the $t$ distribution used to compute the **$p$-value** have the degrees of freedom\n$$df = n_1 + n_2 - 2$$\n\n\n\n## Inference from Independent Samples $(\\sigma_1 = \\sigma_2 = \\sigma)$\n\n$\\large \\color{red}{\\sigma_1 = \\sigma_2}$       | Test Statistic  | Confidence Interval for $\\mu_1 - \\mu_2$\n-------- | ------------- | ------------------\nknown   | $\\large z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$    |  $\\large (\\overline{x}_1 - \\overline{x}_2) \\pm z_{\\alpha/2} \\sigma \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}$\nunknown | $\\large t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$ | $\\large (\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} {\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}$\n\n- $s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}$\n\n- Use $df = n_1+n_2-2$ get the $p$-value, critical value and CI.\n\n- The test from two independent samples with $\\sigma_1 = \\sigma_2  = \\sigma$ is usually called **two-sample pooled $z$-test** or **two-sample pooled $t$-test**.\n\n\n## Example: Weight Loss\n\n:::: columns\n\n::: {.column width=\"80%\"}\n\nA study was conducted to see the effectiveness of a weight loss program.\n\n- Two groups (Control and Experimental) of 10 subjects were selected.\n\n- The two populations are normally distributed and have the same SD.\n\n:::\n\n::: {.column width=\"20%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/11-infer-two-means/weight.jpeg){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n- The data on weight loss was collected at the end of six months\n  + **Control**: $n_1 = 10$, $\\overline{x}_1 = 2.1\\, lb$, $s_1 = 0.5\\, lb$\n  + **Experimental**: $n_2 = 10$, $\\overline{x}_2 = 4.2\\, lb$, $s_2 = 0.7\\, lb$\n  \n- Is there a sufficient evidence at $\\alpha = 0.05$ to conclude that the program is effective?\n\n- If yes, construct a 95% CI for $\\mu_1 - \\mu_2$ to show how much effective it is. \n\n\n::: notes\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/11-infer-two-means/weight.jpeg){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n. . .\n\n- Step 1: <span style=\"color:blue\"> $\\begin{align} &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 < \\mu_2 \\end{align}$ </span>\n\n. . .\n\n- Step 2:  <span style=\"color:blue\"> $\\alpha = 0.05$ </span>\n\n\n## Example Cont'd \n\n- Step 3: \n<span style=\"color:blue\"> $t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$. </span>\n\n- $s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}} = \\sqrt{\\frac{(10-1)0.5^2 + (10-1)0.7^2}{10+10-2}}=0.61$\n\n- <span style=\"color:blue\"> $t_{test} = \\frac{(2.1 - 4.2) - 0}{0.6083\\sqrt{\\frac{1}{10} + \\frac{1}{10}}} = -7.72$</span>\n\n\n. . .\n\n- Step 4-c:  <span style=\"color:blue\"> $df = n_1 + n_2 - 2 = 10 + 10 - 2 = 18$. So $-t_{0.05, df = 18} = -1.734$. </span>\n\n. . .\n\n- Step 5-c:  <span style=\"color:blue\"> We reject $H_0$ if $\\small t_{test} < -t_{\\alpha, df}$. Since $\\small t_{test} = -7.72 < -1.73  = -t_{\\alpha, df}$, we reject $H_0$.</span>\n\n. . .\n\n- Step 4-p:  <span style=\"color:blue\"> The $p$-value is $P(T_{df=18} < t_{test}) \\approx 0$ </span>\n\n. . .\n\n- Step 5-p:  <span style=\"color:blue\"> We reject $H_0$ if $p$-value < $\\alpha$. Since $p$-value $\\approx 0 < 0.05  = \\alpha$, we reject $H_0$.</span>\n\n. . .\n\n- Step 6:  <span style=\"color:blue\"> **There is sufficient evidence to support the claim that the weight loss program is effective.** </span>\n\n## Example Cont'd \n\n- The 95% CI for $\\mu_1 - \\mu_2$ is $$\\begin{align}(\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} {\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}} &= (2.1 - 4.2) \\pm t_{0.025, 18} (0.61)\\sqrt{\\frac{1}{10} + \\frac{1}{10}}\\\\ &= -2.1 \\pm 0.57 = (-2.67, -1.53) \\end{align}$$\n\n- We are 95% confident that the difference in the mean weight loss is between -2.67 and -1.53.\n\n- Since the interval does not include 0, it leads to the same conclusion as rejection of $H_0$.\n\n\n## Two-Sample Pooled t-Test in R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn1 = 10; x1_bar = 2.1; s1 = 0.5\nn2 = 10; x2_bar = 4.2; s2 = 0.7\nsp <- sqrt(((n1 - 1) * s1 ^ 2 + (n2 - 1) * s2 ^ 2) / (n1 + n2 - 2))\nsp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.61\n```\n\n\n:::\n\n```{.r .cell-code}\ndf <- n1 + n2 - 2\n## t_test\n(t_test <- (x1_bar - x2_bar) / (sp * sqrt(1 / n1 + 1 / n2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -7.7\n```\n\n\n:::\n\n```{.r .cell-code}\n## t_cv\nqt(p = 0.05, df = df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.7\n```\n\n\n:::\n\n```{.r .cell-code}\n## p_value\npt(q = t_test, df = df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2e-07\n```\n\n\n:::\n:::\n\n\n\n\n",
    "supporting": [
      "11-infer-two-means_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}