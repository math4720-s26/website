{
  "hash": "6f6f26de958e1cb987adb9f741280067",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Regression `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 640 512\" style=\"height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M64 64a64 64 0 1 1 128 0A64 64 0 1 1 64 64zM25.9 233.4C29.3 191.9 64 160 105.6 160h44.8c27 0 51 13.4 65.5 34.1c-2.7 1.9-5.2 4-7.5 6.3l-64 64c-21.9 21.9-21.9 57.3 0 79.2L192 391.2V464c0 26.5-21.5 48-48 48H112c-26.5 0-48-21.5-48-48V348.3c-26.5-9.5-44.7-35.8-42.2-65.6l4.1-49.3zM448 64a64 64 0 1 1 128 0A64 64 0 1 1 448 64zM431.6 200.4c-2.3-2.3-4.9-4.4-7.5-6.3c14.5-20.7 38.6-34.1 65.5-34.1h44.8c41.6 0 76.3 31.9 79.7 73.4l4.1 49.3c2.5 29.8-15.7 56.1-42.2 65.6V464c0 26.5-21.5 48-48 48H496c-26.5 0-48-21.5-48-48V391.2l47.6-47.6c21.9-21.9 21.9-57.3 0-79.2l-64-64zM272 240v32h96V240c0-9.7 5.8-18.5 14.8-22.2s19.3-1.7 26.2 5.2l64 64c9.4 9.4 9.4 24.6 0 33.9l-64 64c-6.9 6.9-17.2 8.9-26.2 5.2s-14.8-12.5-14.8-22.2V336H272v32c0 9.7-5.8 18.5-14.8 22.2s-19.3 1.7-26.2-5.2l-64-64c-9.4-9.4-9.4-24.6 0-33.9l64-64c6.9-6.9 17.2-8.9 26.2-5.2s14.8 12.5 14.8 22.2z\"/></svg>`{=html}'\ntitle-slide-attributes:\n  data-background-image: ../images/bg.png\n  # data-background-size: stretch\n  # data-slide-number: none\nformat: \n  live-revealjs: \n    output-file: 15-regression-slides.html\n    # theme: slides.scss\n    code-line-numbers: true\nwebr:\n  cell-options:\n    autorun: false\n  packages:\n    - tidyverse\nknitr:\n  opts_chunk:\n    out-width: 100%\n    echo: false\n---\n\n\n# {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n<!-- begin: webr fodder -->\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n<!-- end: webr fodder -->\n\n\n<!-- # [Hypothesis Testing]{.orange}{background-image=\"https://upload.wikimedia.org/wikipedia/commons/4/42/William_Sealy_Gosset.jpg\" background-size=\"cover\" background-position=\"50% 50%\" background-color=\"#447099\"} -->\n\n\n# [Regression]{.orange}{background-image=\"./images/15-regression/correlation_diagram_1875.jpg\" background-size=\"cover\" background-position=\"50% 50%\" background-color=\"#447099\"}\n\n\n## What is Regression\n\n- **Regression** models the relationship between one or more **numerical/categorical response variables $(Y)$** and one or more **numerical/categorical explanatory variables $(X)$**.\n\n- A **regression function** $f(X)$ describes how a response variable $Y$, on average, changes as an explanatory variable $X$ changes.\n\n:::: columns\n\n::: {.column width=\"50%\"}\n\nExamples:\n\n- <span style=\"color:blue\"> college GPA $(Y)$ vs. ACT/SAT score $(X)$</span>\n\n- <span style=\"color:blue\"> sales $(Y)$ vs. advertising expenditure $(X)$</span>\n\n- <span style=\"color:blue\"> crime rate $(Y)$ vs. median income level $(X)$ </span>\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-regression_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n\n::: notes\n\n- **Regression** models the relationship between a numerical **response variable** (dependent variable) and one or more (numerical/categorical) **explanatory variables** (independent variables, predictors, or covariates).\n- A **regression function** describes how a response variable $Y$ changes as an explanatory variable $X$ changes.\n- The true relationship between $X$ and $Y$, the regression function, is **unknown**.\n- The goal of regression is to estimate the regression function and use it to **predict** value of $Y$ given a value of $X$.\n- Examples:\n  + <span style=\"color:blue\"> Relationship between college GPA ($Y$) and ACT/SAT scores ($X$)</span>\n  + <span style=\"color:blue\"> Relationship between sales ($Y$) and advertising expenditure ($X$)</span>\n  + <span style=\"color:blue\"> Relationship between crime rate ($Y$) and median income level ($X$) </span>\n  \n:::\n  \n  \n## Unknown Regression Function\n\n- The true relationship between $X$ and the mean of $Y$, the regression function $f(X)$, is **unknown**.\n\n- The collected data $(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$ is all we know and have.\n\n- Goal: **estimate** $f(X)$ from our data and use it to **predict** value of $Y$ given any value of $X$.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-regression_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Simple Linear Regression\n\n- Start with **simple linear regression**: \n  + **Only one** predictor $X$ (**known** and **constant**) and **one** response variable $Y$\n  + the regression function used for predicting $Y$ is a **linear** function. \n  + use a *regression line* in a X-Y plane to predict the value of $Y$ for a given value of $X = x$.\n  \n<!-- - Suppose that $Y$ is a response variable (plotted on the vertical axis) and $X$ is an explanatory variable (plotted on the horizontal axis). A straight line relating $Y$ to $X$ has an equation of the form -->\n<!-- $$ Y = b_0 + b_1X$$ -->\n\n. . .\n\n\n:::: columns\n\n::: {.column width=\"50%\"}\n\n**Math review**: A linear function $y = f(x) = \\beta_0 + \\beta_1 x$ represents a straight line\n\n  + $\\beta_1$: **slope**, the amount by which $y$ changes when $x$ increases by one unit.\n  \n  + $\\beta_0$: **intercept**, the value of $y$ when $x = 0$.\n  \n  + The linearity assumption: $\\beta_1$ does not change as $x$ changes.\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-regression/reg_line.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n::: notes\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-regression_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n\n\n## Sample Data: Relationship Between X and Y\n\n- Real data $(x_i, y_i), i = 1, 2, \\dots, n$ do not form a perfect straight line!\n\n- $y_i = \\beta_0+\\beta_1x_i + \\color{red}{\\epsilon_i}$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-regression_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n##\n\n- When we collect our data, at any given level of $X = x$, $y$ is assumed being drawn from a normal distribution (for inference purpose).\n\n- Its value varies around and will not be exactly equal to its mean $\\mu_y$.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-regression/regression_line_data.png){fig-align='center' width=100%}\n:::\n:::\n\n\n##\n\n- When we collect our data, at any given level of $X = x$, $y$ is assumed being drawn from a normal distribution (for inference purpose).\n\n- Its value varies around and will not be exactly equal to its mean $\\mu_y$.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-regression/regression_line_data_blue.png){fig-align='center' width=100%}\n:::\n:::\n\n\n##\n\n- The **mean of $Y$** and $X$ form a straight line.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-regression/regression_line.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n## Simple Linear Regression Model (Population)\n\nFor the $i$-th measurement in the target population,\n\n$$Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i$$\n\n- $Y_i$: the $i$-th value of the response (random) variable.\n\n- $X_i$: the $i$-th **known fixed** value of the predictor.\n\n- $\\epsilon_i$: the $i$-th random error with assumption $\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)$.\n\n- $\\beta_0$ and $\\beta_1$ are model **coefficients**.\n\n- $\\beta_0$, $\\beta_1$ and $\\sigma^2$ are **fixed unknown parameters** to be estimated from the sample data once we collect them.\n\n::: notes\n\n.question[\nWhat is the distribution of $Y_i$ given a value of $X = x$?\n\n:::\n\n## Important Features of Model $Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i$\n\n\n:::: columns\n\n::: {.column width=\"40%\"}\n\n$\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)$\n<!-- \\begin{align*} -->\n<!-- \\mu_{Y_i \\mid X_i} &= E(\\beta_0 + \\beta_1X_i + \\epsilon_i) \\\\ -->\n<!-- &= \\beta_0 + \\beta_1X_i -->\n<!-- \\end{align*} -->\n\n\\begin{align*}\n\\mu_{Y_i \\mid X_i} = \\beta_0 + \\beta_1X_i\n\\end{align*}\n\n\nThe **mean response** $\\mu_{Y\\mid X}$ has a **straight-line** relationship with $X$ given by a population regression line\n  $$\\mu_{Y\\mid X} = \\beta_0 + \\beta_1X$$\n\n:::\n\n::: {.column width=\"60%\"}\n\n\n<!-- - The mean of $Y$ is a linear function of $X$ -->\n<!-- - The variance of $Y$ does not depend on $X$. -->\n<!-- - If the simple linear model is appropriate, then we need to estimate the values $\\beta_0$, $\\beta_1$ and $\\sigma^2$. -->\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-regression/regression_line_red.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n## Important Features of Model $Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i$\n\n:::: columns\n\n::: {.column width=\"40%\"}\n\n$\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)$\n\\begin{align*}\n\\var(Y_i \\mid X_i) &= \\var(\\epsilon_i) = \\sigma^2\n\\end{align*}\nThe variance of $Y$ does not depend on $X$.\n\n:::\n\n\n<!-- - The mean of $Y$ is a linear function of $X$ -->\n<!-- - The variance of $Y$ does not depend on $X$. -->\n<!-- - If the simple linear model is appropriate, then we need to estimate the values $\\beta_0$, $\\beta_1$ and $\\sigma^2$. -->\n\n\n::: {.column width=\"60%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-regression/regression_line_sig.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n::: notes\n- The variation of Y is the same no matter what value of x is.\n:::\n\n\n\n## Important Features of Model $Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i$\n\n:::: columns\n\n::: {.column width=\"40%\"}\n\n$\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)$\n\\begin{align*}\nY_i \\mid X_i \\stackrel{indep}{\\sim} N(\\beta_0 + \\beta_1X_i, \\sigma^2)\n\\end{align*}\nFor any fixed value of $X_i = x_i$, the response $Y_i$ varies with $N(\\mu_{Y_i\\mid x_i}, \\sigma^2)$.\n\n:::\n\n\n<!-- - The mean of $Y$ is a linear function of $X$ -->\n<!-- - The variance of $Y$ does not depend on $X$. -->\n<!-- - If the simple linear model is appropriate, then we need to estimate the values $\\beta_0$, $\\beta_1$ and $\\sigma^2$. -->\n\n::: {.column width=\"60%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-regression/regression_line_sig_red.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n:::\n\n\n. . .\n\n- **Job**: Collect data and estimate the unknown parameters $\\beta_0$, $\\beta_1$ and $\\sigma^2$!\n\n\n## Idea of Fitting\n\n- Interested in $\\beta_0$ and $\\beta_1$ in the following *sample* regression model:\n\\begin{align*}\ny_i = \\beta_0 + \\beta_1~x_{i} + \\epsilon_i,\n\\end{align*}\n<!-- or -->\n<!-- $$E({y}_{i}) = \\mu_{y|x_i} = \\beta_0 + \\beta_1~x_{i}$$ -->\n\n\n::: notes\n\n- OK. once we collect the data, we have a sample regression model.\n- Here I use small x and y to represent the collected data.\n- Given this model, we're interested in $\\beta_0$ (population parameter for the intercept) and $\\beta_1$ (population parameter for the slope) because once we know $\\beta_0$ and $\\beta_1$, we know the exact shape of $f$ and we know the relationship of $y$ and $x$, and given any value of $x$, we can predict its corresponding value of $y$ using the regression line $\\hat{y}_{i} = \\beta_0 + \\beta_1~x_{i}$.\n- But again the population parameters are unknown to us.\n- $\\hat{y}_i = E(Y|X_i=x_i)$\n\n:::\n\n\n. . .\n\n- Use sample statistics $b_0$ and $b_1$ computed from our sample data to estimate $\\beta_0$ and $\\beta_1$.\n\n- $\\hat{y}_{i} = b_0 + b_1~x_{i}$ is called **fitted value** of $y_i$, a point estimate of the mean $\\mu_{y|x_i}$ and $y_i$ itself.\n\n\n::: notes\n\n- $b_0$: intercept of the sample regression line\n- $b_1$: slope of  the sample regression line\n\n:::\n\n## Fitting a Regression Line $\\hat{Y} = b_0 + b_1X$\n\nGiven the sample data $\\{ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\},$\n\n- Which *sample* regression line is the **best**? \n\n- What are the **best** estimators $b_0$ and $b_1$ for $\\beta_0$ and $\\beta_1$?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-regression_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n::: notes\n\n- Now suppose we already get our sample, and now we are trying to use the sample to get a sample regression line, and hopefully, the sample regression line and the population regression line are alike. look similarly.\n- So people usually ask\n- Which *sample* regression line is the **best**? \n- What are the **best** estimators $b_0$ and $b_1$ for $\\beta_0$ and $\\beta_1$?\n- After all, given the data, we can generate so many different straight lines, and we need a criterion to help us determine which line is the best in some sense. Right!\n\n:::\n\n## Fitting a Regression Line $\\hat{Y} = b_0 + b_1X$\n\nGiven the sample data $\\{ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\},$\n\n- Which *sample* regression line is the **best**? \n\n- What are the **best** estimators $b_0$ and $b_1$ for $\\beta_0$ and $\\beta_1$?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-regression_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## What does \"best\" mean? Ordinary Least Squares (OLS)\n\n- Choose $b_0$ and $b_1$, or sample regression line $b_0 + b_1x$ that minimizes the **sum of squared residuals $SS_{res}$**\n\n$$SS_{res} = e_1^2 + e_2^2 + \\dots + e_n^2 = \\sum_{i = 1}^n e_i^2,$$ where the **residual** $e_i = y_i - \\hat{y}_i = y_i - (b_0 + b_1x_i)$.\n\n<!-- is a point estimate of $\\epsilon_i$. -->\n<!-- - The sample regression line minimizes $SS_{res} = e_1^2 + e_2^2 + \\dots + e_n^2 = \\sum_{i = 1}^n e_i^2$. -->\n\n. . .\n\n- If $b_0$ and $b_1$ are the best estimators,\n \n$$\\small{\\begin{align} SS_{res} &= (y_1 - b_0 - b_1x_1)^2 + (y_2 - b_0 - b_1x_2)^2 + \\dots + (y_n - b_0 - b_1x_n)^2\\\\ &= \\sum_{i=1}^n(y_i - b_0 - b_1x_i)^2 \\end{align}}$$ that is the smallest comparing to any other $SS_{res} = \\sum_{i=1}^n(y_i - a_0 - a_1x_i)^2$ that uses another pair of values $(a_0, a_1) \\ne (b_0, b_1)$.\n\n\n::: notes\n\n- Now the question is How do we get $b_0$ and $b_1$ that well estimate $\\beta_0$ and $\\beta_1$?\n- We choose $b_0$ and $b_1$, or regression line $b_0 + b_1x$ that minimizes the **sum of squared residuals**.\n- If we define residual as $e_i = y_i - \\hat{y}_i$, then the sum of squared residuals is $\\sum_{i = 1}^n e_i^2$.\n- And this approach that estimates the population parameters $\\beta_0$ and $\\beta_1$ or the population regression line is called Ordinary Least Squares method.\n\n:::\n\n\n## Visualizing Residuals\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-regression_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n::: notes\nThat's see the idea of Ordinary Least Squares visually. Here just showed the data.\n- Later we will work on the data set together.\n<!-- - Do you see why some points are darker than some others? -->\n<!-- - A darker point means that there are several identical (x, y) pairs, or replicates in the data set. -->\n\n:::\n\n\n\n## Visualizing Residuals (cont.)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-regression_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: notes\n\n- All right, with the data, this figure also shows the least squares regression line, and the fitted value of $y$ for each $x$ in the training data, which are those red points.\n- The fitted values of y are right on the regression line.\n- Now the question is, how do we find this line?\n- Given a line, we can have predicted values of y, right?\n- Then what is residual on the plot? The residual will be the difference between the true observation y and the fitted value of y given any value of x.\n- So a residual in the plot will be a vertical bar at the value of x with two ends of the bar $y$ and $\\hat{y}$, right?\n- (Show on board)\n- (add $y_i = b_0+b_1x_i$ and residual line)\n\n:::\n\n\n\n## Visualizing Residuals (cont.)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-regression_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: notes\n\n- Here shows all the residuals in vertical bars.\n- least squares line is the line such that the sum of all the squared residuals is minimized.\n- Why we square the residuals?\n- It's mathematically more convenient.\n- Squaring emphasizes larger differences\n\n:::\n\n\n## Least Squares Estimates (LSE)\n\n- The least squares approach choose $b_0$ and $b_1$ to minimize the $SS_{res}$, i.e.,\n\n$$(b_0, b_1) = \\arg \\min_{\\alpha_0, \\alpha_1} \\sum_{i=1}^n(y_i - \\alpha_0 - \\alpha_1x_i)^2$$\n<!-- - Take derivative with respect to $\\beta_0$ and $\\beta_1$, setting both equal to zero: -->\n<!-- $$\\left.\\frac{\\partial SS_{res}}{\\partial\\beta_0}\\right\\vert_{b_0, b_1} = \\left.\\sum_{i=1}^n\\frac{\\partial (y_i - \\beta_0 - \\beta_1x_i)^2}{\\partial\\beta_0}\\right\\vert_{b_0, b_1} = 0$$ -->\n<!-- $$\\left. \\frac{\\partial SS_{res}}{\\partial\\beta_1}\\right\\vert_{b_0, b_1} = \\left.\\sum_{i=1}^n\\frac{\\partial (y_i - \\beta_0 - \\beta_1x_i)^2}{\\partial\\beta_1}\\right\\vert_{b_0, b_1} = 0$$ -->\n\n<!-- The two equations are called the **normal equations**. -->\n\n. . .\n\nMATH 1450/1455 ... \n\n$$\\color{red}{b_0 = \\overline{y} - b_1\\overline{x}}$$\n<!-- - Solve for $\\beta_1$ given $\\color{red}{b_0 = \\overline{y} - b_1\\overline{x}}$: -->\n<!-- $$\\sum_{i=1}^nx_iy_i - (\\overline{y} - b_1\\overline{x})n\\overline{x} - b_1\\sum_{i=1}^nx_i^2 = 0.$$ Then-->\n\n$$\\color{red}{b_1 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^n(x_i - \\overline{x})^2} = r \\frac{\\sqrt{S_{yy}}}{\\sqrt{S_{xx}}}},$$ where $S_{xx} = \\sum_{i=1}^n(x_i - \\overline{x})^2$ and $S_{yy} = \\sum_{i=1}^n(y_i - \\overline{y})^2$\n\n\n. . .\n\n::: question\nWhat can we learn from the formula of $b_0$ and $b_1$?\n:::\n\n\n::: notes\n\n- The LS regression line passes through the centroid.\n- $b_1$ is kinda like a scaled covariance of X and Y.\n\n:::\n\n\n<!-- ## Population Model vs. Estimated Model -->\n\n<!-- {{< include model-reg-ojs.qmd >}} -->\n\n\n\n\n\n\n## [R Lab]{.pink}: `mpg` Data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlibrary(ggplot2)  ## use data mpg in ggplot2 package\nmpg\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` my_class600\n# A tibble: 234 √ó 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n 1 audi         a4           1.8  1999     4 auto‚Ä¶ f        18    29 p     comp‚Ä¶\n 2 audi         a4           1.8  1999     4 manu‚Ä¶ f        21    29 p     comp‚Ä¶\n 3 audi         a4           2    2008     4 manu‚Ä¶ f        20    31 p     comp‚Ä¶\n 4 audi         a4           2    2008     4 auto‚Ä¶ f        21    30 p     comp‚Ä¶\n 5 audi         a4           2.8  1999     6 auto‚Ä¶ f        16    26 p     comp‚Ä¶\n 6 audi         a4           2.8  1999     6 manu‚Ä¶ f        18    26 p     comp‚Ä¶\n 7 audi         a4           3.1  2008     6 auto‚Ä¶ f        18    27 p     comp‚Ä¶\n 8 audi         a4 quattro   1.8  1999     4 manu‚Ä¶ 4        18    26 p     comp‚Ä¶\n 9 audi         a4 quattro   1.8  1999     4 auto‚Ä¶ 4        16    25 p     comp‚Ä¶\n10 audi         a4 quattro   2    2008     4 manu‚Ä¶ 4        20    28 p     comp‚Ä¶\n# ‚Ñπ 224 more rows\n```\n\n\n:::\n:::\n\n\n\n## [R Lab]{.pink}: Highway MPG `hwy` vs. Displacement `displ`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nplot(x = mpg$displ, y = mpg$hwy,\n     las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-revealjs/unnamed-chunk-20-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n::: notes\n\n- Usually, when we get data, the first thing is plotting your data, doing some exploratory data analysis, and see if there is useful information out there that may help us build an appropriate model.\n- To make a scatter plot, we can simply use the plot() function, and put displ in x axis and hwy in the y axis.\n- To grab a variable or a column of a data frame, we can use the dollar sign, the same way as a list extracting an element.\n- The rest of arguments are optional, they are just used to decorate your plot. You can generate a plot without specifying any of them.\n- And because it seems to a linear trend downwards. We could fit a simple linear regression to the data. Right\n\n:::\n\n\n## [R Lab]{.pink}: Fit Simple Linear Regression\n\n::: midi\n\n:::: columns\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nreg_fit <- lm(formula = hwy ~ displ, \n              data = mpg)\nreg_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nCoefficients:\n(Intercept)        displ  \n      35.70        -3.53  \n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"false\"}\ntypeof(reg_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"list\"\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"60%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n## all elements in reg_fit\nnames(reg_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n## use $ to extract an element of a list\nreg_fit$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)       displ \n      35.70       -3.53 \n```\n\n\n:::\n:::\n\n\n:::\n\n::::\n\n:::\n\n\n. . .\n\n- $\\widehat{hwy}_{i} = b_0 + b_1 \\times displ_{i} =  35.7 - 3.5 \\times displ_{i}$\n\n- $b_1$: For one unit (litre) increase of the displacement, we expect the highway MPG to be decreased, on average, by 3.5.\n\n\n\n::: notes\n\n- In R, to fit a linear regression model, it cannot be easier.\n- We just need to use the command lm(). We put a formula in the function, y ~ x, and let R know which data set you are considering.\n- That's it. And I save the fitted result in an object called reg_fit.\n- You can see lm() function returns a list.\n- We can grab the coefficient estimates this way.\n\n- So your sample regression line is like this.\n\n:::\n\n\n## [R Lab]{.pink} Fitted Values of $y$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n## the first 5 observed response value y\nmpg$hwy[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 29 29 31 30 26\n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n## the first 5 fitted value y_hat\nhead(reg_fit$fitted.values, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   1    2    3    4    5 \n29.3 29.3 28.6 28.6 25.8 \n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n## the first 5 predictor value x\nmpg$displ[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.8 1.8 2.0 2.0 2.8\n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlength(reg_fit$fitted.values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 234\n```\n\n\n:::\n:::\n\n\n## [R Lab]{.pink} Add a Regression Line\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nplot(x = mpg$displ, y = mpg$hwy, las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")\nabline(reg_fit, col = \"#FFCC00\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-revealjs/unnamed-chunk-24-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n## Estimation for $\\sigma^2$\n\n\n:::: columns\n\n::: {.column width=\"50%\"}\n\n- Think of $\\sigma^2$ as **variance around the line** or the **mean square (prediction) error**.\n\n- The estimate of $\\sigma^2$ is the **mean square residual** $MS_{res}$:\n\n$$\\hat{\\sigma}^2 = MS_{res} = \\frac{SS_{res}}{n-2} = \\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}$$\n\n- $MS_{res}$ is often shown in computer output as $\\texttt{MS(Error)}$ or $\\texttt{MS(Residual)}$.\n<!-- - $E(MS_{res}) = \\sigma^2$, i.e., $\\hat{\\sigma}^2$ is an unbiased estimator for $\\sigma^2$. üëç -->\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-regression/regression_line_data.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n::: notes\n\n- So we are done estimation for $beta$. Let's talk about the estimation of $\\sigma^2$.\n- The estimate of $\\sigma^2$, denoted as $\\hat{\\sigma}^2$ or $s_{\\epsilon}^2$, based on the sample data is residual sum of squares divided by $n-2$, the degrees of freedom\n- It can be shown that $E(SS_{res}) = (n-2)\\sigma^2$. That is, $\\hat{\\sigma}^2$ is an *unbiased* estimator for $\\sigma^2$. üëç\n\n:::\n\n\n## [R Lab]{.pink} Standard Error of Regression\n\n\n::: {.cell layout-align=\"center\" highlight.output='16'}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n(summ_reg_fit <- summary(reg_fit))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` my_classfull\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.104 -2.165 -0.224  2.059 15.010 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   35.698      0.720    49.5   <2e-16 ***\ndispl         -3.531      0.195   -18.1   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.84 on 232 degrees of freedom\nMultiple R-squared:  0.587,\tAdjusted R-squared:  0.585 \nF-statistic:  329 on 1 and 232 DF,  p-value: <2e-16\n```\n\n\n:::\n:::\n\n\n\n::: notes\n\n- How do we get sigma_hat in R?\n- Well you could grab residuals and df from lm fit result reg_fit, and use the formula to calculate the sigma_hat. sqrt(sum(reg_fit$residuals^2) / reg_fit$df.residual)\n- If you want R to do the calculation for you, you can get the summary of the fitted result reg_fit.\n- Then the sigma hat is right here.\n\n:::\n\n\n## [R Lab]{.pink} Standard Error of Regression\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# lots of fitted information saved in summary(reg_fit)!\nnames(summ_reg_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# residual standard error (sigma_hat)\nsumm_reg_fit$sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.84\n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# from reg_fit\nsqrt(sum(reg_fit$residuals^2) / reg_fit$df.residual)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.84\n```\n\n\n:::\n:::\n\n\n::: notes\n\n- Here shows the fitted information saved in summary(reg_fit)\n- You see sigma is right here. So you just extract that value if you need.\n\n:::\n\n\n\n## Confidence Interval for $\\beta_1$\n\n- $\\frac{b_1 - \\beta_1}{\\sqrt{\\hat{\\sigma}^2/S_{xx}}} \\sim t_{n-2}$\n\n. . .\n\n- $(1-\\alpha)100\\%$ CI for $\\beta_1$ is $b_1 \\pm t_{\\alpha/2, n-2}\\sqrt{\\hat{\\sigma}^2/S_{xx}}$\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nconfint(reg_fit, level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            2.5 % 97.5 %\n(Intercept) 34.28  37.12\ndispl       -3.91  -3.15\n```\n\n\n:::\n:::\n\n\n<!-- ## Sampling Distribution of $\\beta_0$ and $\\beta_1$ -->\n\n<!-- {{< include model-reg-beta-ojs.qmd >}} -->\n\n\n## Hypothesis Testing: $\\beta_1$\n\n- <span style=\"color:blue\"> $H_0: \\beta_1 = \\beta_1^0 \\quad H_1: \\beta_1 \\ne \\beta_1^0$  </span>\n<!-- - standard error of $b_1$: $se(b_1) = \\sqrt{\\frac{MS_{res}}{S_{xx}}}$ -->\n\n- Test statistic: Under $H_0$,\n\n$$t_{test} = \\frac{b_1 - \\color{red}{\\beta_1^0}}{\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}} \\sim t_{n-2}$$ \n\n- Reject $H_0$ in favor of $H_1$ if \n  + $|t_{test}| > t_{\\alpha/2, \\, n-2}$\n  + $\\text{p-value} = 2P(t_{n-2} > |t_{test}|) < \\alpha$\n\n\n::: notes\n\n- in addition to estimation, we may be interested in testing.\n- To do the testing on $\\beta_1$, the testing procedure is basically the same as the procedure for population mean $\\mu$ we reviewed last week.\n- Usually $\\beta_1^0 = 0$, but we may be interested in other values.\n- good growth $\\beta_1 > 2$\n\n:::\n\n\n## [R Lab]{.pink} Testing on $\\beta_0$ and $\\beta_1$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nsumm_reg_fit$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Estimate Std. Error t value  Pr(>|t|)\n(Intercept)    35.70      0.720    49.6 2.12e-125\ndispl          -3.53      0.195   -18.2  2.04e-46\n```\n\n\n:::\n:::\n\n\n- Testing $H_0: \\beta_0 = 0$ and $H_0: \\beta_1 = 0$\n\n\n\n## Interpretation of Testing Results\n\n- <span style=\"color:blue\"> $H_0: \\beta_1 = 0 \\quad H_1: \\beta_1 \\ne 0$ </span>\n\n- *Failing to reject $H_0: \\beta_1 = 0$* implies there is **no linear relationship** between $Y$ and $X$.\n\n\n::: notes\n\n- So back to the testing on $\\beta_1$.\n- If we do not reject $H_0$, it implies there is **no linear relationship** between $Y$ and $X$. Right? Because $\\beta_1$, the slope of the regression line is pretty much 0. \n\n:::\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-regression_files/figure-revealjs/unnamed-chunk-30-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n::: notes\n\n- But it actually has two cases. One $x$ and $y$ may have no relationship at all, or they don't have linear relationship but have another kind of relationship, like quadratic.\n- So we have to be careful when we interpret the testing result. OK.\n\n:::\n\n\n. . .\n\n::: question\n\nIf we reject $H_0: \\beta_1 = 0$, does it mean $X$ and $Y$ are linearly related?\n\n:::\n\n\n::: notes\n\nWe have to be more careful and precise on what we are claiming.\n\n:::\n\n\n## Test of Significance of Regression\n\n- *Rejecting $H_0: \\beta_1 = 0$* could mean\n  + the straight-line model is adequate\n  + better results could be obtained with a more complicated model\n  \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-regression_files/figure-revealjs/unnamed-chunk-31-1.png){fig-align='center' width=56%}\n:::\n:::\n\n\n\n::: notes\n\n- *Rejecting $H_0: \\beta_1 = 0$* could mean \n  + the straight-line model is adequate\n  + better results could be obtained with a more complicated model even though there is a linear effect of $x$\n\n:::\n\n\n# Analysis of Variance (ANOVA) Approach\n\n\n::: notes\n\n- Let's talk about ANOVA, the idea of partitioning the total variability.\n\n:::\n\n\n## $X$ - $Y$ Relationship Explains Some Deviation\n\n::: question\nSuppose we only have data $Y$ and have no information about $X$ and relationship between $X$ and $Y$. How do we predict a value of $Y$?\n:::\n\n\n::: notes\n- For example, suppose we only have MPG information for the sample of cars and have no info about displacement and their relationship. How do we predict a car's MPG?\n:::\n\n\n. . .\n\n- Our best guess would be $\\overline{y}$ if the data have no pattern, i.e., $\\hat{y}_i = \\overline{y}$.\n\n- Treat $X$ and $Y$ as uncorrelated.\n\n- The (total) deviation from the mean is $(y_i - \\overline{y})$\n\n. . .\n\n- If $X$ and $Y$ are linearly related, fitting a linear regression model helps us predict the value of $Y$ when the value of $X$ is provided.\n\n- $\\hat{y}_i = b_0 + b_1x_i$ is closer to $y_i$ than $\\overline{y}$.\n\n- The regression model explains some deviation of $y$.\n\n\n\n::: notes\n\n- When we have no information about the relationship between $X$ and $Y$, to predict a value of $y$ using the same value given any value of $x$.\n- When $X$ and $Y$ are uncorrelated, the regression model is not helping predict $Y$ because $X$ provides no information about $Y$. \n- It means $b_1 = 0$ and $\\hat{y}_i = \\bar{y}$ for all values of $X$.\n- The result is the same as the one when we only have data of $Y$.\n- This prediction deviation $(y_i - \\overline{y})$ is generally the biggest deviation we can have when we have no information about how y varies or how y is affected by others.\n\n:::\n\n\n## Partition of Deviation\n\n\n- **Total deviation = Deviation explained by regression + Unexplained deviation**\n\n- $(y_i - \\overline{y}) = (\\hat{y}_i - \\overline{y}) + (y_i - \\hat{y}_i)$\n\n- $(19 - 9) = (13 - 9) + (19 - 13)$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-regression/partition.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n::: notes\n\n- If $X$ and $Y$ are actually linearly related, fitting a linear regression helps us predict the value of $Y$ when the value of $X$ is provided. \n- It means that $\\hat{y}_i = b_0 + b_1x_i$ is closer to $y_i$ than $\\overline{y}$.\n- Intuitively, the regression model explains some deviation of $y$ by the predictor $X$.\n- $X$ does contain some useful and valuable information for predicting value of $Y$.\n\n- So, we can actually Partition the total deviation into two parts.\n- Partition of deviation: **Total deviation = Explained deviation by regression + Unexplained deviation**\n- $(y_i - \\overline{y}) = (\\hat{y}_i - \\overline{y}) + (y_i - \\hat{y}_i)$\n\n:::\n\n\n## Sum of Squares (SS)\n\n- $\\sum_{i=1}^n(y_i - \\overline{y})^2 = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 + \\sum_{i=1}^n(y_i - \\hat{y}_i)^2$\n\n- **Total SS $(SS_T)$ = Regression SS $(SS_R)$ + Residual SS $(SS_{res})$**\n\n- $df_T = df_R + df_{res}$\n\n- $\\color{blue}{(n-1) = 1 +(n-2)}$\n\n<!-- -- -->\n\n<!-- + <span style=\"color:blue\"> $df_T = n - 1$</span>: lose 1 df with constraint $\\sum_{i=1}^n(y_i - \\overline{y}) = 0$ -->\n<!-- + <span style=\"color:blue\"> $df_R = 1$</span>: all $\\hat{y}_i$ are on the regression line with 2 dfs (intercept and slope), but with constraint $\\sum_{i=1}^n(\\hat{y}_i - \\overline{y}) = 0$ -->\n<!-- + <span style=\"color:blue\"> $df_{res} = n - 2$</span>: lose 2 dfs because $\\beta_0$ and $\\beta_1$ are estimated by $b_0$ and $b_1$, which are linear combo of $y_i$ -->\n\n::: notes\n\n- Total variability = variability explained by regression + unexplained variability\n- $\\sum_{i=1}^n(y_i - \\overline{y})^2 = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 + \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 + 2\\sum_{i=1}^n(\\hat{y}_i - \\overline{y})(y_i - \\hat{y}_i)$\n- \n+ <span style=\"color:blue\"> $df_T = n - 1$</span>: lose 1 df with constraint $\\sum_{i=1}^n(y_i - \\overline{y}) = 0$\n+ <span style=\"color:blue\"> $df_R = 1$</span>: all $\\hat{y}_i$ are on the regression line with 2 dfs (intercept and slope), but with constraint $\\sum_{i=1}^n(\\hat{y}_i - \\overline{y}) = 0$\n+ <span style=\"color:blue\"> $df_{res} = n - 2$</span>: lose 2 dfs because $\\beta_0$ and $\\beta_1$ are estimated by $b_0$ and $b_1$, which are linear combo of $y_i$\n- degrees of freedom is the equivalent number of values in the calculation of a statistic that are free to vary.\n- $SS_R = b_1S_{xy}$ or $SS_{res} = SS_T - b_1S_{xy}$.\n\n:::\n\n\n\n## ANOVA for Testing Significance of Regression\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-regression/anova_table.png){fig-align='center' width=100%}\n:::\n:::\n\n\n- A larger value of $F_{test}$ indicates that regression is significant.\n\n- Reject $H_0$ if \n  + $F_{test} > F_{\\alpha, 1, n-2}$\n  + $\\text{p-value} = P(F_{1, n-2} > F_{test}) < \\alpha$.\n  \n- The ANOVA is designed to test $H_0$ that **all** predictors have no value in predicting $y$.\n\n- In SLR, the $F$-test of ANOVA gives the same result as a two-sided $t$-test of $H_0: \\beta_1=0$. \n\n\n::: notes\n\n- ANOVA is used for testing significance of regression. \n- It is testing if any predictors or regressors have explanatory power for predicting y.\n- In other words, it is testing if the whole regression model is useful or not.\n- Here is the ANOVA table.\n- $H_0: \\beta_1 = 0$\n- A larger value of $F_{test}$ indicates that regression is significant.\n- Reject $H_0$ in favor of $H_1$ if $F_{test} > F_{\\alpha, 1, n-2}$ or $\\text{p-value} = P(F_{1, n-2} > F_{test}) < \\alpha$.\n- The ANOVA is designed to test $H_0$ that **all** predictors have no value in predicting $y$. \n- In SLR, there is only one predictor, and hence the $F$-test of ANOVA gives the same result as a two-sided $t$-test of $H_0: \\beta_1=0$. \n\n\n<!-- -- -->\n\n<!-- .question[ -->\n<!-- What is the $H_0$ of ANOVA $F$-test if there are $k \\ge 2$ predictors? -->\n<!-- ] -->\n\n- If we have $k \\ge 2$ predictors, the $F$-test is testing  $H_0: \\beta_1=\\beta_2=\\cdots=\\beta_k=0$. \n\n:::\n\n\n\n## [R Lab]{.pink} ANOVA Table\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nanova(reg_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: hwy\n           Df Sum Sq Mean Sq F value Pr(>F)    \ndispl       1   4848    4848     329 <2e-16 ***\nResiduals 232   3414      15                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-regression/anova_table.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: notes\n\n- For $H_0: \\beta_1 = 0$ in SLR, $t_{test}^2 = F_{test}$.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n            Estimate Std. Error t value  Pr(>|t|)\n(Intercept)    35.70      0.720    49.6 2.12e-125\ndispl          -3.53      0.195   -18.2  2.04e-46\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 329\n```\n\n\n:::\n:::\n\n- For $H_0: \\beta_1 = 0$,\n$t_{test}^2 = \\left(\\frac{b_1}{\\sqrt{MS_{res}/S_{xx}}}\\right)^2 = \\frac{b_1^2S_{xx}}{MS_{res}} = \\frac{b_1S_{xy}}{MS_{res}} = \\frac{MS_R}{MS_{res}} = F_{test}$\n\n:::\n\n\n\n\n## Coefficient of Determination\n\n- The **coefficient of determination** $(R^2)$ is the proportion of the variation in $y$ that is explained by the regression model: \n\n$$R^2 = \\frac{SS_R}{SS_T} =\\frac{SS_T - SS_{res}}{SS_T} = 1 - \\frac{SS_{res}}{SS_T}$$\n\n- $R^2$ as the proportionate reduction of total variation associated with the use of $X$.\n\n- **(a)** $\\hat{y}_i = y_i$ and $\\small SS_{res} =  \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = 0$. **(b)** $\\hat{y}_i = \\overline{y}$ and $\\small SS_R = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2  = 0$.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-regression/r_square.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n::: notes\n\n- **coefficient of determination** can be used to measure the quality of our regression or the explanatory power of regressors. \n- Here (a) and (b) are two extreme cases. \n- In (a), the fitted value = the true observation. So the regression model explains all the variation in $Y$, and hence $R^2 = 1$.\n- In (b), the fitted value = mean of y as if we don't have information about $x$ or $x$ is totally useless in predicting $Y$. In this case, the regression model explains no the variation in $Y$, and all variation remain unexplained. So $R^2 = 0$.\n\n:::\n\n\n## [R Lab]{.pink} $R^2$\n\n::: midi\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"true\"}\nsumm_reg_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` my_class600\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.104 -2.165 -0.224  2.059 15.010 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   35.698      0.720    49.5   <2e-16 ***\ndispl         -3.531      0.195   -18.1   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.84 on 232 degrees of freedom\nMultiple R-squared:  0.587,\tAdjusted R-squared:  0.585 \nF-statistic:  329 on 1 and 232 DF,  p-value: <2e-16\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nsumm_reg_fit$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.587\n```\n\n\n:::\n:::\n\n\n\n\n# Prediction\n\n## Predicting the Mean Response\n\n:::: columns\n\n::: {.column width=\"70%\"}\n\n- With predictor value $x = x_0$, we want to estimate the mean response $\\mu_{y|x_0} = \\beta_0 + \\beta_1 x_0$. \n  + <span style=\"color:blue\"> The **mean** highway MPG $\\mu_{y|x_0}$ when displacement is $x = x_0 = 5.5$. </span>\n\n:::\n\n::: {.column width=\"30%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-regression/regression_line_red.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n. . .\n\n::: midi\n\n- If $x_0$ is *within the range of $x$*, an *unbiased* point estimator for $\\mu_{y|x_0}$ is\n\n$$\\hat{\\mu}_{y | x_0} = b_0 + b_1 x_0$$\n\n:::\n\n. . .\n\n\n::: midi\n\n- The $(1-\\alpha)100\\%$ CI for $\\mu_{y|x_0}$ is\n\n$$\\hat{\\mu}_{y | x_0} \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}$$\n\n:::\n\n\n. . .\n\n::: question\nDoes the length of the CI for $\\mu_{y|x_0}$ stay the same at any location of $x_0$?\n:::\n\n<!-- - The sampling distribution of $\\hat{\\mu}_{y | x_0}$ is -->\n<!-- $$N\\left( \\mu_{y | x_0} = \\beta_0 + \\beta_1 x_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}} \\right) \\right)$$ -->\n\n\n\n::: notes\n\n- With predictor value $x = x_0$, we want to estimate the mean response $E(y\\mid x_0) = \\mu_{y|x_0} = \\beta_0 + \\beta_1 x_0$. \n  + <span style=\"color:blue\"> The **mean** highway MPG $E(y \\mid x_0)$ when displacement is $x = x_0 = 5.5$. </span>\n- If $x_0$ is *within the range of $x$*, an *unbiased* point estimator for $E(y\\mid x_0)$ is\n$$\\widehat{E(y\\mid x_0)} = \\hat{\\mu}_{y | x_0} = b_0 + b_1 x_0$$\n\n- The $(1-\\alpha)100\\%$ CI for $E(y\\mid x_0)$ is $\\boxed{\\hat{\\mu}_{y | x_0} \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}}$.\n<!-- - Remember there are two types of prediction, predicting the mean response $E(y\\mid x_0)$ and predicting an observation value given a value of $x$. -->\n<!-- - Here, we are doing real prediction, because we are not predicting the observation given the $x$ value in the data set, but we use our data set to do prediction when $x$ is a new value that is not in the data set. -->\n<!-- - So $x_0$ is any new value that is not shown in the data set. -->\n<!-- - We are interested in predicting a new mean response or new observation given the new value of $X$. OK. -->\n<!-- - For a given predictor value $x = x_0$, we want to estimate the mean response $E(y\\mid x_0) = \\mu_{y|x_0}$.  -->\n<!--   + <span style=\"color:blue\"> The **mean** highway MPG $E(y \\mid x_0)$ when displacement is $x = x_0 = 5.5$. </span> -->\n<!-- - If $x_0$ is within the range of the sample data on $x$, an unbiased point estimator for $E(y\\mid x_0)$ is -->\n<!-- $$\\widehat{E(y\\mid x_0)} = \\hat{\\mu}_{y | x_0} = b_0 + b_1 x_0$$ -->\n<!-- - The sampling distribution of $\\hat{\\mu}_{y | x_0}$ is -->\n<!-- $$N\\left( \\mu_{y | x_0} = \\beta_0 + \\beta_1 x_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}} \\right) \\right)$$ -->\n<!-- - $\\widehat{E(y\\mid x_0)} = \\hat{\\mu}_{y | x_0} = b_0 + b_1 x_0$; $\\quad E(y\\mid x_0) = \\mu_{y | x_0} = \\beta_0 + \\beta_1 x_0$ -->\n\n:::\n\n##\n\n::: midi\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n## CI for the mean response\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"confidence\", level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   fit  lwr  upr\n1 16.3 15.4 17.2\n```\n\n\n:::\n:::\n\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-regression_files/figure-revealjs/unnamed-chunk-42-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Predicting New Observations\n\n- Predict the value of a *new observation $y_0$* with $x = x_0$.\n  + <span style=\"color:blue\"> The **highway MPG of a car** $y_0(x_0)$ when its displacement is $x = x_0 = 5.5$. </span>\n  \n. . .\n\n- An *unbiased* point estimator for $y_0(x_0)$ is\n\n$$\\hat{y}_0(x_0) = b_0 + b_1 x_0$$\n\n. . .\n\n- The $(1-\\alpha)100\\%$ **prediction interval** (PI) for $y_0(x_0)$ is \n\n$$\\hat{y}_0 \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{{\\color{red}{1}}+ \\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}$$\n\n::: question\n\nWhat is the difference between CI for $\\mu_{y | x_0}$ and PI for $y_0(x_0)$?\n\n:::\n\n. . .\n\n- *The PI is wider as it includes the uncertainty about $b_0$, $b_1$ as well as $y_0$ due to error $\\epsilon$*.\n\n\n\n\n::: notes\n\n- Predict the value of a new observation $y_0$ corresponding to a specified value of predictor $x = x_0$.\n  + <span style=\"color:blue\"> The **highway MPG of a car** $y_0(x_0)$ when its displacement is $x = x_0 = 5.5$. </span>\n- An unbiased point estimator for $y_0(x_0)$ is\n$$\\hat{y}_0(x_0) = b_0 + b_1 x_0$$\n<!-- -- -->\n\n<!-- .question[ -->\n<!-- What is the sampling distribution of $\\hat{y}_0$? -->\n<!-- ] -->\n\n<!-- -- -->\n\n<!-- - $\\hat{y}_0 = b_0 + b_1 x_0 \\sim N\\left(\\beta_0 + \\beta_1 x_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}} \\right) \\right)$ -->\n\n<!-- -- -->\n\n<!-- .question[ -->\n<!-- What is the distribution of $y_0 = \\beta_0 + \\beta_1x_0 + \\epsilon$? -->\n<!-- ] -->\n\n<!-- ??? -->\n<!-- - $y_0(x_0)$ itself is a r.v. -->\n\n\n<!-- -- -->\n\n<!-- - $y_0 \\sim N\\left(\\beta_0 + \\beta_1 x_0, \\sigma^2 \\right)$ -->\n\n<!-- -- -->\n\n<!-- .question[ -->\n<!-- What is the distribution of $y_0 - \\hat{y}_0$? -->\n<!-- ] -->\n\n:::\n\n\n\n## [R Lab]{.pink} Prediction\n\n::: midi\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n## CI for the mean response\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"confidence\", level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   fit  lwr  upr\n1 16.3 15.4 17.2\n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n## PI for the new observation\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"predict\", level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   fit  lwr  upr\n1 16.3 8.67 23.9\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-regression_files/figure-revealjs/unnamed-chunk-44-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n::: notes\n\n- If I can only use one point or value to predict $y_0$, the best guess is the predicted value on the regression line. \n- After all, values around the lines would more likely to be drawn based on our model.\n- Here you can understand why uncertainty quantification is important. \n- Yes, we can predict an new observation value, but the prediction quality is gonna be bad because we are predict a random variable, not a constant, and $y_0$ can vary a lot around the regression line.\n- And prediction interval gives us an idea of how good or how bad our prediction is.\n\n:::\n\n\n##\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-regression_files/figure-revealjs/unnamed-chunk-45-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n::: notes\n\n- CI is the shortest when $x = \\bar{x}$. (Also PI)\n- PI length looks the same along with $x$ because the $\\sigma^2$ dominates the uncertainty, comparing to the uncertainty about $b_0$ and $b_1$.\n\n:::\n\n",
    "supporting": [
      "15-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}